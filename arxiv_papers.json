[{"title": "DeepMind Control Suite", "abstract": "The DeepMind Control Suite is a set of continuous control tasks with a\nstandardised structure and interpretable rewards, intended to serve as\nperformance benchmarks for reinforcement learning agents. The tasks are written\nin Python and powered by the MuJoCo physics engine, making them easy to use and\nmodify. We include benchmarks for several learning algorithms. The Control\nSuite is publicly available at https://www.github.com/deepmind/dm_control . A\nvideo summary of all tasks is available at http://youtu.be/rAai4QzcYbs ."}, {"title": "ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling", "abstract": "ViZDoom is a robust, first-person shooter reinforcement learning environment,\ncharacterized by a significant degree of latent state information. In this\npaper, double-Q learning and prioritized experience replay methods are tested\nunder a certain ViZDoom combat scenario using a competitive deep recurrent\nQ-network (DRQN) architecture. In addition, an ensembling technique known as\nsnapshot ensembling is employed using a specific annealed learning rate to\nobserve differences in ensembling efficacy under these two methods. Annealed\nlearning rates are important in general to the training of deep neural network\nmodels, as they shake up the status-quo and counter a model's tending towards\nlocal optima. While both variants show performance exceeding those of built-in\nAI agents of the game, the known stabilizing effects of double-Q learning are\nillustrated, and priority experience replay is again validated in its\nusefulness by showing immediate results early on in agent development, with the\ncaveat that value overestimation is accelerated in this case. In addition, some\nunique behaviors are observed to develop for priority experience replay (PER)\nand double-Q (DDQ) variants, and snapshot ensembling of both PER and DDQ proves\na valuable method for improving performance of the ViZDoom Marine."}, {"title": "Practical Challenges in Explicit Ethical Machine Reasoning", "abstract": "We examine implemented systems for ethical machine reasoning with a view to\nidentifying the practical challenges (as opposed to philosophical challenges)\nposed by the area. We identify a need for complex ethical machine reasoning not\nonly to be multi-objective, proactive, and scrutable but that it must draw on\nheterogeneous evidential reasoning. We also argue that, in many cases, it needs\nto operate in real time and be verifiable. We propose a general architecture\ninvolving a declarative ethical arbiter which draws upon multiple evidential\nreasoners each responsible for a particular ethical feature of the system's\nenvironment. We claim that this architecture enables some separation of\nconcerns among the practical challenges that ethical machine reasoning poses."}, {"title": "Intelligence Graph", "abstract": "In fact, there exist three genres of intelligence architectures: logics (e.g.\n\\textit{Random Forest, A$^*$ Searching}), neurons (e.g. \\textit{CNN, LSTM}) and\nprobabilities (e.g. \\textit{Naive Bayes, HMM}), all of which are incompatible\nto each other. However, to construct powerful intelligence systems with various\nmethods, we propose the intelligence graph (short as \\textbf{\\textit{iGraph}}),\nwhich is composed by both of neural and probabilistic graph, under the\nframework of forward-backward propagation. By the paradigm of iGraph, we design\na recommendation model with semantic principle. First, the probabilistic\ndistributions of categories are generated from the embedding representations of\nusers/items, in the manner of neurons. Second, the probabilistic graph infers\nthe distributions of features, in the manner of probabilities. Last, for the\nrecommendation diversity, we perform an expectation computation then conduct a\nlogic judgment, in the manner of logics. Experimentally, we beat the\nstate-of-the-art baselines and verify our conclusions."}, {"title": "Gatekeeping Algorithms with Human Ethical Bias: The ethics of algorithms in archives, libraries and society", "abstract": "In the age of algorithms, I focus on the question of how to ensure algorithms\nthat will take over many of our familiar archival and library tasks, will\nbehave according to human ethical norms that have evolved over many years. I\nstart by characterizing physical archives in the context of related\ninstitutions such as libraries and museums. In this setting I analyze how\nethical principles, in particular about access to information, have been\nformalized and communicated in the form of ethical codes, or: codes of\nconducts. After that I describe two main developments: digitalization, in which\nphysical aspects of the world are turned into digital data, and\nalgorithmization, in which intelligent computer programs turn this data into\npredictions and decisions. Both affect interactions that were once physical but\nnow digital. In this new setting I survey and analyze the ethical aspects of\nalgorithms and how they shape a vision on the future of archivists and\nlibrarians, in the form of algorithmic documentalists, or: codementalists.\nFinally I outline a general research strategy, called IntERMEeDIUM, to obtain\nalgorithms that obey are human ethical values encoded in code of ethics."}, {"title": "Entropy production rate as a criterion for inconsistency in decision theory", "abstract": "Individual and group decisions are complex, often involving choosing an apt\nalternative from a multitude of options. Evaluating pairwise comparisons breaks\ndown such complex decision problems into tractable ones. Pairwise comparison\nmatrices (PCMs) are regularly used to solve multiple-criteria decision-making\n(MCDM) problems, for example, using Saaty's analytic hierarchy process (AHP)\nframework. However, there are two significant drawbacks of using PCMs. First,\nhumans evaluate PCMs in an inconsistent manner. Second, not all entries of a\nlarge PCM can be reliably filled by human decision makers. We address these two\nissues by first establishing a novel connection between PCMs and\ntime-irreversible Markov processes. Specifically, we show that every PCM\ninduces a family of dissipative maximum path entropy random walks (MERW) over\nthe set of alternatives. We show that only `consistent' PCMs correspond to\ndetailed balanced MERWs. We identify the non-equilibrium entropy production in\nthe induced MERWs as a metric of inconsistency of the underlying PCMs. Notably,\nthe entropy production satisfies all of the recently laid out criteria for\nreasonable consistency indices. We also propose an approach to use incompletely\nfilled PCMs in AHP. Potential future avenues are discussed as well.\n  keywords: analytic hierarchy process, markov chains, maximum entropy"}, {"title": "A Reliability Theory of Truth", "abstract": "Our approach is basically a coherence approach, but we avoid the well-known\npitfalls of coherence theories of truth. Consistency is replaced by\nreliability, which expresses support and attack, and, in principle, every\ntheory (or agent, message) counts. At the same time, we do not require a\npriviledged access to \"reality\". A centerpiece of our approach is that we\nattribute reliability also to agents, messages, etc., so an unreliable source\nof information will be less important in future. Our ideas can also be extended\nto value systems, and even actions, e.g., of animals."}, {"title": "A Greedy Search Tree Heuristic for Symbolic Regression", "abstract": "Symbolic Regression tries to find a mathematical expression that describes\nthe relationship of a set of explanatory variables to a measured variable. The\nmain objective is to find a model that minimizes the error and, optionally,\nthat also minimizes the expression size. A smaller expression can be seen as an\ninterpretable model considered a reliable decision model. This is often\nperformed with Genetic Programming which represents their solution as\nexpression trees. The shortcoming of this algorithm lies on this representation\nthat defines a rugged search space and contains expressions of any size and\ndifficulty. These pose as a challenge to find the optimal solution under\ncomputational constraints. This paper introduces a new data structure, called\nInteraction-Transformation (IT), that constrains the search space in order to\nexclude a region of larger and more complicated expressions. In order to test\nthis data structure, it was also introduced an heuristic called SymTree. The\nobtained results show evidence that SymTree are capable of obtaining the\noptimal solution whenever the target function is within the search space of the\nIT data structure and competitive results when it is not. Overall, the\nalgorithm found a good compromise between accuracy and simplicity for all the\ngenerated models."}, {"title": "Distance formulas capable of unifying Euclidian space and probability space", "abstract": "For pattern recognition like image recognition, it has become clear that each\nmachine-learning dictionary data actually became data in probability space\nbelonging to Euclidean space. However, the distances in the Euclidean space and\nthe distances in the probability space are separated and ununified when machine\nlearning is introduced in the pattern recognition. There is still a problem\nthat it is impossible to directly calculate an accurate matching relation\nbetween the sampling data of the read image and the learned dictionary data. In\nthis research, we focused on the reason why the distance is changed and the\nextent of change when passing through the probability space from the original\nEuclidean distance among data belonging to multiple probability spaces\ncontaining Euclidean space. By finding the reason of the cause of the distance\nerror and finding the formula expressing the error quantitatively, a possible\ndistance formula to unify Euclidean space and probability space is found. Based\non the results of this research, the relationship between machine-learning\ndictionary data and sampling data was clearly understood for pattern\nrecognition. As a result, the calculation of collation among data and\nmachine-learning to compete mutually between data are cleared, and complicated\ncalculations became unnecessary. Finally, using actual pattern recognition\ndata, experimental demonstration of a possible distance formula to unify\nEuclidean space and probability space discovered by this research was carried\nout, and the effectiveness of the result was confirmed."}, {"title": "Multi-platform Version of StarCraft: Brood War in a Docker Container: Technical Report", "abstract": "We present a dockerized version of a real-time strategy game StarCraft: Brood\nWar, commonly used as a domain for AI research, with a pre-installed collection\nof AI developement tools supporting all the major types of StarCraft bots. This\nprovides a convenient way to deploy StarCraft AIs on numerous hosts at once and\nacross multiple platforms despite limited OS support of StarCraft. In this\ntechnical report, we describe the design of our Docker images and present a few\nuse cases."}, {"title": "Winograd Schema - Knowledge Extraction Using Narrative Chains", "abstract": "The Winograd Schema Challenge (WSC) is a test of machine intelligence,\ndesigned to be an improvement on the Turing test. A Winograd Schema consists of\na sentence and a corresponding question. To successfully answer these\nquestions, one requires the use of commonsense knowledge and reasoning. This\nwork focuses on extracting common sense knowledge which can be used to generate\nanswers for the Winograd schema challenge. Common sense knowledge is extracted\nbased on events (or actions) and their participants; called Event-Based\nConditional Commonsense (ECC). I propose an approach using Narrative Event\nChains [Chambers et al., 2008] to extract ECC knowledge. These are stored in\ntemplates, to be later used for answering the WSC questions. This approach\nworks well with respect to a subset of WSC tasks."}, {"title": "A generalized concept-cognitive learning: A machine learning viewpoint", "abstract": "Concept-cognitive learning (CCL) is a hot topic in recent years, and it has\nattracted much attention from the communities of formal concept analysis,\ngranular computing and cognitive computing. However, the relationship among\ncognitive computing (CC), concept-cognitive computing (CCC), CCL and\nconcept-cognitive learning model (CCLM) is not clearly described. To this end,\nwe first explain the relationship of CC, CCC, CCL and CCLM. Then, we propose a\ngeneralized concept-cognitive learning (GCCL) from the point of view of machine\nlearning. Finally, experiments on some data sets are conducted to verify the\nfeasibility of concept formation and concept-cognitive process of GCCL."}, {"title": "Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes", "abstract": "We present a study in Distributed Deep Reinforcement Learning (DDRL) focused\non scalability of a state-of-the-art Deep Reinforcement Learning algorithm\nknown as Batch Asynchronous Advantage ActorCritic (BA3C). We show that using\nthe Adam optimization algorithm with a batch size of up to 2048 is a viable\nchoice for carrying out large scale machine learning computations. This,\ncombined with careful reexamination of the optimizer's hyperparameters, using\nsynchronous training on the node level (while keeping the local, single node\npart of the algorithm asynchronous) and minimizing the memory footprint of the\nmodel, allowed us to achieve linear scaling for up to 64 CPU nodes. This\ncorresponds to a training time of 21 minutes on 768 CPU cores, as opposed to 10\nhours when using a single node with 24 cores achieved by a baseline single-node\nimplementation."}, {"title": "Abstract: Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients", "abstract": "We propose a deep learning model - Probabilistic Prognostic Estimates of\nSurvival in Metastatic Cancer Patients (PPES-Met) for estimating short-term\nlife expectancy (3 months) of the patients by analyzing free-text clinical\nnotes in the electronic medical record, while maintaining the temporal visit\nsequence. In a single framework, we integrated semantic data mapping and neural\nembedding technique to produce a text processing method that extracts relevant\ninformation from heterogeneous types of clinical notes in an unsupervised\nmanner, and we designed a recurrent neural network to model the temporal\ndependency of the patient visits. The model was trained on a large dataset\n(10,293 patients) and validated on a separated dataset (1818 patients). Our\nmethod achieved an area under the ROC curve (AUC) of 0.89. To provide\nexplain-ability, we developed an interactive graphical tool that may improve\nphysician understanding of the basis for the model's predictions. The high\naccuracy and explain-ability of the PPES-Met model may enable our model to be\nused as a decision support tool to personalize metastatic cancer treatment and\nprovide valuable assistance to the physicians."}, {"title": "Deep In-GPU Experience Replay", "abstract": "Experience replay allows a reinforcement learning agent to train on samples\nfrom a large amount of the most recent experiences. A simple in-RAM experience\nreplay stores these most recent experiences in a list in RAM, and then copies\nsampled batches to the GPU for training. I moved this list to the GPU, thus\ncreating an in-GPU experience replay, and a training step that no longer has\ninputs copied from the CPU. I trained an agent to play Super Smash Bros. Melee,\nusing internal game memory values as inputs and outputting controller button\npresses. A single state in Melee contains 27 floats, so the full experience\nreplay fits on a single GPU. For a batch size of 128, the in-GPU experience\nreplay trained twice as fast as the in-RAM experience replay. As far as I know,\nthis is the first in-GPU implementation of experience replay. Finally, I note a\nfew ideas for fitting the experience replay inside the GPU when the environment\nstate requires more memory."}, {"title": "A Formalization of Kant's Second Formulation of the Categorical Imperative", "abstract": "We present a formalization and computational implementation of the second\nformulation of Kant's categorical imperative. This ethical principle requires\nan agent to never treat someone merely as a means but always also as an end.\nHere we interpret this principle in terms of how persons are causally affected\nby actions. We introduce Kantian causal agency models in which moral patients,\nactions, goals, and causal influence are represented, and we show how to\nformalize several readings of Kant's categorical imperative that correspond to\nKant's concept of strict and wide duties towards oneself and others. Stricter\nversions handle cases where an action directly causally affects oneself or\nothers, whereas the wide version maximizes the number of persons being treated\nas an end. We discuss limitations of our formalization by pointing to one of\nKant's cases that the machinery cannot handle in a satisfying way."}, {"title": "Greenhouse: A Zero-Positive Machine Learning System for Time-Series Anomaly Detection", "abstract": "This short paper describes our ongoing research on Greenhouse - a\nzero-positive machine learning system for time-series anomaly detection."}, {"title": "Precision and Recall for Range-Based Anomaly Detection", "abstract": "Classical anomaly detection is principally concerned with point-based\nanomalies, anomalies that occur at a single data point. In this paper, we\npresent a new mathematical model to express range-based anomalies, anomalies\nthat occur over a range (or period) of time."}, {"title": "Reasoning about Unforeseen Possibilities During Policy Learning", "abstract": "Methods for learning optimal policies in autonomous agents often assume that\nthe way the domain is conceptualised---its possible states and actions and\ntheir causal structure---is known in advance and does not change during\nlearning. This is an unrealistic assumption in many scenarios, because new\nevidence can reveal important information about what is possible, possibilities\nthat the agent was not aware existed prior to learning. We present a model of\nan agent which both discovers and learns to exploit unforeseen possibilities\nusing two sources of evidence: direct interaction with the world and\ncommunication with a domain expert. We use a combination of probabilistic and\nsymbolic reasoning to estimate all components of the decision problem,\nincluding its set of random variables and their causal dependencies. Agent\nsimulations show that the agent converges on optimal polices even when it\nstarts out unaware of factors that are critical to behaving optimally."}, {"title": "Planning with Pixels in (Almost) Real Time", "abstract": "Recently, width-based planning methods have been shown to yield\nstate-of-the-art results in the Atari 2600 video games. For this, the states\nwere associated with the (RAM) memory states of the simulator. In this work, we\nconsider the same planning problem but using the screen instead. By using the\nsame visual inputs, the planning results can be compared with those of humans\nand learning methods. We show that the planning approach, out of the box and\nwithout training, results in scores that compare well with those obtained by\nhumans and learning methods, and moreover, by developing an episodic, rollout\nversion of the IW(k) algorithm, we show that such scores can be obtained in\nalmost real time."}, {"title": "Axiomatizations of inconsistency indices for triads", "abstract": "Pairwise comparison matrices often exhibit inconsistency, therefore many\nindices have been suggested to measure their deviation from a consistent\nmatrix. A set of axioms has been proposed recently that is required to be\nsatisfied by any reasonable inconsistency index. This set seems to be not\nexhaustive as illustrated by an example, hence it is expanded by adding two new\nproperties. All axioms are considered on the set of triads, pairwise comparison\nmatrices with three alternatives, which is the simplest case of inconsistency.\nWe choose the logically independent properties and prove that they\ncharacterize, that is, uniquely determine the inconsistency ranking induced by\nmost inconsistency indices that coincide on this restricted domain. Since\ntriads play a prominent role in a number of inconsistency indices, our results\ncan also contribute to the measurement of inconsistency for pairwise comparison\nmatrices with more than three alternatives."}, {"title": "Neural Program Synthesis with Priority Queue Training", "abstract": "We consider the task of program synthesis in the presence of a reward\nfunction over the output of programs, where the goal is to find programs with\nmaximal rewards. We employ an iterative optimization scheme, where we train an\nRNN on a dataset of K best programs from a priority queue of the generated\nprograms so far. Then, we synthesize new programs and add them to the priority\nqueue by sampling from the RNN. We benchmark our algorithm, called priority\nqueue training (or PQT), against genetic algorithm and reinforcement learning\nbaselines on a simple but expressive Turing complete programming language\ncalled BF. Our experimental results show that our simple PQT algorithm\nsignificantly outperforms the baselines. By adding a program length penalty to\nthe reward function, we are able to synthesize short, human readable programs."}, {"title": "Counterfactual equivalence for POMDPs, and underlying deterministic environments", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are rich environments\noften used in machine learning. But the issue of information and causal\nstructures in POMDPs has been relatively little studied. This paper presents\nthe concepts of equivalent and counterfactually equivalent POMDPs, where agents\ncannot distinguish which environment they are in though any observations and\nactions. It shows that any POMDP is counterfactually equivalent, for any finite\nnumber of turns, to a deterministic POMDP with all uncertainty concentrated\ninto the initial state. This allows a better understanding of POMDP\nuncertainty, information, and learning."}, {"title": "Formalized Conceptual Spaces with a Geometric Representation of Correlations", "abstract": "The highly influential framework of conceptual spaces provides a geometric\nway of representing knowledge. Instances are represented by points in a\nsimilarity space and concepts are represented by convex regions in this space.\nAfter pointing out a problem with the convexity requirement, we propose a\nformalization of conceptual spaces based on fuzzy star-shaped sets. Our\nformalization uses a parametric definition of concepts and extends the original\nframework by adding means to represent correlations between different domains\nin a geometric way. Moreover, we define various operations for our\nformalization, both for creating new concepts from old ones and for measuring\nrelations between concepts. We present an illustrative toy-example and sketch a\nresearch project on concept formation that is based on both our formalization\nand its implementation."}, {"title": "Model-Based Action Exploration for Learning Dynamic Motion Skills", "abstract": "Deep reinforcement learning has achieved great strides in solving challenging\nmotion control tasks. Recently, there has been significant work on methods for\nexploiting the data gathered during training, but there has been less work on\nhow to best generate the data to learn from. For continuous action domains, the\nmost common method for generating exploratory actions involves sampling from a\nGaussian distribution centred around the mean action output by a policy.\nAlthough these methods can be quite capable, they do not scale well with the\ndimensionality of the action space, and can be dangerous to apply on hardware.\nWe consider learning a forward dynamics model to predict the result,\n($x_{t+1}$), of taking a particular action, ($u$), given a specific observation\nof the state, ($x_{t}$). With this model we perform internal look-ahead\npredictions of outcomes and seek actions we believe have a reasonable chance of\nsuccess. This method alters the exploratory action space, thereby increasing\nlearning speed and enables higher quality solutions to difficult problems, such\nas robotic locomotion and juggling."}, {"title": "Multilayered Model of Speech", "abstract": "Human speech is the most important part of General Artificial Intelligence\nand subject of much research. The hypothesis proposed in this article provides\nexplanation of difficulties that modern science tackles in the field of human\nbrain simulation. The hypothesis is based on the author's conviction that the\nbrain of any given person has different ability to process and store\ninformation. Therefore, the approaches that are currently used to create\nGeneral Artificial Intelligence have to be altered."}, {"title": "Engineering Cooperative Smart Things based on Embodied Cognition", "abstract": "The goal of the Internet of Things (IoT) is to transform any thing around us,\nsuch as a trash can or a street light, into a smart thing. A smart thing has\nthe ability of sensing, processing, communicating and/or actuating. In order to\nachieve the goal of a smart IoT application, such as minimizing waste\ntransportation costs or reducing energy consumption, the smart things in the\napplication scenario must cooperate with each other without a centralized\ncontrol. Inspired by known approaches to design swarm of cooperative and\nautonomous robots, we modeled our smart things based on the embodied cognition\nconcept. Each smart thing is a physical agent with a body composed of a\nmicrocontroller, sensors and actuators, and a brain that is represented by an\nartificial neural network. This type of agent is commonly called an embodied\nagent. The behavior of these embodied agents is autonomously configured through\nan evolutionary algorithm that is triggered according to the application\nperformance. To illustrate, we have designed three homogeneous prototypes for\nsmart street lights based on an evolved network. This application has shown\nthat the proposed approach results in a feasible way of modeling decentralized\nsmart things with self-developed and cooperative capabilities."}, {"title": "A Computational Model of Commonsense Moral Decision Making", "abstract": "We introduce a new computational model of moral decision making, drawing on a\nrecent theory of commonsense moral learning via social dynamics. Our model\ndescribes moral dilemmas as a utility function that computes trade-offs in\nvalues over abstract moral dimensions, which provide interpretable parameter\nvalues when implemented in machine-led ethical decision-making. Moreover,\ncharacterizing the social structures of individuals and groups as a\nhierarchical Bayesian model, we show that a useful description of an\nindividual's moral values - as well as a group's shared values - can be\ninferred from a limited amount of observed data. Finally, we apply and evaluate\nour approach to data from the Moral Machine, a web application that collects\nhuman judgments on moral dilemmas involving autonomous vehicles."}, {"title": "Top k Memory Candidates in Memory Networks for Common Sense Reasoning", "abstract": "Successful completion of reasoning task requires the agent to have relevant\nprior knowledge or some given context of the world dynamics. Usually, the\ninformation provided to the system for a reasoning task is just the query or\nsome supporting story, which is often not enough for common reasoning tasks.\nThe goal here is that, if the information provided along the question is not\nsufficient to correctly answer the question, the model should choose k most\nrelevant documents that can aid its inference process. In this work, the model\ndynamically selects top k most relevant memory candidates that can be used to\nsuccessfully solve reasoning tasks. Experiments were conducted on a subset of\nWinograd Schema Challenge (WSC) problems to show that the proposed model has\nthe potential for commonsense reasoning. The WSC is a test of machine\nintelligence, designed to be an improvement on the Turing test."}, {"title": "The Role of Conditional Independence in the Evolution of Intelligent Systems", "abstract": "Systems are typically made from simple components regardless of their\ncomplexity. While the function of each part is easily understood, higher order\nfunctions are emergent properties and are notoriously difficult to explain. In\nnetworked systems, both digital and biological, each component receives inputs,\nperforms a simple computation, and creates an output. When these components\nhave multiple outputs, we intuitively assume that the outputs are causally\ndependent on the inputs but are themselves independent of each other given the\nstate of their shared input. However, this intuition can be violated for\ncomponents with probabilistic logic, as these typically cannot be decomposed\ninto separate logic gates with one output each. This violation of conditional\nindependence on the past system state is equivalent to instantaneous\ninteraction --- the idea is that some information between the outputs is not\ncoming from the inputs and thus must have been created instantaneously. Here we\ncompare evolved artificial neural systems with and without instantaneous\ninteraction across several task environments. We show that systems without\ninstantaneous interactions evolve faster, to higher final levels of\nperformance, and require fewer logic components to create a densely connected\ncognitive machinery."}, {"title": "A formal framework for deliberated judgment", "abstract": "While the philosophical literature has extensively studied how decisions\nrelate to arguments, reasons and justifications, decision theory almost\nentirely ignores the latter notions and rather focuses on preference and\nbelief. In this article, we argue that decision theory can largely benefit from\nexplicitly taking into account the stance that decision-makers take towards\narguments and counter-arguments. To that end, we elaborate a formal framework\naiming to integrate the role of arguments and argumentation in decision theory\nand decision aid. We start from a decision situation, where an individual\nrequests decision support. In this context, we formally define, as a\ncommendable basis for decision-aid, this individual's deliberated judgment,\npopularized by Rawls. We explain how models of deliberated judgment can be\nvalidated empirically. We then identify conditions upon which the existence of\na valid model can be taken for granted, and analyze how these conditions can be\nrelaxed. We then explore the significance of our proposed framework for\ndecision aiding practice. We argue that our concept of deliberated judgment\nowes its normative credentials both to its normative foundations (the idea of\nrationality based on arguments) and to its reference to empirical reality (the\nstance that real, empirical individuals hold towards arguments and\ncounter-arguments, on due reflection). We then highlight that our framework\nopens promising avenues for future research involving both philosophical and\ndecision theoretic approaches, as well as empirical implementations."}, {"title": "Innateness, AlphaZero, and Artificial Intelligence", "abstract": "The concept of innateness is rarely discussed in the context of artificial\nintelligence. When it is discussed, or hinted at, it is often the context of\ntrying to reduce the amount of innate machinery in a given system. In this\npaper, I consider as a test case a recent series of papers by Silver et al\n(Silver et al., 2017a) on AlphaGo and its successors that have been presented\nas an argument that a \"even in the most challenging of domains: it is possible\nto train to superhuman level, without human examples or guidance\", \"starting\ntabula rasa.\"\n  I argue that these claims are overstated, for multiple reasons. I close by\narguing that artificial intelligence needs greater attention to innateness, and\nI point to some proposals about what that innateness might look like."}, {"title": "Learning model-based strategies in simple environments with hierarchical q-networks", "abstract": "Recent advances in deep learning have allowed artificial agents to rival\nhuman-level performance on a wide range of complex tasks; however, the ability\nof these networks to learn generalizable strategies remains a pressing\nchallenge. This critical limitation is due in part to two factors: the opaque\ninformation representation in deep neural networks and the complexity of the\ntask environments in which they are typically deployed. Here we propose a novel\nHierarchical Q-Network (HQN) motivated by theories of the hierarchical\norganization of the human prefrontal cortex, that attempts to identify lower\ndimensional patterns in the value landscape that can be exploited to construct\nan internal model of rules in simple environments. We draw on combinatorial\ngames, where there exists a single optimal strategy for winning that\ngeneralizes across other features of the game, to probe the strategy\ngeneralization of the HQN and other reinforcement learning (RL) agents using\nvariations of Wythoff's game. Traditional RL approaches failed to reach\nsatisfactory performance on variants of Wythoff's Game; however, the HQN\nlearned heuristic-like strategies that generalized across changes in board\nconfiguration. More importantly, the HQN allowed for transparent inspection of\nthe agent's internal model of the game following training. Our results show how\na biologically inspired hierarchical learner can facilitate learning abstract\nrules to promote robust and flexible action policies in simplified training\nenvironments with clearly delineated optimal strategies."}, {"title": "Reasoning about multiple aspects in DLs: Semantics and Closure Construction", "abstract": "Starting from the observation that rational closure has the undesirable\nproperty of being an \"all or nothing\" mechanism, we here propose a\nmultipreferential semantics, which enriches the preferential semantics\nunderlying rational closure in order to separately deal with the inheritance of\ndifferent properties in an ontology with exceptions. We provide a\nmultipreference closure mechanism which is sound with respect to the\nmultipreference semantics."}, {"title": "CHALET: Cornell House Agent Learning Environment", "abstract": "We present CHALET, a 3D house simulator with support for navigation and\nmanipulation. CHALET includes 58 rooms and 10 house configuration, and allows\nto easily create new house and room layouts. CHALET supports a range of common\nhousehold activities, including moving objects, toggling appliances, and\nplacing objects inside closeable containers. The environment and actions\navailable are designed to create a challenging domain to train and evaluate\nautonomous agents, including for tasks that combine language, vision, and\nplanning in a dynamic environment."}, {"title": "Comparison Training for Computer Chinese Chess", "abstract": "This paper describes the application of comparison training (CT) for\nautomatic feature weight tuning, with the final objective of improving the\nevaluation functions used in Chinese chess programs. First, we propose an\nn-tuple network to extract features, since n-tuple networks require very little\nexpert knowledge through its large numbers of features, while simulta-neously\nallowing easy access. Second, we propose a novel evalua-tion method that\nincorporates tapered eval into CT. Experiments show that with the same features\nand the same Chinese chess program, the automatically tuned comparison training\nfeature weights achieved a win rate of 86.58% against the weights that were\nhand-tuned. The above trained version was then improved by adding additional\nfeatures, most importantly n-tuple features. This improved version achieved a\nwin rate of 81.65% against the trained version without additional features."}, {"title": "Curiosity-driven reinforcement learning with homeostatic regulation", "abstract": "We propose a curiosity reward based on information theory principles and\nconsistent with the animal instinct to maintain certain critical parameters\nwithin a bounded range. Our experimental validation shows the added value of\nthe additional homeostatic drive to enhance the overall information gain of a\nreinforcement learning agent interacting with a complex environment using\ncontinuous actions. Our method builds upon two ideas: i) To take advantage of a\nnew Bellman-like equation of information gain and ii) to simplify the\ncomputation of the local rewards by avoiding the approximation of complex\ndistributions over continuous states and actions."}, {"title": "Development and application of a machine learning supported methodology for measurement and verification (M&V) 2.0", "abstract": "The foundations of all methodologies for the measurement and verification\n(M&V) of energy savings are based on the same five key principles: accuracy,\ncompleteness, conservatism, consistency and transparency. The most widely\naccepted methodologies tend to generalise M&V so as to ensure applicability\nacross the spectrum of energy conservation measures (ECM's). These do not\nprovide a rigid calculation procedure to follow. This paper aims to bridge the\ngap between high-level methodologies and the practical application of modelling\nalgorithms, with a focus on the industrial buildings sector. This is achieved\nwith the development of a novel, machine learning supported methodology for M&V\n2.0 which enables accurate quantification of savings.\n  A novel and computationally efficient feature selection algorithm and\npowerful machine learning regression algorithms are employed to maximise the\neffectiveness of available data. The baseline period energy consumption is\nmodelled using artificial neural networks, support vector machines, k-nearest\nneighbours and multiple ordinary least squares regression. Improved knowledge\ndiscovery and an expanded boundary of analysis allow more complex energy\nsystems be analysed, thus increasing the applicability of M&V. A case study in\na large biomedical manufacturing facility is used to demonstrate the\nmethodology's ability to accurately quantify the savings under real-world\nconditions. The ECM was found to result in 604,527 kWh of energy savings with\n57% uncertainty at a confidence interval of 68%. 20 baseline energy models are\ndeveloped using an exhaustive approach with the optimal model being used to\nquantify savings. The range of savings estimated with each model are presented\nand the acceptability of uncertainty is reviewed. The case study demonstrates\nthe ability of the methodology to perform M&V to an acceptable standard in\nchallenging circumstances."}, {"title": "Directly Estimating the Variance of the λ-Return Using Temporal-Difference Methods", "abstract": "This paper investigates estimating the variance of a temporal-difference\nlearning agent's update target. Most reinforcement learning methods use an\nestimate of the value function, which captures how good it is for the agent to\nbe in a particular state and is mathematically expressed as the expected sum of\ndiscounted future rewards (called the return). These values can be\nstraightforwardly estimated by averaging batches of returns using Monte Carlo\nmethods. However, if we wish to update the agent's value estimates during\nlearning--before terminal outcomes are observed--we must use a different\nestimation target called the {\\lambda}-return, which truncates the return with\nthe agent's own estimate of the value function. Temporal difference learning\nmethods estimate the expected {\\lambda}-return for each state, allowing these\nmethods to update online and incrementally, and in most cases achieve better\ngeneralization error and faster learning than Monte Carlo methods. Naturally\none could attempt to estimate higher-order moments of the {\\lambda}-return.\nThis paper is about estimating the variance of the {\\lambda}-return. Prior work\nhas shown that given estimates of the variance of the {\\lambda}-return,\nlearning systems can be constructed to (1) mitigate risk in action selection,\nand (2) automatically adapt the parameters of the learning process itself to\nimprove performance. Unfortunately, existing methods for estimating the\nvariance of the {\\lambda}-return are complex and not well understood\nempirically. We contribute a method for estimating the variance of the\n{\\lambda}-return directly using policy evaluation methods from reinforcement\nlearning. Our approach is significantly simpler than prior methods that\nindependently estimate the second moment of the {\\lambda}-return. Empirically\nour new approach behaves at least as well as existing approaches, but is\ngenerally more robust."}, {"title": "Discovering Markov Blanket from Multiple interventional Datasets", "abstract": "In this paper, we study the problem of discovering the Markov blanket (MB) of\na target variable from multiple interventional datasets. Datasets attained from\ninterventional experiments contain richer causal information than passively\nobserved data (observational data) for MB discovery. However, almost all\nexisting MB discovery methods are designed for finding MBs from a single\nobservational dataset. To identify MBs from multiple interventional datasets,\nwe face two challenges: (1) unknown intervention variables; (2) nonidentical\ndata distributions. To tackle the challenges, we theoretically analyze (a)\nunder what conditions we can find the correct MB of a target variable, and (b)\nunder what conditions we can identify the causes of the target variable via\ndiscovering its MB. Based on the theoretical analysis, we propose a new\nalgorithm for discovering MBs from multiple interventional datasets, and\npresent the conditions/assumptions which assure the correctness of the\nalgorithm. To our knowledge, this work is the first to present the theoretical\nanalyses about the conditions for MB discovery in multiple interventional\ndatasets and the algorithm to find the MBs in relation to the conditions. Using\nbenchmark Bayesian networks and real-world datasets, the experiments have\nvalidated the effectiveness and efficiency of the proposed algorithm in the\npaper."}, {"title": "Probabilistic Planning by Probabilistic Programming", "abstract": "Automated planning is a major topic of research in artificial intelligence,\nand enjoys a long and distinguished history. The classical paradigm assumes a\ndistinguished initial state, comprised of a set of facts, and is defined over a\nset of actions which change that state in one way or another. Planning in many\nreal-world settings, however, is much more involved: an agent's knowledge is\nalmost never simply a set of facts that are true, and actions that the agent\nintends to execute never operate the way they are supposed to. Thus,\nprobabilistic planning attempts to incorporate stochastic models directly into\nthe planning process. In this article, we briefly report on probabilistic\nplanning through the lens of probabilistic programming: a programming paradigm\nthat aims to ease the specification of structured probability distributions. In\nparticular, we provide an overview of the features of two systems, HYPE and\nALLEGRO, which emphasise different strengths of probabilistic programming that\nare particularly useful for complex modelling issues raised in probabilistic\nplanning. Among other things, with these systems, one can instantiate planning\nproblems with growing and shrinking state spaces, discrete and continuous\nprobability distributions, and non-unique prior distributions in a first-order\nsetting."}, {"title": "Finding ReMO (Related Memory Object): A Simple Neural Architecture for Text based Reasoning", "abstract": "To solve the text-based question and answering task that requires relational\nreasoning, it is necessary to memorize a large amount of information and find\nout the question relevant information from the memory. Most approaches were\nbased on external memory and four components proposed by Memory Network. The\ndistinctive component among them was the way of finding the necessary\ninformation and it contributes to the performance. Recently, a simple but\npowerful neural network module for reasoning called Relation Network (RN) has\nbeen introduced. We analyzed RN from the view of Memory Network, and realized\nthat its MLP component is able to reveal the complicate relation between\nquestion and object pair. Motivated from it, we introduce which uses MLP to\nfind out relevant information on Memory Network architecture. It shows new\nstate-of-the-art results in jointly trained bAbI-10k story-based question\nanswering tasks and bAbI dialog-based question answering tasks."}, {"title": "Knowledge Graph Embedding with Multiple Relation Projections", "abstract": "Knowledge graphs contain rich relational structures of the world, and thus\ncomplement data-driven machine learning in heterogeneous data. One of the most\neffective methods in representing knowledge graphs is to embed symbolic\nrelations and entities into continuous spaces, where relations are\napproximately linear translation between projected images of entities in the\nrelation space. However, state-of-the-art relation projection methods such as\nTransR, TransD or TransSparse do not model the correlation between relations,\nand thus are not scalable to complex knowledge graphs with thousands of\nrelations, both in computational demand and in statistical robustness. To this\nend we introduce TransF, a novel translation-based method which mitigates the\nburden of relation projection by explicitly modeling the basis subspaces of\nprojection matrices. As a result, TransF is far more light weight than the\nexisting projection methods, and is robust when facing a high number of\nrelations. Experimental results on the canonical link prediction task show that\nour proposed model outperforms competing rivals by a large margin and achieves\nstate-of-the-art performance. Especially, TransF improves by 9%/5% in the\nhead/tail entity prediction task for N-to-1/1-to-N relations over the best\nperforming translation-based method."}, {"title": "Ontology-based Fuzzy Markup Language Agent for Student and Robot Co-Learning", "abstract": "An intelligent robot agent based on domain ontology, machine learning\nmechanism, and Fuzzy Markup Language (FML) for students and robot co-learning\nis presented in this paper. The machine-human co-learning model is established\nto help various students learn the mathematical concepts based on their\nlearning ability and performance. Meanwhile, the robot acts as a teacher's\nassistant to co-learn with children in the class. The FML-based knowledge base\nand rule base are embedded in the robot so that the teachers can get feedback\nfrom the robot on whether students make progress or not. Next, we inferred\nstudents' learning performance based on learning content's difficulty and\nstudents' ability, concentration level, as well as teamwork sprit in the class.\nExperimental results show that learning with the robot is helpful for\ndisadvantaged and below-basic children. Moreover, the accuracy of the\nintelligent FML-based agent for student learning is increased after machine\nlearning mechanism."}, {"title": "Safe Exploration in Continuous Action Spaces", "abstract": "We address the problem of deploying a reinforcement learning (RL) agent on a\nphysical system such as a datacenter cooling unit or robot, where critical\nconstraints must never be violated. We show how to exploit the typically smooth\ndynamics of these systems and enable RL algorithms to never violate constraints\nduring learning. Our technique is to directly add to the policy a safety layer\nthat analytically solves an action correction formulation per each state. The\nnovelty of obtaining an elegant closed-form solution is attained due to a\nlinearized model, learned on past trajectories consisting of arbitrary actions.\nThis is to mimic the real-world circumstances where data logs were generated\nwith a behavior policy that is implausible to describe mathematically; such\ncases render the known safety-aware off-policy methods inapplicable. We\ndemonstrate the efficacy of our approach on new representative physics-based\nenvironments, and prevail where reward shaping fails by maintaining zero\nconstraint violations."}, {"title": "SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to object-oriented SPIN rules", "abstract": "Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language)\nontologies with Horn Logic rules of the Rule Markup Language (RuleML) family.\nBeing supported by ontology editors, rule engines and ontology reasoners, it\nhas become a very popular choice for developing rule-based applications on top\nof ontologies. However, SWRL is probably not go-ing to become a WWW Consortium\nstandard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL\nInferencing Notation) has become a de-facto industry standard to rep-resent\nSPARQL rules and constraints on Semantic Web models, building on the widespread\nacceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper,\nwe ar-gue that the life of existing SWRL rule-based ontology applications can\nbe prolonged by con-verting them to SPIN. To this end, we have developed the\nSWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules,\nconsidering the object-orientation of SPIN, i.e. linking rules to the\nappropriate ontology classes and optimizing them, as derived by analysing the\nrule conditions."}, {"title": "A Cyber Science Based Ontology for Artificial General Intelligence Containment", "abstract": "The development of artificial general intelligence is considered by many to\nbe inevitable. What such intelligence does after becoming aware is not so\ncertain. To that end, research suggests that the likelihood of artificial\ngeneral intelligence becoming hostile to humans is significant enough to\nwarrant inquiry into methods to limit such potential. Thus, containment of\nartificial general intelligence is a timely and meaningful research topic.\nWhile there is limited research exploring possible containment strategies, such\nwork is bounded by the underlying field the strategies draw upon. Accordingly,\nwe set out to construct an ontology to describe necessary elements in any\nfuture containment technology. Using existing academic literature, we developed\na single domain ontology containing five levels, 32 codes, and 32 associated\ndescriptors. Further, we constructed ontology diagrams to demonstrate intended\nrelationships. We then identified humans, AGI, and the cyber world as novel\nagent objects necessary for future containment activities. Collectively, the\nwork addresses three critical gaps: (a) identifying and arranging fundamental\nconstructs; (b) situating AGI containment within cyber science; and (c)\ndeveloping scientific rigor within the field."}, {"title": "Algorithms for the Greater Good! On Mental Modeling and Acceptable Symbiosis in Human-AI Collaboration", "abstract": "Effective collaboration between humans and AI-based systems requires\neffective modeling of the human in the loop, both in terms of the mental state\nas well as the physical capabilities of the latter. However, these models can\nalso open up pathways for manipulating and exploiting the human in the hopes of\nachieving some greater good, especially when the intent or values of the AI and\nthe human are not aligned or when they have an asymmetrical relationship with\nrespect to knowledge or computation power. In fact, such behavior does not\nnecessarily require any malicious intent but can rather be borne out of\ncooperative scenarios. It is also beyond simple misinterpretation of intents,\nas in the case of value alignment problems, and thus can be effectively\nengineered if desired. Such techniques already exist and pose several\nunresolved ethical and moral questions with regards to the design of autonomy.\nIn this paper, we illustrate some of these issues in a teaming scenario and\ninvestigate how they are perceived by participants in a thought experiment."}, {"title": "Features, Projections, and Representation Change for Generalized Planning", "abstract": "Generalized planning is concerned with the characterization and computation\nof plans that solve many instances at once. In the standard formulation, a\ngeneralized plan is a mapping from feature or observation histories into\nactions, assuming that the instances share a common pool of features and\nactions. This assumption, however, excludes the standard relational planning\ndomains where actions and objects change across instances. In this work, we\nextend the standard formulation of generalized planning to such domains. This\nis achieved by projecting the actions over the features, resulting in a common\nset of abstract actions which can be tested for soundness and completeness, and\nwhich can be used for generating general policies such as \"if the gripper is\nempty, pick the clear block above x and place it on the table\" that achieve the\ngoal clear(x) in any Blocksworld instance. In this policy, \"pick the clear\nblock above x\" is an abstract action that may represent the action Unstack(a,\nb) in one situation and the action Unstack(b, c) in another. Transformations\nare also introduced for computing such policies by means of fully observable\nnon-deterministic (FOND) planners. The value of generalized representations for\nlearning general policies is also discussed."}, {"title": "An Incremental Off-policy Search in a Model-free Markov Decision Process Using a Single Sample Path", "abstract": "In this paper, we consider a modified version of the control problem in a\nmodel free Markov decision process (MDP) setting with large state and action\nspaces. The control problem most commonly addressed in the contemporary\nliterature is to find an optimal policy which maximizes the value function,\ni.e., the long run discounted reward of the MDP. The current settings also\nassume access to a generative model of the MDP with the hidden premise that\nobservations of the system behaviour in the form of sample trajectories can be\nobtained with ease from the model. In this paper, we consider a modified\nversion, where the cost function is the expectation of a non-convex function of\nthe value function without access to the generative model. Rather, we assume\nthat a sample trajectory generated using a priori chosen behaviour policy is\nmade available. In this restricted setting, we solve the modified control\nproblem in its true sense, i.e., to find the best possible policy given this\nlimited information. We propose a stochastic approximation algorithm based on\nthe well-known cross entropy method which is data (sample trajectory)\nefficient, stable, robust as well as computationally and storage efficient. We\nprovide a proof of convergence of our algorithm to a policy which is globally\noptimal relative to the behaviour policy. We also present experimental results\nto corroborate our claims and we demonstrate the superiority of the solution\nproduced by our algorithm compared to the state-of-the-art algorithms under\nappropriately chosen behaviour policy."}, {"title": "Deep Learning Works in Practice. But Does it Work in Theory?", "abstract": "Deep learning relies on a very specific kind of neural networks: those\nsuperposing several neural layers. In the last few years, deep learning\nachieved major breakthroughs in many tasks such as image analysis, speech\nrecognition, natural language processing, and so on. Yet, there is no\ntheoretical explanation of this success. In particular, it is not clear why the\ndeeper the network, the better it actually performs.\n  We argue that the explanation is intimately connected to a key feature of the\ndata collected from our surrounding universe to feed the machine learning\nalgorithms: large non-parallelizable logical depth. Roughly speaking, we\nconjecture that the shortest computational descriptions of the universe are\nalgorithms with inherently large computation times, even when a large number of\ncomputers are available for parallelization. Interestingly, this conjecture,\ncombined with the folklore conjecture in theoretical computer science that $ P\n\\neq NC$, explains the success of deep learning."}, {"title": "Lifted Filtering via Exchangeable Decomposition", "abstract": "We present a model for exact recursive Bayesian filtering based on lifted\nmultiset states. Combining multisets with lifting makes it possible to\nsimultaneously exploit multiple strategies for reducing inference complexity\nwhen compared to list-based grounded state representations. The core idea is to\nborrow the concept of Maximally Parallel Multiset Rewriting Systems and to\nenhance it by concepts from Rao-Blackwellization and Lifted Inference, giving a\nrepresentation of state distributions that enables efficient inference. In\nworlds where the random variables that define the system state are exchangeable\n-- where the identity of entities does not matter -- it automatically uses a\nrepresentation that abstracts from ordering (achieving an exponential reduction\nin complexity) -- and it automatically adapts when observations or system\ndynamics destroy exchangeability by breaking symmetry."}, {"title": "A family of OWA operators based on Faulhaber's formulas", "abstract": "In this paper we develop a new family of Ordered Weighted Averaging (OWA)\noperators. Weight vector is obtained from a desired orness of the operator.\nUsing Faulhaber's formulas we obtain direct and simple expressions for the\nweight vector without any iteration loop. With the exception of one weight, the\nremaining follow a straight line relation. As a result, a fast and robust\nalgorithm is developed. The resulting weight vector is suboptimal according\nwith the Maximum Entropy criterion, but it is very close to the optimal.\nComparisons are done with other procedures."}, {"title": "Deceptive Games", "abstract": "Deceptive games are games where the reward structure or other aspects of the\ngame are designed to lead the agent away from a globally optimal policy. While\nmany games are already deceptive to some extent, we designed a series of games\nin the Video Game Description Language (VGDL) implementing specific types of\ndeception, classified by the cognitive biases they exploit. VGDL games can be\nrun in the General Video Game Artificial Intelligence (GVGAI) Framework, making\nit possible to test a variety of existing AI agents that have been submitted to\nthe GVGAI Competition on these deceptive games. Our results show that all\ntested agents are vulnerable to several kinds of deception, but that different\nagents have different weaknesses. This suggests that we can use deception to\nunderstand the capabilities of a game-playing algorithm, and game-playing\nalgorithms to characterize the deception displayed by a game."}, {"title": "Recursive Feature Generation for Knowledge-based Learning", "abstract": "When humans perform inductive learning, they often enhance the process with\nbackground knowledge. With the increasing availability of well-formed\ncollaborative knowledge bases, the performance of learning algorithms could be\nsignificantly enhanced if a way were found to exploit these knowledge bases. In\nthis work, we present a novel algorithm for injecting external knowledge into\ninduction algorithms using feature generation. Given a feature, the algorithm\ndefines a new learning task over its set of values, and uses the knowledge base\nto solve the constructed learning task. The resulting classifier is then used\nas a new feature for the original problem. We have applied our algorithm to the\ndomain of text classification using large semantic knowledge bases. We have\nshown that the generated features significantly improve the performance of\nexisting learning algorithms."}, {"title": "A Semantic Model for Historical Manuscripts", "abstract": "The study and publication of historical scientific manuscripts are com- plex\ntasks that involve, among others, the explicit representation of the text mean-\nings and reasoning on temporal entities. In this paper we present the first\nresults of an interdisciplinary project dedicated to the study of Saussure's\nmanuscripts. These results aim to fulfill requirements elaborated with\nSaussurean humanists. They comprise a model for the representation of\ntime-varying statements and time-varying domain knowledge (in particular\nterminologies) as well as imple- mentation techniques for the semantic indexing\nof manuscripts and for temporal reasoning on knowledge extracted from the\nmanuscripts."}, {"title": "Cross-City Transfer Learning for Deep Spatio-Temporal Prediction", "abstract": "Spatio-temporal prediction is a key type of tasks in urban computing, e.g.,\ntraffic flow and air quality. Adequate data is usually a prerequisite,\nespecially when deep learning is adopted. However, the development levels of\ndifferent cities are unbalanced, and still many cities suffer from data\nscarcity. To address the problem, we propose a novel cross-city transfer\nlearning method for deep spatio-temporal prediction tasks, called RegionTrans.\nRegionTrans aims to effectively transfer knowledge from a data-rich source city\nto a data-scarce target city. More specifically, we first learn an inter-city\nregion matching function to match each target city region to a similar source\ncity region. A neural network is designed to effectively extract region-level\nrepresentation for spatio-temporal prediction. Finally, an optimization\nalgorithm is proposed to transfer learned features from the source city to the\ntarget city with the region matching function. Using citywide crowd flow\nprediction as a demonstration experiment, we verify the effectiveness of\nRegionTrans. Results show that RegionTrans can outperform the state-of-the-art\nfine-tuning deep spatio-temporal prediction models by reducing up to 10.7%\nprediction error."}, {"title": "How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation", "abstract": "Recent years have seen a boom in interest in machine learning systems that\ncan provide a human-understandable rationale for their predictions or\ndecisions. However, exactly what kinds of explanation are truly\nhuman-interpretable remains poorly understood. This work advances our\nunderstanding of what makes explanations interpretable in the specific context\nof verification. Suppose we have a machine learning system that predicts X, and\nwe provide rationale for this prediction X. Given an input, an explanation, and\nan output, is the output consistent with the input and the supposed rationale?\nVia a series of user-studies, we identify what kinds of increases in complexity\nhave the greatest effect on the time it takes for humans to verify the\nrationale, and which seem relatively insensitive."}, {"title": "Modelling contextuality by probabilistic programs with hypergraph semantics", "abstract": "Models of a phenomenon are often developed by examining it under different\nexperimental conditions, or measurement contexts. The resultant probabilistic\nmodels assume that the underlying random variables, which define a measurable\nset of outcomes, can be defined independent of the measurement context. The\nphenomenon is deemed contextual when this assumption fails. Contextuality is an\nimportant issue in quantum physics. However, there has been growing speculation\nthat it manifests outside the quantum realm with human cognition being a\nparticularly prominent area of investigation. This article contributes the\nfoundations of a probabilistic programming language that allows convenient\nexploration of contextuality in wide range of applications relevant to\ncognitive science and artificial intelligence. Specific syntax is proposed to\nallow the specification of \"measurement contexts\". Each such context delivers a\npartial model of the phenomenon based on the associated experimental condition\ndescribed by the measurement context. The probabilistic program is translated\ninto a hypergraph in a modular way. Recent theoretical results from the field\nof quantum physics show that contextuality can be equated with the possibility\nof constructing a probabilistic model on the resulting hypergraph. The use of\nhypergraphs opens the door for a theoretically succinct and efficient\ncomputational semantics sensitive to modelling both contextual and\nnon-contextual phenomena. Finally, this article raises awareness of\ncontextuality beyond quantum physics and to contribute formal methods to detect\nits presence by means of hypergraph semantics."}, {"title": "Plan Explanations as Model Reconciliation -- An Empirical Study", "abstract": "Recent work in explanation generation for decision making agents has looked\nat how unexplained behavior of autonomous systems can be understood in terms of\ndifferences in the model of the system and the human's understanding of the\nsame, and how the explanation process as a result of this mismatch can be then\nseen as a process of reconciliation of these models. Existing algorithms in\nsuch settings, while having been built on contrastive, selective and social\nproperties of explanations as studied extensively in the psychology literature,\nhave not, to the best of our knowledge, been evaluated in settings with actual\nhumans in the loop. As such, the applicability of such explanations to human-AI\nand human-robot interactions remains suspect. In this paper, we set out to\nevaluate these explanation generation algorithms in a series of studies in a\nmock search and rescue scenario with an internal semi-autonomous robot and an\nexternal human commander. We demonstrate to what extent the properties of these\nalgorithms hold as they are evaluated by humans, and how the dynamics of trust\nbetween the human and the robot evolve during the process of these\ninteractions."}, {"title": "Tunneling Neural Perception and Logic Reasoning through Abductive Learning", "abstract": "Perception and reasoning are basic human abilities that are seamlessly\nconnected as part of human intelligence. However, in current machine learning\nsystems, the perception and reasoning modules are incompatible. Tasks requiring\njoint perception and reasoning ability are difficult to accomplish autonomously\nand still demand human intervention. Inspired by the way language experts\ndecoded Mayan scripts by joining two abilities in an abductive manner, this\npaper proposes the abductive learning framework. The framework learns\nperception and reasoning simultaneously with the help of a trial-and-error\nabductive process. We present the Neural-Logical Machine as an implementation\nof this novel learning framework. We demonstrate that--using human-like\nabductive learning--the machine learns from a small set of simple hand-written\nequations and then generalizes well to complex equations, a feat that is beyond\nthe capability of state-of-the-art neural network models. The abductive\nlearning framework explores a new direction for approaching human-level\nlearning ability."}, {"title": "Coordinated Exploration in Concurrent Reinforcement Learning", "abstract": "We consider a team of reinforcement learning agents that concurrently learn\nto operate in a common environment. We identify three properties - adaptivity,\ncommitment, and diversity - which are necessary for efficient coordinated\nexploration and demonstrate that straightforward extensions to single-agent\noptimistic and posterior sampling approaches fail to satisfy them. As an\nalternative, we propose seed sampling, which extends posterior sampling in a\nmanner that meets these requirements. Simulation results investigate how\nper-agent regret decreases as the number of agents grows, establishing\nsubstantial advantages of seed sampling over alternative exploration schemes."}, {"title": "The Sea Exploration Problem: Data-driven Orienteering on a Continuous Surface", "abstract": "This paper describes a problem arising in sea exploration, where the aim is\nto schedule the expedition of a ship for collecting information about the\nresources on the seafloor. The aim is to collect data by probing on a set of\ncarefully chosen locations, so that the information available is optimally\nenriched. This problem has similarities with the orienteering problem, where\nthe aim is to plan a time-limited trip for visiting a set of vertices,\ncollecting a prize at each of them, in such a way that the total value\ncollected is maximum. In our problem, the score at each vertex is associated\nwith an estimation of the level of the resource on the given surface, which is\ndone by regression using Gaussian processes. Hence, there is a correlation\namong scores on the selected vertices; this is a first difference with respect\nto the standard orienteering problem. The second difference is the location of\neach vertex, which in our problem is a freely chosen point on a given surface."}, {"title": "Guided Policy Exploration for Markov Decision Processes using an Uncertainty-Based Value-of-Information Criterion", "abstract": "Reinforcement learning in environments with many action-state pairs is\nchallenging. At issue is the number of episodes needed to thoroughly search the\npolicy space. Most conventional heuristics address this search problem in a\nstochastic manner. This can leave large portions of the policy space unvisited\nduring the early training stages. In this paper, we propose an\nuncertainty-based, information-theoretic approach for performing guided\nstochastic searches that more effectively cover the policy space. Our approach\nis based on the value of information, a criterion that provides the optimal\ntrade-off between expected costs and the granularity of the search process. The\nvalue of information yields a stochastic routine for choosing actions during\nlearning that can explore the policy space in a coarse to fine manner. We\naugment this criterion with a state-transition uncertainty factor, which guides\nthe search process into previously unexplored regions of the policy space."}, {"title": "Abstractly Interpreting Argumentation Frameworks for Sharpening Extensions", "abstract": "Cycles of attacking arguments pose non-trivial issues in Dung style\nargumentation theory, apparent behavioural difference between odd and even\nlength cycles being a notable one. While a few methods were proposed for\ntreating them, to - in particular - enable selection of acceptable arguments in\nan odd-length cycle when Dung semantics could select none, so far the issues\nhave been observed from a purely argument-graph-theoretic perspective. Per\ncontra, we consider argument graphs together with a certain lattice like\nsemantic structure over arguments e.g. ontology. As we show, the\nsemantic-argumentgraphic hybrid theory allows us to apply abstract\ninterpretation, a widely known methodology in static program analysis, to\nformal argumentation. With this, even where no arguments in a cycle could be\nselected sensibly, we could say more about arguments acceptability of an\nargument framework that contains it. In a certain sense, we can verify Dung\nextensions with respect to a semantic structure in this hybrid theory, to\nconsolidate our confidence in their suitability. By defining the theory, and by\nmaking comparisons to existing approaches, we ultimately discover that whether\nDung semantics, or an alternative semantics such as cf2, is adequate or\nproblematic depends not just on an argument graph but also on the semantic\nrelation among the arguments in the graph."}, {"title": "Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries", "abstract": "We focus on learning the desired objective function for a robot. Although\ntrajectory demonstrations can be very informative of the desired objective,\nthey can also be difficult for users to provide. Answers to comparison queries,\nasking which of two trajectories is preferable, are much easier for users, and\nhave emerged as an effective alternative. Unfortunately, comparisons are far\nless informative. We propose that there is much richer information that users\ncan easily provide and that robots ought to leverage. We focus on augmenting\ncomparisons with feature queries, and introduce a unified formalism for\ntreating all answers as observations about the true desired reward. We derive\nan active query selection algorithm, and test these queries in simulation and\non real users. We find that richer, feature-augmented queries can extract more\ninformation faster, leading to robots that better match user preferences in\ntheir behavior."}, {"title": "Augmented Artificial Intelligence: a Conceptual Framework", "abstract": "All artificial Intelligence (AI) systems make errors. These errors are\nunexpected, and differ often from the typical human mistakes (\"non-human\"\nerrors). The AI errors should be corrected without damage of existing skills\nand, hopefully, avoiding direct human expertise. This paper presents an initial\nsummary report of project taking new and systematic approach to improving the\nintellectual effectiveness of the individual AI by communities of AIs. We\ncombine some ideas of learning in heterogeneous multiagent systems with new and\noriginal mathematical approaches for non-iterative corrections of errors of\nlegacy AI systems. The mathematical foundations of AI non-destructive\ncorrection are presented and a series of new stochastic separation theorems is\nproven. These theorems provide a new instrument for the development, analysis,\nand assessment of machine learning methods and algorithms in high dimension.\nThey demonstrate that in high dimensions and even for exponentially large\nsamples, linear classifiers in their classical Fisher's form are powerful\nenough to separate errors from correct responses with high probability and to\nprovide efficient solution to the non-destructive corrector problem. In\nparticular, we prove some hypotheses formulated in our paper `Stochastic\nSeparation Theorems' (Neural Networks, 94, 255--259, 2017), and answer one\ngeneral problem published by Donoho and Tanner in 2009."}, {"title": "Evolutionary Computation plus Dynamic Programming for the Bi-Objective Travelling Thief Problem", "abstract": "This research proposes a novel indicator-based hybrid evolutionary approach\nthat combines approximate and exact algorithms. We apply it to a new\nbi-criteria formulation of the travelling thief problem, which is known to the\nEvolutionary Computation community as a benchmark multi-component optimisation\nproblem that interconnects two classical NP-hard problems: the travelling\nsalesman problem and the 0-1 knapsack problem. Our approach employs the exact\ndynamic programming algorithm for the underlying Packing-While-Travelling (PWT)\nproblem as a subroutine within a bi-objective evolutionary algorithm. This\ndesign takes advantage of the data extracted from Pareto fronts generated by\nthe dynamic program to achieve better solutions. Furthermore, we develop a\nnumber of novel indicators and selection mechanisms to strengthen synergy of\nthe two algorithmic components of our approach. The results of computational\nexperiments show that the approach is capable to outperform the\nstate-of-the-art results for the single-objective case of the problem."}, {"title": "Efficient Learning of Bounded-Treewidth Bayesian Networks from Complete and Incomplete Data Sets", "abstract": "Learning a Bayesian networks with bounded treewidth is important for reducing\nthe complexity of the inferences. We present a novel anytime algorithm (k-MAX)\nmethod for this task, which scales up to thousands of variables. Through\nextensive experiments we show that it consistently yields higher-scoring\nstructures than its competitors on complete data sets. We then consider the\nproblem of structure learning from incomplete data sets. This can be addressed\nby structural EM, which however is computationally very demanding. We thus\nadopt the novel k-MAX algorithm in the maximization step of structural EM,\nobtaining an efficient computation of the expected sufficient statistics. We\ntest the resulting structural EM method on the task of imputing missing data,\ncomparing it against the state-of-the-art approach based on random forests. Our\napproach achieves the same imputation accuracy of the competitors, but in about\none tenth of the time. Furthermore we show that it has worst-case complexity\nlinear in the input size, and that it is easily parallelizable."}, {"title": "Balancing Two-Player Stochastic Games with Soft Q-Learning", "abstract": "Within the context of video games the notion of perfectly rational agents can\nbe undesirable as it leads to uninteresting situations, where humans face tough\nadversarial decision makers. Current frameworks for stochastic games and\nreinforcement learning prohibit tuneable strategies as they seek optimal\nperformance. In this paper, we enable such tuneable behaviour by generalising\nsoft Q-learning to stochastic games, where more than one agent interact\nstrategically. We contribute both theoretically and empirically. On the theory\nside, we show that games with soft Q-learning exhibit a unique value and\ngeneralise team games and zero-sum games far beyond these two extremes to cover\na continuous spectrum of gaming behaviour. Experimentally, we show how tuning\nagents' constraints affect performance and demonstrate, through a neural\nnetwork architecture, how to reliably balance games with high-dimensional\nrepresentations."}, {"title": "Narrow Artificial Intelligence with Machine Learning for Real-Time Estimation of a Mobile Agents Location Using Hidden Markov Models", "abstract": "We propose to use a supervised machine learning technique to track the\nlocation of a mobile agent in real time. Hidden Markov Models are used to build\nartificial intelligence that estimates the unknown position of a mobile target\nmoving in a defined environment. This narrow artificial intelligence performs\ntwo distinct tasks. First, it provides real-time estimation of the mobile\nagent's position using the forward algorithm. Second, it uses the Baum-Welch\nalgorithm as a statistical learning tool to gain knowledge of the mobile\ntarget. Finally, an experimental environment is proposed, namely a video game\nthat we use to test our artificial intelligence. We present statistical and\ngraphical results to illustrate the efficiency of our method."}, {"title": "More Robust Doubly Robust Off-policy Evaluation", "abstract": "We study the problem of off-policy evaluation (OPE) in reinforcement learning\n(RL), where the goal is to estimate the performance of a policy from the data\ngenerated by another policy(ies). In particular, we focus on the doubly robust\n(DR) estimators that consist of an importance sampling (IS) component and a\nperformance model, and utilize the low (or zero) bias of IS and low variance of\nthe model at the same time. Although the accuracy of the model has a huge\nimpact on the overall performance of DR, most of the work on using the DR\nestimators in OPE has been focused on improving the IS part, and not much on\nhow to learn the model. In this paper, we propose alternative DR estimators,\ncalled more robust doubly robust (MRDR), that learn the model parameter by\nminimizing the variance of the DR estimator. We first present a formulation for\nlearning the DR model in RL. We then derive formulas for the variance of the DR\nestimator in both contextual bandits and RL, such that their gradients\nw.r.t.~the model parameters can be estimated from the samples, and propose\nmethods to efficiently minimize the variance. We prove that the MRDR estimators\nare strongly consistent and asymptotically optimal. Finally, we evaluate MRDR\nin bandits and RL benchmark problems, and compare its performance with the\nexisting methods."}, {"title": "Graph Planning with Expected Finite Horizon", "abstract": "Graph planning gives rise to fundamental algorithmic questions such as\nshortest path, traveling salesman problem, etc. A classical problem in discrete\nplanning is to consider a weighted graph and construct a path that maximizes\nthe sum of weights for a given time horizon $T$. However, in many scenarios,\nthe time horizon is not fixed, but the stopping time is chosen according to\nsome distribution such that the expected stopping time is $T$. If the stopping\ntime distribution is not known, then to ensure robustness, the distribution is\nchosen by an adversary, to represent the worst-case scenario.\n  A stationary plan for every vertex always chooses the same outgoing edge. For\nfixed horizon or fixed stopping-time distribution, stationary plans are not\nsufficient for optimality. Quite surprisingly we show that when an adversary\nchooses the stopping-time distribution with expected stopping time $T$, then\nstationary plans are sufficient. While computing optimal stationary plans for\nfixed horizon is NP-complete, we show that computing optimal stationary plans\nunder adversarial stopping-time distribution can be achieved in polynomial\ntime. Consequently, our polynomial-time algorithm for adversarial stopping time\nalso computes an optimal plan among all possible plans."}, {"title": "Distinguishing Question Subjectivity from Difficulty for Improved Crowdsourcing", "abstract": "The questions in a crowdsourcing task typically exhibit varying degrees of\ndifficulty and subjectivity. Their joint effects give rise to the variation in\nresponses to the same question by different crowd-workers. This variation is\nlow when the question is easy to answer and objective, and high when it is\ndifficult and subjective. Unfortunately, current quality control methods for\ncrowdsourcing consider only the question difficulty to account for the\nvariation. As a result,these methods cannot distinguish workers personal\npreferences for different correct answers of a partially subjective question\nfrom their ability/expertise to avoid objectively wrong answers for that\nquestion. To address this issue, we present a probabilistic model which (i)\nexplicitly encodes question difficulty as a model parameter and (ii) implicitly\nencodes question subjectivity via latent preference factors for crowd-workers.\nWe show that question subjectivity induces grouping of crowd-workers, revealed\nthrough clustering of their latent preferences. Moreover, we develop a\nquantitative measure of the subjectivity of a question. Experiments show that\nour model(1) improves the performance of both quality control for crowd-sourced\nanswers and next answer prediction for crowd-workers,and (2) can potentially\nprovide coherent rankings of questions in terms of their difficulty and\nsubjectivity, so that task providers can refine their designs of the\ncrowdsourcing tasks, e.g. by removing highly subjective questions or\ninappropriately difficult questions."}, {"title": "The Complex Event Recognition Group", "abstract": "The Complex Event Recognition (CER) group is a research team, affiliated with\nthe National Centre of Scientific Research \"Demokritos\" in Greece. The CER\ngroup works towards advanced and efficient methods for the recognition of\ncomplex events in a multitude of large, heterogeneous and interdependent data\nstreams. Its research covers multiple aspects of complex event recognition,\nfrom efficient detection of patterns on event streams to handling uncertainty\nand noise in streams, and machine learning techniques for inferring interesting\npatterns. Lately, it has expanded to methods for forecasting the occurrence of\nevents. It was founded in 2009 and currently hosts 3 senior researchers, 5 PhD\nstudents and works regularly with under-graduate students."}, {"title": "Reasoning in a Hierarchical System with Missing Group Size Information", "abstract": "The paper analyzes the problem of judgments or preferences subsequent to\ninitial analysis by autonomous agents in a hierarchical system where the higher\nlevel agents does not have access to group size information. We propose methods\nthat reduce instances of preference reversal of the kind encountered in\nSimpson's paradox."}, {"title": "A New Multi Criteria Decision Making Method: Approach of Logarithmic Concept (APLOCO)", "abstract": "The primary aim of the study is to introduce APLOCO method which is developed\nfor the solution of multicriteria decision making problems both theoretically\nand practically. In this context, application subject of APLACO constitutes\nevaluation of investment potential of different cities in metropolitan status\nin Turkey. The secondary purpose of the study is to identify the independent\nvariables affecting the factories in the operating phase and to estimate the\neffect levels of independent variables on the dependent variable in the\norganized industrial zones (OIZs), whose mission is to reduce regional\ndevelopment disparities and to mobilize local production dynamics. For this\npurpose, the effect levels of independent variables on dependent variables have\nbeen determined using the multilayer perceptron (MLP) method, which has a wide\nuse in artificial neural networks (ANNs). The effect levels derived from MLP\nhave been then used as the weight levels of the decision criteria in APLOCO.\nThe independent variables included in MLP are also used as the decision\ncriteria in APLOCO. According to the results obtained from APLOCO, Istanbul\ncity is the best alternative in term of the investment potential and other\nalternatives are Manisa, Denizli, Izmir, Kocaeli, Bursa, Ankara, Adana, and\nAntalya, respectively. Although APLOCO is used to solve the ranking problem in\norder to show application process in the paper, it can be employed easily in\nthe solution of classification and selection problems. On the other hand, the\nstudy also shows a rare example of the nested usage of APLOCO which is one of\nthe methods of operation research as well as MLP used in determination of\nweights."}, {"title": "Blockchain and Artificial Intelligence", "abstract": "It is undeniable that artificial intelligence (AI) and blockchain concepts\nare spreading at a phenomenal rate. Both technologies have distinct degree of\ntechnological complexity and multi-dimensional business implications. However,\na common misunderstanding about blockchain concept, in particular, is that\nblockchain is decentralized and is not controlled by anyone. But the underlying\ndevelopment of a blockchain system is still attributed to a cluster of core\ndevelopers. Take smart contract as an example, it is essentially a collection\nof codes (or functions) and data (or states) that are programmed and deployed\non a blockchain (say, Ethereum) by different human programmers. It is thus,\nunfortunately, less likely to be free of loopholes and flaws. In this article,\nthrough a brief overview about how artificial intelligence could be used to\ndeliver bug-free smart contract so as to achieve the goal of blockchain 2.0, we\nto emphasize that the blockchain implementation can be assisted or enhanced via\nvarious AI techniques. The alliance of AI and blockchain is expected to create\nnumerous possibilities."}, {"title": "Learning Robust and Adaptive Real-World Continuous Control Using Simulation and Transfer Learning", "abstract": "We use model-free reinforcement learning, extensive simulation, and transfer\nlearning to develop a continuous control algorithm that has good zero-shot\nperformance in a real physical environment. We train a simulated agent to act\noptimally across a set of similar environments, each with dynamics drawn from a\nprior distribution. We propose that the agent is able to adjust its actions\nalmost immediately, based on small set of observations. This robust and\nadaptive behavior is enabled by using a policy gradient algorithm with an Long\nShort Term Memory (LSTM) function approximation. Finally, we train an agent to\nnavigate a two-dimensional environment with uncertain dynamics and noisy\nobservations. We demonstrate that this agent has good zero-shot performance in\na real physical environment. Our preliminary results indicate that the agent is\nable to infer the environmental dynamics after only a few timesteps, and adjust\nits actions accordingly."}, {"title": "A Deep Reinforcement Learning Framework for Rebalancing Dockless Bike Sharing Systems", "abstract": "Bike sharing provides an environment-friendly way for traveling and is\nbooming all over the world. Yet, due to the high similarity of user travel\npatterns, the bike imbalance problem constantly occurs, especially for dockless\nbike sharing systems, causing significant impact on service quality and company\nrevenue. Thus, it has become a critical task for bike sharing systems to\nresolve such imbalance efficiently. In this paper, we propose a novel deep\nreinforcement learning framework for incentivizing users to rebalance such\nsystems. We model the problem as a Markov decision process and take both\nspatial and temporal features into consideration. We develop a novel deep\nreinforcement learning algorithm called Hierarchical Reinforcement Pricing\n(HRP), which builds upon the Deep Deterministic Policy Gradient algorithm.\nDifferent from existing methods that often ignore spatial information and rely\nheavily on accurate prediction, HRP captures both spatial and temporal\ndependencies using a divide-and-conquer structure with an embedded localized\nmodule. We conduct extensive experiments to evaluate HRP, based on a dataset\nfrom Mobike, a major Chinese dockless bike sharing company. Results show that\nHRP performs close to the 24-timeslot look-ahead optimization, and outperforms\nstate-of-the-art methods in both service level and bike distribution. It also\ntransfers well when applied to unseen areas."}, {"title": "Story Generation and Aviation Incident Representation", "abstract": "This working note discusses the topic of story generation, with a view to\nidentifying the knowledge required to understand aviation incident narratives\n(which have structural similarities to stories), following the premise that to\nunderstand aviation incidents, one should at least be able to generate examples\nof them. We give a brief overview of aviation incidents and their relation to\nstories, and then describe two of our earlier attempts (using `scripts' and\n`story grammars') at incident generation which did not evolve promisingly.\nFollowing this, we describe a simple incident generator which did work (at a\n`toy' level), using a `world simulation' approach. This generator is based on\nMeehan's TALE-SPIN story generator (1977). We conclude with a critique of the\napproach."}, {"title": "Morphologic for knowledge dynamics: revision, fusion, abduction", "abstract": "Several tasks in artificial intelligence require to be able to find models\nabout knowledge dynamics. They include belief revision, fusion and belief\nmerging, and abduction. In this paper we exploit the algebraic framework of\nmathematical morphology in the context of propositional logic, and define\noperations such as dilation or erosion of a set of formulas. We derive concrete\noperators, based on a semantic approach, that have an intuitive interpretation\nand that are formally well behaved, to perform revision, fusion and abduction.\nComputation and tractability are addressed, and simple examples illustrate the\ntypical results that can be obtained."}, {"title": "Who Killed Albert Einstein? From Open Data to Murder Mystery Games", "abstract": "This paper presents a framework for generating adventure games from open\ndata. Focusing on the murder mystery type of adventure games, the generator is\nable to transform open data from Wikipedia articles, OpenStreetMap and images\nfrom Wikimedia Commons into WikiMysteries. Every WikiMystery game revolves\naround the murder of a person with a Wikipedia article and populates the game\nwith suspects who must be arrested by the player if guilty of the murder or\nabsolved if innocent. Starting from only one person as the victim, an extensive\ngenerative pipeline finds suspects, their alibis, and paths connecting them\nfrom open data, transforms open data into cities, buildings, non-player\ncharacters, locks and keys and dialog options. The paper describes in detail\neach generative step, provides a specific playthrough of one WikiMystery where\nAlbert Einstein is murdered, and evaluates the outcomes of games generated for\nthe 100 most influential people of the 20th century."}, {"title": "From Gameplay to Symbolic Reasoning: Learning SAT Solver Heuristics in the Style of Alpha(Go) Zero", "abstract": "Despite the recent successes of deep neural networks in various fields such\nas image and speech recognition, natural language processing, and reinforcement\nlearning, we still face big challenges in bringing the power of numeric\noptimization to symbolic reasoning. Researchers have proposed different avenues\nsuch as neural machine translation for proof synthesis, vectorization of\nsymbols and expressions for representing symbolic patterns, and coupling of\nneural back-ends for dimensionality reduction with symbolic front-ends for\ndecision making. However, these initial explorations are still only point\nsolutions, and bear other shortcomings such as lack of correctness guarantees.\nIn this paper, we present our approach of casting symbolic reasoning as games,\nand directly harnessing the power of deep reinforcement learning in the style\nof Alpha(Go) Zero on symbolic problems. Using the Boolean Satisfiability (SAT)\nproblem as showcase, we demonstrate the feasibility of our method, and the\nadvantages of modularity, efficiency, and correctness guarantees."}, {"title": "Reliable Uncertain Evidence Modeling in Bayesian Networks by Credal Networks", "abstract": "A reliable modeling of uncertain evidence in Bayesian networks based on a\nset-valued quantification is proposed. Both soft and virtual evidences are\nconsidered. We show that evidence propagation in this setup can be reduced to\nstandard updating in an augmented credal network, equivalent to a set of\nconsistent Bayesian networks. A characterization of the computational\ncomplexity for this task is derived together with an efficient exact procedure\nfor a subclass of instances. In the case of multiple uncertain evidences over\nthe same variable, the proposed procedure can provide a set-valued version of\nthe geometric approach to opinion pooling."}, {"title": "An Anytime Algorithm for Task and Motion MDPs", "abstract": "Integrated task and motion planning has emerged as a challenging problem in\nsequential decision making, where a robot needs to compute high-level strategy\nand low-level motion plans for solving complex tasks. While high-level\nstrategies require decision making over longer time-horizons and scales, their\nfeasibility depends on low-level constraints based upon the geometries and\ncontinuous dynamics of the environment. The hybrid nature of this problem makes\nit difficult to scale; most existing approaches focus on deterministic, fully\nobservable scenarios. We present a new approach where the high-level decision\nproblem occurs in a stochastic setting and can be modeled as a Markov decision\nprocess. In contrast to prior efforts, we show that complete MDP policies, or\ncontingent behaviors, can be computed effectively in an anytime fashion. Our\nalgorithm continuously improves the quality of the solution and is guaranteed\nto be probabilistically complete. We evaluate the performance of our approach\non a challenging, realistic test problem: autonomous aircraft inspection. Our\nresults show that we can effectively compute consistent task and motion\npolicies for the most likely execution-time outcomes using only a fraction of\nthe computation required to develop the complete task and motion policy."}, {"title": "Detecting truth, just on parts", "abstract": "We introduce and discuss, through a computational algebraic geometry\napproach, the automatic reasoning handling of propositions that are\nsimultaneously true and false over some relevant collections of instances. A\nrigorous, algorithmic criterion is presented for detecting such cases, and its\nperformance is exemplified through the implementation of this test on the\ndynamic geometry program GeoGebra."}, {"title": "Monte Carlo Q-learning for General Game Playing", "abstract": "After the recent groundbreaking results of AlphaGo, we have seen a strong\ninterest in reinforcement learning in game playing. General Game Playing (GGP)\nprovides a good testbed for reinforcement learning. In GGP, a specification of\ngames rules is given. GGP problems can be solved by reinforcement learning.\nQ-learning is one of the canonical reinforcement learning methods, and has been\nused by (Banerjee & Stone, IJCAI 2007) in GGP. In this paper we implement\nQ-learning in GGP for three small-board games (Tic-Tac-Toe, Connect Four, Hex),\nto allow comparison to Banerjee et al. As expected, Q-learning converges,\nalthough much slower than MCTS. Borrowing an idea from MCTS, we enhance\nQ-learning with Monte Carlo Search, to give QM-learning. This enhancement\nimproves the performance of pure Q-learning. We believe that QM-learning can\nalso be used to improve performance of reinforcement learning further for\nlarger games, something which we will test in future work."}, {"title": "Artificial intelligence and pediatrics: A synthetic mini review", "abstract": "The use of artificial intelligence intelligencein medicine can be traced back\nto 1968 when Paycha published his paper Le diagnostic a l'aide d'intelligences\nartificielle, presentation de la premiere machine diagnostri. Few years later\nShortliffe et al. presented an expert system named Mycin which was able to\nidentify bacteria causing severe blood infections and to recommend antibiotics.\nDespite the fact that Mycin outperformed members of the Stanford medical school\nin the reliability of diagnosis it was never used in practice due to a legal\nissue who do you sue if it gives a wrong diagnosis?. However only in 2016 when\nthe artificial intelligence software built into the IBM Watson AI platform\ncorrectly diagnosed and proposed an effective treatment for a 60-year-old\nwomans rare form of leukemia the AI use in medicine become really popular.On of\nfirst papers presenting the use of AI in paediatrics was published in 1984. The\npaper introduced a computer-assisted medical decision making system called\nSHELP."}, {"title": "A Unified Framework for Planning in Adversarial and Cooperative Environments", "abstract": "Users of AI systems may rely upon them to produce plans for achieving desired\nobjectives. Such AI systems should be able to compute obfuscated plans whose\nexecution in adversarial situations protects privacy, as well as legible plans\nwhich are easy for team members to understand in cooperative situations. We\ndevelop a unified framework that addresses these dual problems by computing\nplans with a desired level of comprehensibility from the point of view of a\npartially informed observer. For adversarial settings, our approach produces\nobfuscated plans with observations that are consistent with at least k goals\nfrom a set of decoy goals. By slightly varying our framework, we present an\napproach for goal legibility in cooperative settings which produces plans that\nachieve a goal while being consistent with at most j goals from a set of\nconfounding goals. In addition, we show how the observability of the observer\ncan be controlled to either obfuscate or clarify the next actions in a plan\nwhen the goal is known to the observer. We present theoretical results on the\ncomplexity analysis of our problems. We demonstrate the execution of obfuscated\nand legible plans in a cooking domain using a physical robot Fetch. We also\nprovide an empirical evaluation to show the feasibility and usefulness of our\napproaches using IPC domains."}, {"title": "HyP-DESPOT: A Hybrid Parallel Algorithm for Online Planning under Uncertainty", "abstract": "Planning under uncertainty is critical for robust robot performance in\nuncertain, dynamic environments, but it incurs high computational cost.\nState-of-the-art online search algorithms, such as DESPOT, have vastly improved\nthe computational efficiency of planning under uncertainty and made it a\nvaluable tool for robotics in practice. This work takes one step further by\nleveraging both CPU and GPU parallelization in order to achieve near real-time\nonline planning performance for complex tasks with large state, action, and\nobservation spaces. Specifically, we propose Hybrid Parallel DESPOT\n(HyP-DESPOT), a massively parallel online planning algorithm that integrates\nCPU and GPU parallelism in a multi-level scheme. It performs parallel DESPOT\ntree search by simultaneously traversing multiple independent paths using\nmulti-core CPUs and performs parallel Monte-Carlo simulations at the leaf nodes\nof the search tree using GPUs. Experimental results show that HyP-DESPOT speeds\nup online planning by up to several hundred times, compared with the original\nDESPOT algorithm, in several challenging robotic tasks in simulation."}, {"title": "Large Neighborhood-Based Metaheuristic and Branch-and-Price for the Pickup and Delivery Problem with Split Loads", "abstract": "We consider the multi-vehicle one-to-one pickup and delivery problem with\nsplit loads, a NP-hard problem linked with a variety of applications for bulk\nproduct transportation, bike-sharing systems and inventory re-balancing. This\nproblem is notoriously difficult due to the interaction of two challenging\nvehicle routing attributes, \"pickups and deliveries\" and \"split deliveries\".\nThis possibly leads to optimal solutions of a size that grows exponentially\nwith the instance size, containing multiple visits per customer pair, even in\nthe same route. To solve this problem, we propose an iterated local search\nmetaheuristic as well as a branch-and-price algorithm. The core of the\nmetaheuristic consists of a new large neighborhood search, which reduces the\nproblem of finding the best insertion combination of a pickup and delivery pair\ninto a route (with possible splits) to a resource-constrained shortest path and\nknapsack problem. Similarly, the branch-and-price algorithm uses sophisticated\nlabeling techniques, route relaxations, pre-processing and branching rules for\nan efficient resolution. Our computational experiments on classical\nsingle-vehicle instances demonstrate the excellent performance of the\nmetaheuristic, which produces new best known solutions for 92 out of 93 test\ninstances, and outperforms all previous algorithms. Experimental results on new\nmulti-vehicle instances with distance constraints are also reported. The\nbranch-and-price algorithm produces optimal solutions for instances with up to\n20 pickup-and-delivery pairs, and very accurate solutions are found by the\nmetaheuristic."}, {"title": "A Machine Learning Approach to Air Traffic Route Choice Modelling", "abstract": "Air Traffic Flow and Capacity Management (ATFCM) is one of the constituent\nparts of Air Traffic Management (ATM). The goal of ATFCM is to make airport and\nairspace capacity meet traffic demand and, when capacity opportunities are\nexhausted, optimise traffic flows to meet the available capacity. One of the\nkey enablers of ATFCM is the accurate estimation of future traffic demand. The\navailable information (schedules, flight plans, etc.) and its associated level\nof uncertainty differ across the different ATFCM planning phases, leading to\nqualitative differences between the types of forecasting that are feasible at\neach time horizon. While abundant research has been conducted on tactical\ntrajectory prediction (i.e., during the day of operations), trajectory\nprediction in the pre-tactical phase, when few or no flight plans are\navailable, has received much less attention. As a consequence, the methods\ncurrently in use for pre-tactical traffic forecast are still rather\nrudimentary, often resulting in suboptimal ATFCM decision making. This paper\nproposes a machine learning approach for the prediction of airlines route\nchoices between two airports as a function of route characteristics, such as\nflight efficiency, air navigation charges and expected level of congestion.\nDifferent predictive models based on multinomial logistic regression and\ndecision trees are formulated and calibrated with historical traffic data, and\na critical evaluation of each model is conducted. We analyse the predictive\npower of each model in terms of its ability to forecast traffic volumes at the\nlevel of charging zones, proving significant potential to enhance pre-tactical\ntraffic forecast. We conclude by discussing the limitations and room for\nimprovement of the proposed approach, as well as the future developments\nrequired to produce reliable traffic forecasts at a higher spatial and temporal\nresolution."}, {"title": "Learning High-level Representations from Demonstrations", "abstract": "Hierarchical learning (HL) is key to solving complex sequential decision\nproblems with long horizons and sparse rewards. It allows learning agents to\nbreak-up large problems into smaller, more manageable subtasks. A common\napproach to HL, is to provide the agent with a number of high-level skills that\nsolve small parts of the overall problem. A major open question, however, is\nhow to identify a suitable set of reusable skills. We propose a principled\napproach that uses human demonstrations to infer a set of subgoals based on\nchanges in the demonstration dynamics. Using these subgoals, we decompose the\nlearning problem into an abstract high-level representation and a set of\nlow-level subtasks. The abstract description captures the overall problem\nstructure, while subtasks capture desired skills. We demonstrate that we can\njointly optimize over both levels of learning. We show that the resulting\nmethod significantly outperforms previous baselines on two challenging\nproblems: the Atari 2600 game Montezuma's Revenge, and a simulated robotics\nproblem moving the ant robot through a maze."}, {"title": "Analysis of cause-effect inference by comparing regression errors", "abstract": "We address the problem of inferring the causal direction between two\nvariables by comparing the least-squares errors of the predictions in both\npossible directions. Under the assumption of an independence between the\nfunction relating cause and effect, the conditional noise distribution, and the\ndistribution of the cause, we show that the errors are smaller in causal\ndirection if both variables are equally scaled and the causal relation is close\nto deterministic. Based on this, we provide an easily applicable algorithm that\nonly requires a regression in both possible causal directions and a comparison\nof the errors. The performance of the algorithm is compared with various\nrelated causal inference methods in different artificial and real-world data\nsets."}, {"title": "The problem of the development ontology-driven architecture of intellectual software systems", "abstract": "The paper describes the architecture of the intelligence system for automated\ndesign of ontological knowledge bases of domain areas and the software model of\nthe management GUI (Graphical User Interface) subsystem"}, {"title": "Design and software implementation of subsystems for creating and using the ontological base of a research scientist", "abstract": "Creation of the information systems and tools for scientific research and\ndevelopment support has always been one of the central directions of the\ndevelopment of computer science. The main features of the modern evolution of\nscientific research and development are the transdisciplinary approach and the\ndeep intellectualisation of all stages of the life cycle of formulation and\nsolution of scientific problems. The theoretical and practical aspects of the\ndevelopment of perspective complex knowledge-oriented information systems and\ntheir components are considered in the paper. The analysis of existing\nscientific information systems (or current research information systems, CRIS)\nand synthesis of general principles of design of the research and development\nworkstation environment of a researcher and its components are carried out in\nthe work. The functional components of knowledge-oriented information system\nresearch and development workstation environment of a researcher are designed.\nDesigned and developed functional components of knowledge-oriented information\nsystem developing research and development workstation environment,including\nfunctional models and software implementation of the software subsystem for\ncreation and use of ontological knowledge base for research fellow\npublications, as part of personalized knowledge base of scientific researcher.\nResearch in modern conditions of e-Science paradigm requires pooling scientific\ncommunity and intensive exchange of research results that may be achieved\nthrough the use of scientific information systems. research and development\nworkstation environment allows to solve problems of contructivisation and\nformalisation of knowledge representation, obtained during the research process\nand collective accomplices interaction."}, {"title": "Technique for designing a domain ontology", "abstract": "The article describes the technique for designing a domain ontology, shows\nthe flowchart of algorithm design and example of constructing a fragment of the\nontology of the subject area of Computer Science is considered."}, {"title": "Integrated Tools for Engineering Ontologies", "abstract": "The article presents an overview of current specialized ontology engineering\ntools, as well as texts' annotation tools based on ontologies. The main\nfunctions and features of these tools, their advantages and disadvantages are\ndiscussed. A systematic comparative analysis of means for engineering\nontologies is presented."}, {"title": "Principles of design and software development models of ontological-driven computer systems", "abstract": "This paper describes the design principles of methodology of\nknowledge-oriented information systems based on ontological approach. Such\nsystems implement technology subject-oriented extraction of knowledge from the\nset of natural language texts and their formal and logical presentation and\napplication processing"}, {"title": "Polynomial-based rotation invariant features", "abstract": "One of basic difficulties of machine learning is handling unknown rotations\nof objects, for example in image recognition. A related problem is evaluation\nof similarity of shapes, for example of two chemical molecules, for which\ndirect approach requires costly pairwise rotation alignment and comparison.\nRotation invariants are useful tools for such purposes, allowing to extract\nfeatures describing shape up to rotation, which can be used for example to\nsearch for similar rotated patterns, or fast evaluation of similarity of shapes\ne.g. for virtual screening, or machine learning including features directly\ndescribing shape. A standard approach are rotationally invariant cylindrical or\nspherical harmonics, which can be seen as based on polynomials on sphere,\nhowever, they provide very few invariants - only one per degree of polynomial.\nThere will be discussed a general approach to construct arbitrarily large sets\nof rotation invariants of polynomials, for degree $D$ in $\\mathbb{R}^n$ up to\n$O(n^D)$ independent invariants instead of $O(D)$ offered by standard\napproaches, possibly also a complete set: providing not only necessary, but\nalso sufficient condition for differing only by rotation (and reflectional\nsymmetry)."}, {"title": "Predicting Chronic Disease Hospitalizations from Electronic Health Records: An Interpretable Classification Approach", "abstract": "Urban living in modern large cities has significant adverse effects on\nhealth, increasing the risk of several chronic diseases. We focus on the two\nleading clusters of chronic disease, heart disease and diabetes, and develop\ndata-driven methods to predict hospitalizations due to these conditions. We\nbase these predictions on the patients' medical history, recent and more\ndistant, as described in their Electronic Health Records (EHR). We formulate\nthe prediction problem as a binary classification problem and consider a\nvariety of machine learning methods, including kernelized and sparse Support\nVector Machines (SVM), sparse logistic regression, and random forests. To\nstrike a balance between accuracy and interpretability of the prediction, which\nis important in a medical setting, we propose two novel methods: K-LRT, a\nlikelihood ratio test-based method, and a Joint Clustering and Classification\n(JCC) method which identifies hidden patient clusters and adapts classifiers to\neach cluster. We develop theoretical out-of-sample guarantees for the latter\nmethod. We validate our algorithms on large datasets from the Boston Medical\nCenter, the largest safety-net hospital system in New England."}, {"title": "Learning $3$D-FilterMap for Deep Convolutional Neural Networks", "abstract": "We present a novel and compact architecture for deep Convolutional Neural\nNetworks (CNNs) in this paper, termed $3$D-FilterMap Convolutional Neural\nNetworks ($3$D-FM-CNNs). The convolution layer of $3$D-FM-CNN learns a compact\nrepresentation of the filters, named $3$D-FilterMap, instead of a set of\nindependent filters in the conventional convolution layer. The filters are\nextracted from the $3$D-FilterMap as overlapping $3$D submatrics with weight\nsharing among nearby filters, and these filters are convolved with the input to\ngenerate the output of the convolution layer for $3$D-FM-CNN. Due to the weight\nsharing scheme, the parameter size of the $3$D-FilterMap is much smaller than\nthat of the filters to be learned in the conventional convolution layer when\n$3$D-FilterMap generates the same number of filters. Our work is fundamentally\ndifferent from the network compression literature that reduces the size of a\nlearned large network in the sense that a small network is directly learned\nfrom scratch. Experimental results demonstrate that $3$D-FM-CNN enjoys a small\nparameter space by learning compact $3$D-FilterMaps, while achieving\nperformance compared to that of the baseline CNNs which learn the same number\nof filters as that generated by the corresponding $3$D-FilterMap."}, {"title": "Deep Bidirectional and Unidirectional LSTM Recurrent Neural Network for Network-wide Traffic Speed Prediction", "abstract": "Short-term traffic forecasting based on deep learning methods, especially\nlong short-term memory (LSTM) neural networks, has received much attention in\nrecent years. However, the potential of deep learning methods in traffic\nforecasting has not yet fully been exploited in terms of the depth of the model\narchitecture, the spatial scale of the prediction area, and the predictive\npower of spatial-temporal data. In this paper, a deep stacked bidirectional and\nunidirectional LSTM (SBU- LSTM) neural network architecture is proposed, which\nconsiders both forward and backward dependencies in time series data, to\npredict network-wide traffic speed. A bidirectional LSTM (BDLSM) layer is\nexploited to capture spatial features and bidirectional temporal dependencies\nfrom historical data. To the best of our knowledge, this is the first time that\nBDLSTMs have been applied as building blocks for a deep architecture model to\nmeasure the backward dependency of traffic data for prediction. The proposed\nmodel can handle missing values in input data by using a masking mechanism.\nFurther, this scalable model can predict traffic speed for both freeway and\ncomplex urban traffic networks. Comparisons with other classical and\nstate-of-the-art models indicate that the proposed SBU-LSTM neural network\nachieves superior prediction performance for the whole traffic network in both\naccuracy and robustness."}, {"title": "Covariant Compositional Networks For Learning Graphs", "abstract": "Most existing neural networks for learning graphs address permutation\ninvariance by conceiving of the network as a message passing scheme, where each\nnode sums the feature vectors coming from its neighbors. We argue that this\nimposes a limitation on their representation power, and instead propose a new\ngeneral architecture for representing objects consisting of a hierarchy of\nparts, which we call Covariant Compositional Networks (CCNs). Here, covariance\nmeans that the activation of each neuron must transform in a specific way under\npermutations, similarly to steerability in CNNs. We achieve covariance by\nmaking each activation transform according to a tensor representation of the\npermutation group, and derive the corresponding tensor aggregation rules that\neach neuron must implement. Experiments show that CCNs can outperform competing\nmethods on standard graph learning benchmarks."}, {"title": "Theory of Deep Learning IIb: Optimization Properties of SGD", "abstract": "In Theory IIb we characterize with a mix of theory and experiments the\noptimization of deep convolutional networks by Stochastic Gradient Descent. The\nmain new result in this paper is theoretical and experimental evidence for the\nfollowing conjecture about SGD: SGD concentrates in probability -- like the\nclassical Langevin equation -- on large volume, \"flat\" minima, selecting flat\nminimizers which are with very high probability also global minimizers"}, {"title": "A Machine Learning Framework for Register Placement Optimization in Digital Circuit Design", "abstract": "In modern digital circuit back-end design, designers heavily rely on\nelectronic-design-automoation (EDA) tool to close timing. However, the\nheuristic algorithms used in the place and route tool usually does not result\nin optimal solution. Thus, significant design effort is used to tune parameters\nor provide user constraints or guidelines to improve the tool performance. In\nthis paper, we targeted at those optimization space left behind by the EDA\ntools and propose a machine learning framework that helps to define what are\nthe guidelines and constraints for registers placement, which can yield better\nperformance and quality for back-end design. In other words, the framework is\ntrying to learn what are the flaws of the existing EDA tools and tries to\noptimize it by providing additional information. We discuss what is the proper\ninput feature vector to be extracted, and what is metric to be used for\nreference output. We also develop a scheme to generate perturbed training\nsamples using existing design based on Gaussian randomization. By applying our\nmethodology, we are able to improve the design runtime by up to 36% and timing\nquality by up to 23%."}, {"title": "Graph Memory Networks for Molecular Activity Prediction", "abstract": "Molecular activity prediction is critical in drug design. Machine learning\ntechniques such as kernel methods and random forests have been successful for\nthis task. These models require fixed-size feature vectors as input while the\nmolecules are variable in size and structure. As a result, fixed-size\nfingerprint representation is poor in handling substructures for large\nmolecules. In addition, molecular activity tests, or a so-called BioAssays, are\nrelatively small in the number of tested molecules due to its complexity. Here\nwe approach the problem through deep neural networks as they are flexible in\nmodeling structured data such as grids, sequences and graphs. We train multiple\nBioAssays using a multi-task learning framework, which combines information\nfrom multiple sources to improve the performance of prediction, especially on\nsmall datasets. We propose Graph Memory Network (GraphMem), a memory-augmented\nneural network to model the graph structure in molecules. GraphMem consists of\na recurrent controller coupled with an external memory whose cells dynamically\ninteract and change through a multi-hop reasoning process. Applied to the\nmolecules, the dynamic interactions enable an iterative refinement of the\nrepresentation of molecular graphs with multiple bond types. GraphMem is\ncapable of jointly training on multiple datasets by using a specific-task query\nfed to the controller as an input. We demonstrate the effectiveness of the\nproposed model for separately and jointly training on more than 100K\nmeasurements, spanning across 9 BioAssay activity tests."}, {"title": "Modeling urbanization patterns with generative adversarial networks", "abstract": "In this study we propose a new method to simulate hyper-realistic urban\npatterns using Generative Adversarial Networks trained with a global urban\nland-use inventory. We generated a synthetic urban \"universe\" that\nqualitatively reproduces the complex spatial organization observed in global\nurban patterns, while being able to quantitatively recover certain key\nhigh-level urban spatial metrics."}, {"title": "Compressing Deep Neural Networks: A New Hashing Pipeline Using Kac's Random Walk Matrices", "abstract": "The popularity of deep learning is increasing by the day. However, despite\nthe recent advancements in hardware, deep neural networks remain\ncomputationally intensive. Recent work has shown that by preserving the angular\ndistance between vectors, random feature maps are able to reduce dimensionality\nwithout introducing bias to the estimator. We test a variety of established\nhashing pipelines as well as a new approach using Kac's random walk matrices.\nWe demonstrate that this method achieves similar accuracy to existing\npipelines."}, {"title": "eCommerceGAN : A Generative Adversarial Network for E-commerce", "abstract": "E-commerce companies such as Amazon, Alibaba and Flipkart process billions of\norders every year. However, these orders represent only a small fraction of all\nplausible orders. Exploring the space of all plausible orders could help us\nbetter understand the relationships between the various entities in an\ne-commerce ecosystem, namely the customers and the products they purchase. In\nthis paper, we propose a Generative Adversarial Network (GAN) for orders made\nin e-commerce websites. Once trained, the generator in the GAN could generate\nany number of plausible orders. Our contributions include: (a) creating a dense\nand low-dimensional representation of e-commerce orders, (b) train an\necommerceGAN (ecGAN) with real orders to show the feasibility of the proposed\nparadigm, and (c) train an ecommerce-conditional-GAN (ec^2GAN) to generate the\nplausible orders involving a particular product. We propose several qualitative\nmethods to evaluate ecGAN and demonstrate its effectiveness. The ec^2GAN is\nused for various kinds of characterization of possible orders involving a\nproduct that has just been introduced into the e-commerce system. The proposed\napproach ec^2GAN performs significantly better than the baseline in most of the\nscenarios."}, {"title": "Blessing of dimensionality: mathematical foundations of the statistical physics of data", "abstract": "The concentration of measure phenomena were discovered as the mathematical\nbackground of statistical mechanics at the end of the XIX - beginning of the XX\ncentury and were then explored in mathematics of the XX-XXI centuries. At the\nbeginning of the XXI century, it became clear that the proper utilisation of\nthese phenomena in machine learning might transform the curse of dimensionality\ninto the blessing of dimensionality.\n  This paper summarises recently discovered phenomena of measure concentration\nwhich drastically simplify some machine learning problems in high dimension,\nand allow us to correct legacy artificial intelligence systems. The classical\nconcentration of measure theorems state that i.i.d. random points are\nconcentrated in a thin layer near a surface (a sphere or equators of a sphere,\nan average or median level set of energy or another Lipschitz function, etc.).\n  The new stochastic separation theorems describe the thin structure of these\nthin layers: the random points are not only concentrated in a thin layer but\nare all linearly separable from the rest of the set, even for exponentially\nlarge random sets. The linear functionals for separation of points can be\nselected in the form of the linear Fisher's discriminant.\n  All artificial intelligence systems make errors. Non-destructive correction\nrequires separation of the situations (samples) with errors from the samples\ncorresponding to correct behaviour by a simple and robust classifier. The\nstochastic separation theorems provide us by such classifiers and a\nnon-iterative (one-shot) procedure for learning."}, {"title": "A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem", "abstract": "Bandit learning is characterized by the tension between long-term exploration\nand short-term exploitation. However, as has recently been noted, in settings\nin which the choices of the learning algorithm correspond to important\ndecisions about individual people (such as criminal recidivism prediction,\nlending, and sequential drug trials), exploration corresponds to explicitly\nsacrificing the well-being of one individual for the potential future benefit\nof others. This raises a fairness concern. In such settings, one might like to\nrun a \"greedy\" algorithm, which always makes the (myopically) optimal decision\nfor the individuals at hand - but doing this can result in a catastrophic\nfailure to learn. In this paper, we consider the linear contextual bandit\nproblem and revisit the performance of the greedy algorithm. We give a smoothed\nanalysis, showing that even when contexts may be chosen by an adversary, small\nperturbations of the adversary's choices suffice for the algorithm to achieve\n\"no regret\", perhaps (depending on the specifics of the setting) with a\nconstant amount of initial training data. This suggests that \"generically\"\n(i.e. in slightly perturbed environments), exploration and exploitation need\nnot be in conflict in the linear setting."}, {"title": "A Hardware-Friendly Algorithm for Scalable Training and Deployment of Dimensionality Reduction Models on FPGA", "abstract": "With ever-increasing application of machine learning models in various\ndomains such as image classification, speech recognition and synthesis, and\nhealth care, designing efficient hardware for these models has gained a lot of\npopularity. While the majority of researches in this area focus on efficient\ndeployment of machine learning models (a.k.a inference), this work concentrates\non challenges of training these models in hardware. In particular, this paper\npresents a high-performance, scalable, reconfigurable solution for both\ntraining and deployment of different dimensionality reduction models in\nhardware by introducing a hardware-friendly algorithm. Compared to\nstate-of-the-art implementations, our proposed algorithm and its hardware\nrealization decrease resource consumption by 50\\% without any degradation in\naccuracy."}, {"title": "Cost-Sensitive Convolution based Neural Networks for Imbalanced Time-Series Classification", "abstract": "Some deep convolutional neural networks were proposed for time-series\nclassification and class imbalanced problems. However, those models performed\ndegraded and even failed to recognize the minority class of an imbalanced\ntemporal sequences dataset. Minority samples would bring troubles for temporal\ndeep learning classifiers due to the equal treatments of majority and minority\nclass. Until recently, there were few works applying deep learning on\nimbalanced time-series classification (ITSC) tasks. Here, this paper aimed at\ntackling ITSC problems with deep learning. An adaptive cost-sensitive learning\nstrategy was proposed to modify temporal deep learning models. Through the\nproposed strategy, classifiers could automatically assign misclassification\npenalties to each class. In the experimental section, the proposed method was\nutilized to modify five neural networks. They were evaluated on a large volume,\nreal-life and imbalanced time-series dataset with six metrics. Each single\nnetwork was also tested alone and combined with several mainstream data\nsamplers. Experimental results illustrated that the proposed cost-sensitive\nmodified networks worked well on ITSC tasks. Compared to other methods, the\ncost-sensitive convolution neural network and residual network won out in the\nterms of all metrics. Consequently, the proposed cost-sensitive learning\nstrategy can be used to modify deep learning classifiers from cost-insensitive\nto cost-sensitive. Those cost-sensitive convolutional networks can be\neffectively applied to address ITSC issues."}, {"title": "Towards a more efficient representation of imputation operators in TPOT", "abstract": "Automated Machine Learning encompasses a set of meta-algorithms intended to\ndesign and apply machine learning techniques (e.g., model selection,\nhyperparameter tuning, model assessment, etc.). TPOT, a software for optimizing\nmachine learning pipelines based on genetic programming (GP), is a novel\nexample of this kind of applications. Recently we have proposed a way to\nintroduce imputation methods as part of TPOT. While our approach was able to\ndeal with problems with missing data, it can produce a high number of\nunfeasible pipelines. In this paper we propose a strongly-typed-GP based\napproach that enforces constraint satisfaction by GP solutions. The enhancement\nwe introduce is based on the redefinition of the operators and implicit\nenforcement of constraints in the generation of the GP trees. We evaluate the\nmethod to introduce imputation methods as part of TPOT. We show that the method\ncan notably increase the efficiency of the GP search for optimal pipelines."}, {"title": "Unsupervised Cipher Cracking Using Discrete GANs", "abstract": "This work details CipherGAN, an architecture inspired by CycleGAN used for\ninferring the underlying cipher mapping given banks of unpaired ciphertext and\nplaintext. We demonstrate that CipherGAN is capable of cracking language data\nenciphered using shift and Vigenere ciphers to a high degree of fidelity and\nfor vocabularies much larger than previously achieved. We present how CycleGAN\ncan be made compatible with discrete data and train in a stable way. We then\nprove that the technique used in CipherGAN avoids the common problem of\nuninformative discrimination associated with GANs applied to discrete data."}, {"title": "Rank Selection of CP-decomposed Convolutional Layers with Variational Bayesian Matrix Factorization", "abstract": "Convolutional Neural Networks (CNNs) is one of successful method in many\nareas such as image classification tasks. However, the amount of memory and\ncomputational cost needed for CNNs inference obstructs them to run efficiently\nin mobile devices because of memory and computational ability limitation. One\nof the method to compress CNNs is compressing the layers iteratively, i.e. by\nlayer-by-layer compression and fine-tuning, with CP-decomposition in\nconvolutional layers. To compress with CP-decomposition, rank selection is\nimportant. In the previous approach rank selection that is based on sensitivity\nof each layer, the average rank of the network was still arbitrarily selected.\nAdditionally, the rank of all layers were decided before whole process of\niterative compression, while the rank of a layer can be changed after\nfine-tuning. Therefore, this paper proposes selecting rank of each layer using\nVariational Bayesian Matrix Factorization (VBMF) which is more systematic than\narbitrary approach. Furthermore, to consider the change of each layer's rank\nafter fine-tuning of previous iteration, the method is applied just before\ncompressing the target layer, i.e. after fine-tuning of the previous iteration.\nThe results show better accuracy while also having more compression rate in\nAlexNet's convolutional layers compression."}, {"title": "ALE: Additive Latent Effect Models for Grade Prediction", "abstract": "The past decade has seen a growth in the development and deployment of\neducational technologies for assisting college-going students in choosing\nmajors, selecting courses and acquiring feedback based on past academic\nperformance. Grade prediction methods seek to estimate a grade that a student\nmay achieve in a course that she may take in the future (e.g., next term).\nAccurate and timely prediction of students' academic grades is important for\ndeveloping effective degree planners and early warning systems, and ultimately\nimproving educational outcomes. Existing grade pre- diction methods mostly\nfocus on modeling the knowledge components associated with each course and\nstudent, and often overlook other factors such as the difficulty of each\nknowledge component, course instructors, student interest, capabilities and\neffort. In this paper, we propose additive latent effect models that\nincorporate these factors to predict the student next-term grades.\nSpecifically, the proposed models take into account four factors: (i) student's\nacademic level, (ii) course instructors, (iii) student global latent factor,\nand (iv) latent knowledge factors. We compared the new models with several\nstate-of-the-art methods on students of various characteristics (e.g., whether\na student transferred in or not). The experimental results demonstrate that the\nproposed methods significantly outperform the baselines on grade prediction\nproblem. Moreover, we perform a thorough analysis on the importance of\ndifferent factors and how these factors can practically assist students in\ncourse selection, and finally improve their academic performance."}, {"title": "On the Reduction of Biases in Big Data Sets for the Detection of Irregular Power Usage", "abstract": "In machine learning, a bias occurs whenever training sets are not\nrepresentative for the test data, which results in unreliable models. The most\ncommon biases in data are arguably class imbalance and covariate shift. In this\nwork, we aim to shed light on this topic in order to increase the overall\nattention to this issue in the field of machine learning. We propose a scalable\nnovel framework for reducing multiple biases in high-dimensional data sets in\norder to train more reliable predictors. We apply our methodology to the\ndetection of irregular power usage from real, noisy industrial data. In\nemerging markets, irregular power usage, and electricity theft in particular,\nmay range up to 40% of the total electricity distributed. Biased data sets are\nof particular issue in this domain. We show that reducing these biases\nincreases the accuracy of the trained predictors. Our models have the potential\nto generate significant economic value in a real world application, as they are\nbeing deployed in a commercial software for the detection of irregular power\nusage."}, {"title": "An Overview of Machine Teaching", "abstract": "In this paper we try to organize machine teaching as a coherent set of ideas.\nEach idea is presented as varying along a dimension. The collection of\ndimensions then form the problem space of machine teaching, such that existing\nteaching problems can be characterized in this space. We hope this organization\nallows us to gain deeper understanding of individual teaching problems,\ndiscover connections among them, and identify gaps in the field."}, {"title": "Latitude: A Model for Mixed Linear-Tropical Matrix Factorization", "abstract": "Nonnegative matrix factorization (NMF) is one of the most frequently-used\nmatrix factorization models in data analysis. A significant reason to the\npopularity of NMF is its interpretability and the `parts of whole'\ninterpretation of its components. Recently, max-times, or subtropical, matrix\nfactorization (SMF) has been introduced as an alternative model with equally\ninterpretable `winner takes it all' interpretation. In this paper we propose a\nnew mixed linear--tropical model, and a new algorithm, called Latitude, that\ncombines NMF and SMF, being able to smoothly alternate between the two. In our\nmodel, the data is modeled using the latent factors and latent parameters that\ncontrol whether the factors are interpreted as NMF or SMF features, or their\nmixtures. We present an algorithm for our novel matrix factorization. Our\nexperiments show that our algorithm improves over both baselines, and can yield\ninterpretable results that reveal more of the latent structure than either NMF\nor SMF alone."}, {"title": "Tractable Learning and Inference for Large-Scale Probabilistic Boolean Networks", "abstract": "Probabilistic Boolean Networks (PBNs) have been previously proposed so as to\ngain insights into complex dy- namical systems. However, identification of\nlarge networks and of the underlying discrete Markov Chain which describes\ntheir temporal evolution, still remains a challenge. In this paper, we\nintroduce an equivalent representation for the PBN, the Stochastic Conjunctive\nNormal Form (SCNF), which paves the way to a scalable learning algorithm and\nhelps predict long- run dynamic behavior of large-scale systems. Moreover, SCNF\nallows its efficient sampling so as to statistically infer multi- step\ntransition probabilities which can provide knowledge on the activity levels of\nindividual nodes in the long run."}, {"title": "Quantization Error as a Metric for Dynamic Precision Scaling in Neural Net Training", "abstract": "Recent work has explored reduced numerical precision for parameters,\nactivations, and gradients during neural network training as a way to reduce\nthe computational cost of training (Na & Mukhopadhyay, 2016) (Courbariaux et\nal., 2014). We present a novel dynamic precision scaling (DPS) scheme. Using\nstochastic fixed-point rounding, a quantization-error based scaling scheme, and\ndynamic bit-widths during training, we achieve 98.8% test accuracy on the MNIST\ndataset using an average bit-width of just 16 bits for weights and 14 bits for\nactivations, compared to the standard 32-bit floating point values used in deep\nlearning frameworks."}, {"title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes", "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes\nas a basis for quickly improving performance on a novel task. Bayesian\nhierarchical modeling provides a theoretical framework for formalizing\nmeta-learning as inference for a set of parameters that are shared across\ntasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML)\nof Finn et al. (2017) as a method for probabilistic inference in a hierarchical\nBayesian model. In contrast to prior methods for meta-learning via hierarchical\nBayes, MAML is naturally applicable to complex function approximators through\nits use of a scalable gradient descent procedure for posterior inference.\nFurthermore, the identification of MAML as hierarchical Bayes provides a way to\nunderstand the algorithm's operation as a meta-learning procedure, as well as\nan opportunity to make use of computational strategies for efficient inference.\nWe use this opportunity to propose an improvement to the MAML algorithm that\nmakes use of techniques from approximate inference and curvature estimation."}, {"title": "Approximate Inference via Weighted Rademacher Complexity", "abstract": "Rademacher complexity is often used to characterize the learnability of a\nhypothesis class and is known to be related to the class size. We leverage this\nobservation and introduce a new technique for estimating the size of an\narbitrary weighted set, defined as the sum of weights of all elements in the\nset. Our technique provides upper and lower bounds on a novel generalization of\nRademacher complexity to the weighted setting in terms of the weighted set\nsize. This generalizes Massart's Lemma, a known upper bound on the Rademacher\ncomplexity in terms of the unweighted set size. We show that the weighted\nRademacher complexity can be estimated by solving a randomly perturbed\noptimization problem, allowing us to derive high-probability bounds on the size\nof any weighted set. We apply our method to the problems of calculating the\npartition function of an Ising model and computing propositional model counts\n(#SAT). Our experiments demonstrate that we can produce tighter bounds than\ncompeting methods in both the weighted and unweighted settings."}, {"title": "Certified Defenses against Adversarial Examples", "abstract": "While neural networks have achieved high accuracy on standard image\nclassification benchmarks, their accuracy drops to nearly zero in the presence\nof small adversarial perturbations to test inputs. Defenses based on\nregularization and adversarial training have been proposed, but often followed\nby new, stronger attacks that defeat these defenses. Can we somehow end this\narms race? In this work, we study this problem for neural networks with one\nhidden layer. We first propose a method based on a semidefinite relaxation that\noutputs a certificate that for a given network and test input, no attack can\nforce the error to exceed a certain value. Second, as this certificate is\ndifferentiable, we jointly optimize it with the network parameters, providing\nan adaptive regularizer that encourages robustness against all attacks. On\nMNIST, our approach produces a network and a certificate that no attack that\nperturbs each pixel by at most \\epsilon = 0.1 can cause more than 35% test\nerror."}, {"title": "Learning Combinations of Activation Functions", "abstract": "In the last decade, an active area of research has been devoted to design\nnovel activation functions that are able to help deep neural networks to\nconverge, obtaining better performance. The training procedure of these\narchitectures usually involves optimization of the weights of their layers\nonly, while non-linearities are generally pre-specified and their (possible)\nparameters are usually considered as hyper-parameters to be tuned manually. In\nthis paper, we introduce two approaches to automatically learn different\ncombinations of base activation functions (such as the identity function, ReLU,\nand tanh) during the training phase. We present a thorough comparison of our\nnovel approaches with well-known architectures (such as LeNet-5, AlexNet, and\nResNet-56) on three standard datasets (Fashion-MNIST, CIFAR-10, and\nILSVRC-2012), showing substantial improvements in the overall performance, such\nas an increase in the top-1 accuracy for AlexNet on ILSVRC-2012 of 3.01\npercentage points."}, {"title": "Learning the Reward Function for a Misspecified Model", "abstract": "In model-based reinforcement learning it is typical to decouple the problems\nof learning the dynamics model and learning the reward function. However, when\nthe dynamics model is flawed, it may generate erroneous states that would never\noccur in the true environment. It is not clear a priori what value the reward\nfunction should assign to such states. This paper presents a novel error bound\nthat accounts for the reward model's behavior in states sampled from the model.\nThis bound is used to extend the existing Hallucinated DAgger-MC algorithm,\nwhich offers theoretical performance guarantees in deterministic MDPs that do\nnot assume a perfect model can be learned. Empirically, this approach to reward\nlearning can yield dramatic improvements in control performance when the\ndynamics model is flawed."}, {"title": "Learning to Emulate an Expert Projective Cone Scheduler", "abstract": "Projective cone scheduling defines a large class of rate-stabilizing policies\nfor queueing models relevant to several applications. While there exists\nconsiderable theory on the properties of projective cone schedulers, there is\nlittle practical guidance on choosing the parameters that define them. In this\npaper, we propose an algorithm for designing an automated projective cone\nscheduling system based on observations of an expert projective cone scheduler.\nWe show that the estimated scheduling policy is able to emulate the expert in\nthe sense that the average loss realized by the learned policy will converge to\nzero. Specifically, for a system with $n$ queues observed over a time horizon\n$T$, the average loss for the algorithm is $O(\\ln(T)\\sqrt{\\ln(n)/T})$. This\nupper bound holds regardless of the statistical characteristics of the system.\nThe algorithm uses the multiplicative weights update method and can be applied\nonline so that additional observations of the expert scheduler can be used to\nimprove an existing estimate of the policy. This provides a data-driven method\nfor designing a scheduling policy based on observations of a human expert. We\ndemonstrate the efficacy of the algorithm with a simple numerical example and\ndiscuss several extensions."}, {"title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling", "abstract": "The graph convolutional networks (GCN) recently proposed by Kipf and Welling\nare an effective graph model for semi-supervised learning. This model, however,\nwas originally designed to be learned with the presence of both training and\ntest data. Moreover, the recursive neighborhood expansion across layers poses\ntime and memory challenges for training with large, dense graphs. To relax the\nrequirement of simultaneous availability of test data, we interpret graph\nconvolutions as integral transforms of embedding functions under probability\nmeasures. Such an interpretation allows for the use of Monte Carlo approaches\nto consistently estimate the integrals, which in turn leads to a batched\ntraining scheme as we propose in this work---FastGCN. Enhanced with importance\nsampling, FastGCN not only is efficient for training but also generalizes well\nfor inference. We show a comprehensive set of experiments to demonstrate its\neffectiveness compared with GCN and related models. In particular, training is\norders of magnitude more efficient while predictions remain comparably\naccurate."}, {"title": "Deep Learning of Nonnegativity-Constrained Autoencoders for Enhanced Understanding of Data", "abstract": "Unsupervised feature extractors are known to perform an efficient and\ndiscriminative representation of data. Insight into the mappings they perform\nand human ability to understand them, however, remain very limited. This is\nespecially prominent when multilayer deep learning architectures are used. This\npaper demonstrates how to remove these bottlenecks within the architecture of\nNonnegativity Constrained Autoencoder (NCSAE). It is shown that by using both\nL1 and L2 regularization that induce nonnegativity of weights, most of the\nweights in the network become constrained to be nonnegative thereby resulting\ninto a more understandable structure with minute deterioration in\nclassification accuracy. Also, this proposed approach extracts features that\nare more sparse and produces additional output layer sparsification. The method\nis analyzed for accuracy and feature interpretation on the MNIST data, the NORB\nnormalized uniform object data, and the Reuters text categorization dataset."}, {"title": "A New Backpropagation Algorithm without Gradient Descent", "abstract": "The backpropagation algorithm, which had been originally introduced in the\n1970s, is the workhorse of learning in neural networks. This backpropagation\nalgorithm makes use of the famous machine learning algorithm known as Gradient\nDescent, which is a first-order iterative optimization algorithm for finding\nthe minimum of a function. To find a local minimum of a function using gradient\ndescent, one takes steps proportional to the negative of the gradient (or of\nthe approximate gradient) of the function at the current point. In this paper,\nwe develop an alternative to the backpropagation without the use of the\nGradient Descent Algorithm, but instead we are going to devise a new algorithm\nto find the error in the weights and biases of an artificial neuron using\nMoore-Penrose Pseudo Inverse. The numerical studies and the experiments\nperformed on various datasets are used to verify the working of this\nalternative algorithm."}, {"title": "Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers", "abstract": "Model pruning has become a useful technique that improves the computational\nefficiency of deep learning, making it possible to deploy solutions in\nresource-limited scenarios. A widely-used practice in relevant work assumes\nthat a smaller-norm parameter or feature plays a less informative role at the\ninference time. In this paper, we propose a channel pruning technique for\naccelerating the computations of deep convolutional neural networks (CNNs) that\ndoes not critically rely on this assumption. Instead, it focuses on direct\nsimplification of the channel-to-channel computation graph of a CNN without the\nneed of performing a computationally difficult and not-always-useful task of\nmaking high-dimensional tensors of CNN structured sparse. Our approach takes\ntwo stages: first to adopt an end-to- end stochastic training method that\neventually forces the outputs of some channels to be constant, and then to\nprune those constant channels from the original neural network by adjusting the\nbiases of their impacting layers such that the resulting compact model can be\nquickly fine-tuned. Our approach is mathematically appealing from an\noptimization perspective and easy to reproduce. We experimented our approach\nthrough several image learning benchmarks and demonstrate its interesting\naspects and competitive performance."}, {"title": "Bootstrapping and Multiple Imputation Ensemble Approaches for Missing Data", "abstract": "Presence of missing values in a dataset can adversely affect the performance\nof a classifier. Single and Multiple Imputation are normally performed to fill\nin the missing values. In this paper, we present several variants of combining\nsingle and multiple imputation with bootstrapping to create ensembles that can\nmodel uncertainty and diversity in the data, and that are robust to high\nmissingness in the data. We present three ensemble strategies: bootstrapping on\nincomplete data followed by (i) single imputation and (ii) multiple imputation,\nand (iii) multiple imputation ensemble without bootstrapping. We perform an\nextensive evaluation of the performance of the these ensemble strategies on 8\ndatasets by varying the missingness ratio. Our results show that bootstrapping\nfollowed by multiple imputation using expectation maximization is the most\nrobust method even at high missingness ratio (up to 30%). For small missingness\nratio (up to 10%) most of the ensemble methods perform quivalently but better\nthan single imputation. Kappa-error plots suggest that accurate classifiers\nwith reasonable diversity is the reason for this behaviour. A consistent\nobservation in all the datasets suggests that for small missingness (up to\n10%), bootstrapping on incomplete data without any imputation produces\nequivalent results to other ensemble methods."}, {"title": "Augmented Space Linear Model", "abstract": "The linear model uses the space defined by the input to project the target or\ndesired signal and find the optimal set of model parameters. When the problem\nis nonlinear, the adaption requires nonlinear models for good performance, but\nit becomes slower and more cumbersome. In this paper, we propose a linear model\ncalled Augmented Space Linear Model (ASLM), which uses the full joint space of\ninput and desired signal as the projection space and approaches the performance\nof nonlinear models. This new algorithm takes advantage of the linear solution,\nand corrects the estimate for the current testing phase input with the error\nassigned to the input space neighborhood in the training phase. This algorithm\ncan solve the nonlinear problem with the computational efficiency of linear\nmethods, which can be regarded as a trade off between accuracy and\ncomputational complexity. Making full use of the training data, the proposed\naugmented space model may provide a new way to improve many modeling tasks."}, {"title": "Clustering and Unsupervised Anomaly Detection with L2 Normalized Deep Auto-Encoder Representations", "abstract": "Clustering is essential to many tasks in pattern recognition and computer\nvision. With the advent of deep learning, there is an increasing interest in\nlearning deep unsupervised representations for clustering analysis. Many works\non this domain rely on variants of auto-encoders and use the encoder outputs as\nrepresentations/features for clustering. In this paper, we show that an l2\nnormalization constraint on these representations during auto-encoder training,\nmakes the representations more separable and compact in the Euclidean space\nafter training. This greatly improves the clustering accuracy when k-means\nclustering is employed on the representations. We also propose a clustering\nbased unsupervised anomaly detection method using l2 normalized deep\nauto-encoder representations. We show the effect of l2 normalization on anomaly\ndetection accuracy. We further show that the proposed anomaly detection method\ngreatly improves accuracy compared to previously proposed deep methods such as\nreconstruction error based anomaly detection."}, {"title": "Training Neural Networks by Using Power Linear Units (PoLUs)", "abstract": "In this paper, we introduce \"Power Linear Unit\" (PoLU) which increases the\nnonlinearity capacity of a neural network and thus helps improving its\nperformance. PoLU adopts several advantages of previously proposed activation\nfunctions. First, the output of PoLU for positive inputs is designed to be\nidentity to avoid the gradient vanishing problem. Second, PoLU has a non-zero\noutput for negative inputs such that the output mean of the units is close to\nzero, hence reducing the bias shift effect. Thirdly, there is a saturation on\nthe negative part of PoLU, which makes it more noise-robust for negative\ninputs. Furthermore, we prove that PoLU is able to map more portions of every\nlayer's input to the same space by using the power function and thus increases\nthe number of response regions of the neural network. We use image\nclassification for comparing our proposed activation function with others. In\nthe experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN)\nand ImageNet are used as benchmark datasets. The neural networks we implemented\ninclude widely-used ELU-Network, ResNet-50, and VGG16, plus a couple of shallow\nnetworks. Experimental results show that our proposed activation function\noutperforms other state-of-the-art models with most networks."}, {"title": "Analysis of Fast Alternating Minimization for Structured Dictionary Learning", "abstract": "Methods exploiting sparsity have been popular in imaging and signal\nprocessing applications including compression, denoising, and imaging inverse\nproblems. Data-driven approaches such as dictionary learning and transform\nlearning enable one to discover complex image features from datasets and\nprovide promising performance over analytical models. Alternating minimization\nalgorithms have been particularly popular in dictionary or transform learning.\nIn this work, we study the properties of alternating minimization for\nstructured (unitary) sparsifying operator learning. While the algorithm\nconverges to the stationary points of the non-convex problem in general, we\nprove rapid local linear convergence to the underlying generative model under\nmild assumptions. Our experiments show that the unitary operator learning\nalgorithm is robust to initialization."}, {"title": "GeniePath: Graph Neural Networks with Adaptive Receptive Paths", "abstract": "We present, GeniePath, a scalable approach for learning adaptive receptive\nfields of neural networks defined on permutation invariant graph data. In\nGeniePath, we propose an adaptive path layer consists of two complementary\nfunctions designed for breadth and depth exploration respectively, where the\nformer learns the importance of different sized neighborhoods, while the latter\nextracts and filters signals aggregated from neighbors of different hops away.\nOur method works in both transductive and inductive settings, and extensive\nexperiments compared with competitive methods show that our approaches yield\nstate-of-the-art results on large graphs."}, {"title": "Online Compact Convexified Factorization Machine", "abstract": "Factorization Machine (FM) is a supervised learning approach with a powerful\ncapability of feature engineering. It yields state-of-the-art performance in\nvarious batch learning tasks where all the training data is made available\nprior to the training. However, in real-world applications where the data\narrives sequentially in a streaming manner, the high cost of re-training with\nbatch learning algorithms has posed formidable challenges in the online\nlearning scenario. The initial challenge is that no prior formulations of FM\ncould fulfill the requirements in Online Convex Optimization (OCO) -- the\nparamount framework for online learning algorithm design. To address the\naforementioned challenge, we invent a new convexification scheme leading to a\nCompact Convexified FM (CCFM) that seamlessly meets the requirements in OCO.\nHowever for learning Compact Convexified FM (CCFM) in the online learning\nsetting, most existing algorithms suffer from expensive projection operations.\nTo address this subsequent challenge, we follow the general projection-free\nalgorithmic framework of Online Conditional Gradient and propose an Online\nCompact Convex Factorization Machine (OCCFM) algorithm that eschews the\nprojection operation with efficient linear optimization steps. In support of\nthe proposed OCCFM in terms of its theoretical foundation, we prove that the\ndeveloped algorithm achieves a sub-linear regret bound. To evaluate the\nempirical performance of OCCFM, we conduct extensive experiments on 6\nreal-world datasets for online recommendation and binary classification tasks.\nThe experimental results show that OCCFM outperforms the state-of-art online\nlearning algorithms."}, {"title": "Explicit Inductive Bias for Transfer Learning with Convolutional Networks", "abstract": "In inductive transfer learning, fine-tuning pre-trained convolutional\nnetworks substantially outperforms training from scratch. When using\nfine-tuning, the underlying assumption is that the pre-trained model extracts\ngeneric features, which are at least partially relevant for solving the target\ntask, but would be difficult to extract from the limited amount of data\navailable on the target task. However, besides the initialization with the\npre-trained model and the early stopping, there is no mechanism in fine-tuning\nfor retaining the features learned on the source task. In this paper, we\ninvestigate several regularization schemes that explicitly promote the\nsimilarity of the final solution with the initial model. We show the benefit of\nhaving an explicit inductive bias towards the initial model, and we eventually\nrecommend a simple $L^2$ penalty with the pre-trained model being a reference\nas the baseline of penalty for transfer learning tasks."}, {"title": "Selective Sampling and Mixture Models in Generative Adversarial Networks", "abstract": "In this paper, we propose a multi-generator extension to the adversarial\ntraining framework, in which the objective of each generator is to represent a\nunique component of a target mixture distribution. In the training phase, the\ngenerators cooperate to represent, as a mixture, the target distribution while\nmaintaining distinct manifolds. As opposed to traditional generative models,\ninference from a particular generator after training resembles selective\nsampling from a unique component in the target distribution. We demonstrate the\nfeasibility of the proposed architecture both analytically and with basic\nMulti-Layer Perceptron (MLP) models trained on the MNIST dataset."}, {"title": "MotifNet: a motif-based Graph Convolutional Network for directed graphs", "abstract": "Deep learning on graphs and in particular, graph convolutional neural\nnetworks, have recently attracted significant attention in the machine learning\ncommunity. Many of such techniques explore the analogy between the graph\nLaplacian eigenvectors and the classical Fourier basis, allowing to formulate\nthe convolution as a multiplication in the spectral domain. One of the key\ndrawback of spectral CNNs is their explicit assumption of an undirected graph,\nleading to a symmetric Laplacian matrix with orthogonal eigendecomposition. In\nthis work we propose MotifNet, a graph CNN capable of dealing with directed\ngraphs by exploiting local graph motifs. We present experimental evidence\nshowing the advantage of our approach on real data."}, {"title": "Re-Weighted Learning for Sparsifying Deep Neural Networks", "abstract": "This paper addresses the topic of sparsifying deep neural networks (DNN's).\nWhile DNN's are powerful models that achieve state-of-the-art performance on a\nlarge number of tasks, the large number of model parameters poses serious\nstorage and computational challenges. To combat these difficulties, a growing\nline of work focuses on pruning network weights without sacrificing\nperformance. We propose a general affine scaling transformation (AST) algorithm\nto sparsify DNN's. Our approach follows in the footsteps of popular sparse\nrecovery techniques, which have yet to be explored in the context of DNN's. We\ndescribe a principled framework for transforming densely connected DNN's into\nsparsely connected ones without sacrificing network performance. Unlike\nexisting methods, our approach is able to learn sparse connections at each\nlayer simultaneously, and achieves comparable pruning results on the\narchitecture tested."}, {"title": "Mixed Link Networks", "abstract": "Basing on the analysis by revealing the equivalence of modern networks, we\nfind that both ResNet and DenseNet are essentially derived from the same \"dense\ntopology\", yet they only differ in the form of connection -- addition (dubbed\n\"inner link\") vs. concatenation (dubbed \"outer link\"). However, both two forms\nof connections have the superiority and insufficiency. To combine their\nadvantages and avoid certain limitations on representation learning, we present\na highly efficient and modularized Mixed Link Network (MixNet) which is\nequipped with flexible inner link and outer link modules. Consequently, ResNet,\nDenseNet and Dual Path Network (DPN) can be regarded as a special case of\nMixNet, respectively. Furthermore, we demonstrate that MixNets can achieve\nsuperior efficiency in parameter over the state-of-the-art architectures on\nmany competitive datasets like CIFAR-10/100, SVHN and ImageNet."}, {"title": "Directly and Efficiently Optimizing Prediction Error and AUC of Linear Classifiers", "abstract": "The predictive quality of machine learning models is typically measured in\nterms of their (approximate) expected prediction error or the so-called Area\nUnder the Curve (AUC) for a particular data distribution. However, when the\nmodels are constructed by the means of empirical risk minimization, surrogate\nfunctions such as the logistic loss are optimized instead. This is done because\nthe empirical approximations of the expected error and AUC functions are\nnonconvex and nonsmooth, and more importantly have zero derivative almost\neverywhere. In this work, we show that in the case of linear predictors, and\nunder the assumption that the data has normal distribution, the expected error\nand the expected AUC are not only smooth, but have closed form expressions,\nwhich depend on the first and second moments of the normal distribution. Hence,\nwe derive derivatives of these two functions and use these derivatives in an\noptimization algorithm to directly optimize the expected error and the AUC. In\nthe case of real data sets, the derivatives can be approximated using empirical\nmoments. We show that even when data is not normally distributed, computed\nderivatives are sufficiently useful to render an efficient optimization method\nand high quality solutions. Thus, we propose a gradient-based optimization\nmethod for direct optimization of the prediction error and AUC. Moreover, the\nper-iteration complexity of the proposed algorithm has no dependence on the\nsize of the data set, unlike those for optimizing logistic regression and all\nother well known empirical risk minimization problems."}, {"title": "Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction", "abstract": "To overcome the limitations of Neural Programmer-Interpreters (NPI) in its\nuniversality and learnability, we propose the incorporation of combinator\nabstraction into neural programing and a new NPI architecture to support this\nabstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI).\nCombinator abstraction dramatically reduces the number and complexity of\nprograms that need to be interpreted by the core controller of CNPI, while\nstill allowing the CNPI to represent and interpret arbitrary complex programs\nby the collaboration of the core with the other components. We propose a small\nset of four combinators to capture the most pervasive programming patterns. Due\nto the finiteness and simplicity of this combinator set and the offloading of\nsome burden of interpretation from the core, we are able construct a CNPI that\nis universal with respect to the set of all combinatorizable programs, which is\nadequate for solving most algorithmic tasks. Moreover, besides supervised\ntraining on execution traces, CNPI can be trained by policy gradient\nreinforcement learning with appropriately designed curricula."}, {"title": "Online Learning: A Comprehensive Survey", "abstract": "Online learning represents an important family of machine learning\nalgorithms, in which a learner attempts to resolve an online prediction (or any\ntype of decision-making) task by learning a model/hypothesis from a sequence of\ndata instances one at a time. The goal of online learning is to ensure that the\nonline learner would make a sequence of accurate predictions (or correct\ndecisions) given the knowledge of correct answers to previous prediction or\nlearning tasks and possibly additional information. This is in contrast to many\ntraditional batch learning or offline machine learning algorithms that are\noften designed to train a model in batch from a given collection of training\ndata instances. This survey aims to provide a comprehensive survey of the\nonline machine learning literatures through a systematic review of basic ideas\nand key principles and a proper categorization of different algorithms and\ntechniques. Generally speaking, according to the learning type and the forms of\nfeedback information, the existing online learning works can be classified into\nthree major categories: (i) supervised online learning where full feedback\ninformation is always available, (ii) online learning with limited feedback,\nand (iii) unsupervised online learning where there is no feedback available.\nDue to space limitation, the survey will be mainly focused on the first\ncategory, but also briefly cover some basics of the other two categories.\nFinally, we also discuss some open issues and attempt to shed light on\npotential future research directions in this field."}, {"title": "Learning and Querying Fast Generative Models for Reinforcement Learning", "abstract": "A key challenge in model-based reinforcement learning (RL) is to synthesize\ncomputationally efficient and accurate environment models. We show that\ncarefully designed generative models that learn and operate on compact state\nrepresentations, so-called state-space models, substantially reduce the\ncomputational costs for predicting outcomes of sequences of actions. Extensive\nexperiments establish that state-space models accurately capture the dynamics\nof Atari games from the Arcade Learning Environment from raw pixels. The\ncomputational speed-up of state-space models while maintaining high accuracy\nmakes their application in RL feasible: We demonstrate that agents which query\nthese models for decision making outperform strong model-free baselines on the\ngame MSPACMAN, demonstrating the potential of using learned environment models\nfor planning."}, {"title": "Make the Minority Great Again: First-Order Regret Bound for Contextual Bandits", "abstract": "Regret bounds in online learning compare the player's performance to $L^*$,\nthe optimal performance in hindsight with a fixed strategy. Typically such\nbounds scale with the square root of the time horizon $T$. The more refined\nconcept of first-order regret bound replaces this with a scaling $\\sqrt{L^*}$,\nwhich may be much smaller than $\\sqrt{T}$. It is well known that minor variants\nof standard algorithms satisfy first-order regret bounds in the full\ninformation and multi-armed bandit settings. In a COLT 2017 open problem,\nAgarwal, Krishnamurthy, Langford, Luo, and Schapire raised the issue that\nexisting techniques do not seem sufficient to obtain first-order regret bounds\nfor the contextual bandit problem. In the present paper, we resolve this open\nproblem by presenting a new strategy based on augmenting the policy space."}, {"title": "Learning Local Metrics and Influential Regions for Classification", "abstract": "The performance of distance-based classifiers heavily depends on the\nunderlying distance metric, so it is valuable to learn a suitable metric from\nthe data. To address the problem of multimodality, it is desirable to learn\nlocal metrics. In this short paper, we define a new intuitive distance with\nlocal metrics and influential regions, and subsequently propose a novel local\nmetric learning method for distance-based classification. Our key intuition is\nto partition the metric space into influential regions and a background region,\nand then regulate the effectiveness of each local metric to be within the\nrelated influential regions. We learn local metrics and influential regions to\nreduce the empirical hinge loss, and regularize the parameters on the basis of\na resultant learning bound. Encouraging experimental results are obtained from\nvarious public and popular data sets."}, {"title": "Metric Learning via Maximizing the Lipschitz Margin Ratio", "abstract": "In this paper, we propose the Lipschitz margin ratio and a new metric\nlearning framework for classification through maximizing the ratio. This\nframework enables the integration of both the inter-class margin and the\nintra-class dispersion, as well as the enhancement of the generalization\nability of a classifier. To introduce the Lipschitz margin ratio and its\nassociated learning bound, we elaborate the relationship between metric\nlearning and Lipschitz functions, as well as the representability and\nlearnability of the Lipschitz functions. After proposing the new metric\nlearning framework based on the introduced Lipschitz margin ratio, we also\nprove that some well known metric learning algorithms can be shown as special\ncases of the proposed framework. In addition, we illustrate the framework by\nimplementing it for learning the squared Mahalanobis metric, and by\ndemonstrating its encouraging results on eight popular datasets of machine\nlearning."}, {"title": "A Continuation Method for Discrete Optimization and its Application to Nearest Neighbor Classification", "abstract": "The continuation method is a popular approach in non-convex optimization and\ncomputer vision. The main idea is to start from a simple function that can be\nminimized efficiently, and gradually transform it to the more complicated\noriginal objective function. The solution of the simpler problem is used as the\nstarting point to solve the original problem. We show a continuation method for\ndiscrete optimization problems. Ideally, we would like the evolved function to\nbe hill-climbing friendly and to have the same global minima as the original\nfunction. We show that the proposed continuation method is the best affine\napproximation of a transformation that is guaranteed to transform the function\nto a hill-climbing friendly function and to have the same global minima.\n  We show the effectiveness of the proposed technique in the problem of nearest\nneighbor classification. Although nearest neighbor methods are often\ncompetitive in terms of sample efficiency, the computational complexity in the\ntest phase has been a major obstacle in their applicability in big data\nproblems. Using the proposed continuation method, we show an improved\ngraph-based nearest neighbor algorithm. The method is readily understood and\neasy to implement. We show how the computational complexity of the method in\nthe test phase scales gracefully with the size of the training set, a property\nthat is particularly important in big data applications."}, {"title": "Disturbance Grassmann Kernels for Subspace-Based Learning", "abstract": "In this paper, we focus on subspace-based learning problems, where data\nelements are linear subspaces instead of vectors. To handle this kind of data,\nGrassmann kernels were proposed to measure the space structure and used with\nclassifiers, e.g., Support Vector Machines (SVMs). However, the existing\ndiscriminative algorithms mostly ignore the instability of subspaces, which\nwould cause the classifiers misled by disturbed instances. Thus we propose\nconsidering all potential disturbance of subspaces in learning processes to\nobtain more robust classifiers. Firstly, we derive the dual optimization of\nlinear classifiers with disturbance subject to a known distribution, resulting\nin a new kernel, Disturbance Grassmann (DG) kernel. Secondly, we research into\ntwo kinds of disturbance, relevant to the subspace matrix and singular values\nof bases, with which we extend the Projection kernel on Grassmann manifolds to\ntwo new kernels. Experiments on action data indicate that the proposed kernels\nperform better compared to state-of-the-art subspace-based methods, even in a\nworse environment."}, {"title": "The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration", "abstract": "Learning distributed representations for nodes in graphs is a crucial\nprimitive in network analysis with a wide spectrum of applications. Linear\ngraph embedding methods learn such representations by optimizing the likelihood\nof both positive and negative edges while constraining the dimension of the\nembedding vectors. We argue that the generalization performance of these\nmethods is not due to the dimensionality constraint as commonly believed, but\nrather the small norm of embedding vectors. Both theoretical and empirical\nevidence are provided to support this argument: (a) we prove that the\ngeneralization error of these methods can be bounded by limiting the norm of\nvectors, regardless of the embedding dimension; (b) we show that the\ngeneralization performance of linear graph embedding methods is correlated with\nthe norm of embedding vectors, which is small due to the early stopping of SGD\nand the vanishing gradients. We performed extensive experiments to validate our\nanalysis and showcased the importance of proper norm regularization in\npractice."}, {"title": "Tips, guidelines and tools for managing multi-label datasets: the mldr.datasets R package and the Cometa data repository", "abstract": "New proposals in the field of multi-label learning algorithms have been\ngrowing in number steadily over the last few years. The experimentation\nassociated with each of them always goes through the same phases: selection of\ndatasets, partitioning, training, analysis of results and, finally, comparison\nwith existing methods. This last step is often hampered since it involves using\nexactly the same datasets, partitioned in the same way and using the same\nvalidation strategy. In this paper we present a set of tools whose objective is\nto facilitate the management of multi-label datasets, aiming to standardize the\nexperimentation procedure. The two main tools are an R package, mldr.datasets,\nand a web repository with datasets, Cometa. Together, these tools will simplify\nthe collection of datasets, their partitioning, documentation and export to\nmultiple formats, among other functions. Some tips, recommendations and\nguidelines for a good experimental analysis of multi-label methods are also\npresented."}, {"title": "Deep Meta-Learning: Learning to Learn in the Concept Space", "abstract": "Few-shot learning remains challenging for meta-learning that learns a\nlearning algorithm (meta-learner) from many related tasks. In this work, we\nargue that this is due to the lack of a good representation for meta-learning,\nand propose deep meta-learning to integrate the representation power of deep\nlearning into meta-learning. The framework is composed of three modules, a\nconcept generator, a meta-learner, and a concept discriminator, which are\nlearned jointly. The concept generator, e.g. a deep residual net, extracts a\nrepresentation for each instance that captures its high-level concept, on which\nthe meta-learner performs few-shot learning, and the concept discriminator\nrecognizes the concepts. By learning to learn in the concept space rather than\nin the complicated instance space, deep meta-learning can substantially improve\nvanilla meta-learning, which is demonstrated on various few-shot image\nrecognition problems. For example, on 5-way-1-shot image recognition on\nCIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to\n58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%,\nand improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%,\nrespectively."}, {"title": "Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks", "abstract": "We provide theoretical investigation of curriculum learning in the context of\nstochastic gradient descent when optimizing the convex linear regression loss.\nWe prove that the rate of convergence of an ideal curriculum learning method is\nmonotonically increasing with the difficulty of the examples. Moreover, among\nall equally difficult points, convergence is faster when using points which\nincur higher loss with respect to the current hypothesis. We then analyze\ncurriculum learning in the context of training a CNN. We describe a method\nwhich infers the curriculum by way of transfer learning from another network,\npre-trained on a different task. While this approach can only approximate the\nideal curriculum, we observe empirically similar behavior to the one predicted\nby the theory, namely, a significant boost in convergence speed at the\nbeginning of training. When the task is made more difficult, improvement in\ngeneralization performance is also observed. Finally, curriculum learning\nexhibits robustness against unfavorable conditions such as excessive\nregularization."}, {"title": "PRIL: Perceptron Ranking Using Interval Labeled Data", "abstract": "In this paper, we propose an online learning algorithm PRIL for learning\nranking classifiers using interval labeled data and show its correctness. We\nshow its convergence in finite number of steps if there exists an ideal\nclassifier such that the rank given by it for an example always lies in its\nlabel interval. We then generalize this mistake bound result for the general\ncase. We also provide regret bound for the proposed algorithm. We propose a\nmultiplicative update algorithm for PRIL called M-PRIL. We provide its\ncorrectness and convergence results. We show the effectiveness of PRIL by\nshowing its performance on various datasets."}, {"title": "On the Needs for Rotations in Hypercubic Quantization Hashing", "abstract": "The aim of this paper is to endow the well-known family of hypercubic\nquantization hashing methods with theoretical guarantees. In hypercubic\nquantization, applying a suitable (random or learned) rotation after\ndimensionality reduction has been experimentally shown to improve the results\naccuracy in the nearest neighbors search problem. We prove in this paper that\nthe use of these rotations is optimal under some mild assumptions: getting\noptimal binary sketches is equivalent to applying a rotation uniformizing the\ndiagonal of the covariance matrix between data points. Moreover, for two closed\npoints, the probability to have dissimilar binary sketches is upper bounded by\na factor of the initial distance between the data points. Relaxing these\nassumptions, we obtain a general concentration result for random matrices. We\nalso provide some experiments illustrating these theoretical points and compare\na set of algorithms in both the batch and online settings."}, {"title": "Policy Gradients for Contextual Recommendations", "abstract": "Decision making is a challenging task in online recommender systems. The\ndecision maker often needs to choose a contextual item at each step from a set\nof candidates. Contextual bandit algorithms have been successfully deployed to\nsuch applications, for the trade-off between exploration and exploitation and\nthe state-of-art performance on minimizing online costs. However, the\napplicability of existing contextual bandit methods is limited by the\nover-simplified assumptions of the problem, such as assuming a simple form of\nthe reward function or assuming a static environment where the states are not\naffected by previous actions. In this work, we put forward Policy Gradients for\nContextual Recommendations (PGCR) to solve the problem without those\nunrealistic assumptions. It optimizes over a restricted class of policies where\nthe marginal probability of choosing an item (in expectation of other items)\nhas a simple closed form, and the gradient of the expected return over the\npolicy in this class is in a succinct form. Moreover, PGCR leverages two useful\nheuristic techniques called Time-Dependent Greed and Actor-Dropout. The former\nensures PGCR to be empirically greedy in the limit, and the latter addresses\nthe trade-off between exploration and exploitation by using the policy network\nwith Dropout as a Bayesian approximation. PGCR can solve the standard\ncontextual bandits as well as its Markov Decision Process generalization.\nTherefore it can be applied to a wide range of realistic settings of\nrecommendations, such as personalized advertising. We evaluate PGCR on toy\ndatasets as well as a real-world dataset of personalized music recommendations.\nExperiments show that PGCR enables fast convergence and low regret, and\noutperforms both classic contextual-bandits and vanilla policy gradient\nmethods."}, {"title": "Electric Vehicle Driver Clustering using Statistical Model and Machine Learning", "abstract": "Electric Vehicle (EV) is playing a significant role in the distribution\nenergy management systems since the power consumption level of the EVs is much\nhigher than the other regular home appliances. The randomness of the EV driver\nbehaviors make the optimal charging or discharging scheduling even more\ndifficult due to the uncertain charging session parameters. To minimize the\nimpact of behavioral uncertainties, it is critical to develop effective methods\nto predict EV load for smart EV energy management. Using the EV smart charging\ninfrastructures on UCLA campus and city of Santa Monica as testbeds, we have\ncollected real-world datasets of EV charging behaviors, based on which we\nproposed an EV user modeling technique which combines statistical analysis and\nmachine learning approaches. Specifically, unsupervised clustering algorithm,\nand multilayer perceptron are applied to historical charging record to make the\nday-ahead EV parking and load prediction. Experimental results with\ncross-validation show that our model can achieve good performance for charging\ncontrol scheduling and online EV load forecasting."}, {"title": "Sparse Reject Option Classifier Using Successive Linear Programming", "abstract": "In this paper, we propose an approach for learning sparse reject option\nclassifiers using double ramp loss $L_{dr}$. We use DC programming to find the\nrisk minimizer. The algorithm solves a sequence of linear programs to learn the\nreject option classifier. We show that the loss $L_{dr}$ is Fisher consistent.\nWe also show that the excess risk of loss $L_d$ is upper bounded by the excess\nrisk of $L_{dr}$. We derive the generalization error bounds for the proposed\napproach. We show the effectiveness of the proposed approach by experimenting\nit on several real world datasets. The proposed approach not only performs\ncomparable to the state of the art but it also successfully learns sparse\nclassifiers."}, {"title": "Multi-Armed Bandits on Partially Revealed Unit Interval Graphs", "abstract": "A stochastic multi-armed bandit problem with side information on the\nsimilarity and dissimilarity across different arms is considered. The action\nspace of the problem can be represented by a unit interval graph (UIG) where\neach node represents an arm and the presence (absence) of an edge between two\nnodes indicates similarity (dissimilarity) between their mean rewards. Two\nsettings of complete and partial side information based on whether the UIG is\nfully revealed are studied and a general two-step learning structure consisting\nof an offline reduction of the action space and online aggregation of reward\nobservations from similar arms is proposed to fully exploit the topological\nstructure of the side information. In both cases, the computation efficiency\nand the order optimality of the proposed learning policies in terms of both the\nsize of the action space and the time length are established."}, {"title": "Classification from Pairwise Similarity and Unlabeled Data", "abstract": "Supervised learning needs a huge amount of labeled data, which can be a big\nbottleneck under the situation where there is a privacy concern or labeling\ncost is high. To overcome this problem, we propose a new weakly-supervised\nlearning setting where only similar (S) data pairs (two examples belong to the\nsame class) and unlabeled (U) data points are needed instead of fully labeled\ndata, which is called SU classification. We show that an unbiased estimator of\nthe classification risk can be obtained only from SU data, and the estimation\nerror of its empirical risk minimizer achieves the optimal parametric\nconvergence rate. Finally, we demonstrate the effectiveness of the proposed\nmethod through experiments."}, {"title": "Neural Tensor Factorization", "abstract": "Neural collaborative filtering (NCF) and recurrent recommender systems (RRN)\nhave been successful in modeling user-item relational data. However, they are\nalso limited in their assumption of static or sequential modeling of relational\ndata as they do not account for evolving users' preference over time as well as\nchanges in the underlying factors that drive the change in user-item\nrelationship over time. We address these limitations by proposing a Neural\nTensor Factorization (NTF) model for predictive tasks on dynamic relational\ndata. The NTF model generalizes conventional tensor factorization from two\nperspectives: First, it leverages the long short-term memory architecture to\ncharacterize the multi-dimensional temporal interactions on relational data.\nSecond, it incorporates the multi-layer perceptron structure for learning the\nnon-linearities between different latent factors. Our extensive experiments\ndemonstrate the significant improvement in rating prediction and link\nprediction on dynamic relational data by our NTF model over both neural network\nbased factorization models and other traditional methods."}, {"title": "Flipped-Adversarial AutoEncoders", "abstract": "We propose a flipped-Adversarial AutoEncoder (FAAE) that simultaneously\ntrains a generative model G that maps an arbitrary latent code distribution to\na data distribution and an encoder E that embodies an \"inverse mapping\" that\nencodes a data sample into a latent code vector. Unlike previous hybrid\napproaches that leverage adversarial training criterion in constructing\nautoencoders, FAAE minimizes re-encoding errors in the latent space and\nexploits adversarial criterion in the data space. Experimental evaluations\ndemonstrate that the proposed framework produces sharper reconstructed images\nwhile at the same time enabling inference that captures rich semantic\nrepresentation of data."}, {"title": "Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring", "abstract": "Deep Neural Networks have recently gained lots of success after enabling\nseveral breakthroughs in notoriously challenging problems. Training these\nnetworks is computationally expensive and requires vast amounts of training\ndata. Selling such pre-trained models can, therefore, be a lucrative business\nmodel. Unfortunately, once the models are sold they can be easily copied and\nredistributed. To avoid this, a tracking mechanism to identify models as the\nintellectual property of a particular vendor is necessary.\n  In this work, we present an approach for watermarking Deep Neural Networks in\na black-box way. Our scheme works for general classification tasks and can\neasily be combined with current learning algorithms. We show experimentally\nthat such a watermark has no noticeable impact on the primary task that the\nmodel is designed for and evaluate the robustness of our proposal against a\nmultitude of practical attacks. Moreover, we provide a theoretical analysis,\nrelating our approach to previous work on backdooring."}, {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their\ndeployment in embedded systems have been active and promising topics. Although\nprevious works have successfully reduced precision in inference, transferring\nboth training and inference processes to low-bitwidth integers has not been\ndemonstrated simultaneously. In this work, we develop a new method termed as\n\"WAGE\" to discretize both training and inference, where weights (W),\nactivations (A), gradients (G) and errors (E) among layers are shifted and\nlinearly constrained to low-bitwidth integers. To perform pure discrete\ndataflow for fixed-point devices, we further replace batch normalization by a\nconstant scaling layer and simplify other components that are arduous for\ninteger implementation. Improved accuracies can be obtained on multiple\ndatasets, which indicates that WAGE somehow acts as a type of regularization.\nEmpirically, we demonstrate the potential to deploy training in hardware\nsystems such as integer-based deep learning accelerators and neuromorphic chips\nwith comparable accuracy and higher energy efficiency, which is crucial to\nfuture AI applications in variable scenarios with transfer and continual\nlearning demands."}, {"title": "Predict and Constrain: Modeling Cardinality in Deep Structured Prediction", "abstract": "Many machine learning problems require the prediction of multi-dimensional\nlabels. Such structured prediction models can benefit from modeling\ndependencies between labels. Recently, several deep learning approaches to\nstructured prediction have been proposed. Here we focus on capturing\ncardinality constraints in such models. Namely, constraining the number of\nnon-zero labels that the model outputs. Such constraints have proven very\nuseful in previous structured prediction approaches, but it is a challenge to\nintroduce them into a deep learning framework. Here we show how to do this via\na novel deep architecture. Our approach outperforms strong baselines, achieving\nstate-of-the-art results on multi-label classification benchmarks."}, {"title": "Identify Susceptible Locations in Medical Records via Adversarial Attacks on Deep Predictive Models", "abstract": "The surging availability of electronic medical records (EHR) leads to\nincreased research interests in medical predictive modeling. Recently many deep\nlearning based predicted models are also developed for EHR data and\ndemonstrated impressive performance. However, a series of recent studies showed\nthat these deep models are not safe: they suffer from certain vulnerabilities.\nIn short, a well-trained deep network can be extremely sensitive to inputs with\nnegligible changes. These inputs are referred to as adversarial examples. In\nthe context of medical informatics, such attacks could alter the result of a\nhigh performance deep predictive model by slightly perturbing a patient's\nmedical records. Such instability not only reflects the weakness of deep\narchitectures, more importantly, it offers guide on detecting susceptible parts\non the inputs. In this paper, we propose an efficient and effective framework\nthat learns a time-preferential minimum attack targeting the LSTM model with\nEHR inputs, and we leverage this attack strategy to screen medical records of\npatients and identify susceptible events and measurements. The efficient\nscreening procedure can assist decision makers to pay extra attentions to the\nlocations that can cause severe consequence if not measured correctly. We\nconduct extensive empirical studies on a real-world urgent care cohort and\ndemonstrate the effectiveness of the proposed screening approach."}, {"title": "Geometry-Based Data Generation", "abstract": "Many generative models attempt to replicate the density of their input data.\nHowever, this approach is often undesirable, since data density is highly\naffected by sampling biases, noise, and artifacts. We propose a method called\nSUGAR (Synthesis Using Geometrically Aligned Random-walks) that uses a\ndiffusion process to learn a manifold geometry from the data. Then, it\ngenerates new points evenly along the manifold by pulling randomly generated\npoints into its intrinsic structure using a diffusion kernel. SUGAR equalizes\nthe density along the manifold by selectively generating points in sparse areas\nof the manifold. We demonstrate how the approach corrects sampling biases and\nartifacts, while also revealing intrinsic patterns (e.g. progression) and\nrelations in the data. The method is applicable for correcting missing data,\nfinding hypothetical data points, and learning relationships between data\nfeatures."}, {"title": "Attack RMSE Leaderboard: An Introduction and Case Study", "abstract": "In this manuscript, we briefly introduce several tricks to climb the\nleaderboards which use RMSE for evaluation without exploiting any training\ndata."}, {"title": "Graph2Seq: Scalable Learning Dynamics for Graphs", "abstract": "Neural networks have been shown to be an effective tool for learning\nalgorithms over graph-structured data. However, graph representation\ntechniques---that convert graphs to real-valued vectors for use with neural\nnetworks---are still in their infancy. Recent works have proposed several\napproaches (e.g., graph convolutional networks), but these methods have\ndifficulty scaling and generalizing to graphs with different sizes and shapes.\nWe present Graph2Seq, a new technique that represents vertices of graphs as\ninfinite time-series. By not limiting the representation to a fixed dimension,\nGraph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq\nis also reversible, allowing full recovery of the graph structure from the\nsequences. By analyzing a formal computational model for graph representation,\nwe show that an unbounded sequence is necessary for scalability. Our\nexperimental results with Graph2Seq show strong generalization and new\nstate-of-the-art performance on a variety of graph combinatorial optimization\nproblems."}, {"title": "DESlib: A Dynamic ensemble selection library in Python", "abstract": "DESlib is an open-source python library providing the implementation of\nseveral dynamic selection techniques. The library is divided into three\nmodules: (i) \\emph{dcs}, containing the implementation of dynamic classifier\nselection methods (DCS); (ii) \\emph{des}, containing the implementation of\ndynamic ensemble selection methods (DES); (iii) \\emph{static}, with the\nimplementation of static ensemble techniques. The library is fully documented\n(documentation available online on Read the Docs), has a high test coverage\n(codecov.io) and is part of the scikit-learn-contrib supported projects.\nDocumentation, code and examples can be found on its GitHub page:\nhttps://github.com/scikit-learn-contrib/DESlib."}, {"title": "Tackling Multilabel Imbalance through Label Decoupling and Data Resampling Hybridization", "abstract": "The learning from imbalanced data is a deeply studied problem in standard\nclassification and, in recent times, also in multilabel classification. A\nhandful of multilabel resampling methods have been proposed in late years,\naiming to balance the labels distribution. However these methods have to face a\nnew obstacle, specific for multilabel data, as is the joint appearance of\nminority and majority labels in the same data patterns. We proposed recently a\nnew algorithm designed to decouple imbalanced labels concurring in the same\ninstance, called REMEDIAL (\\textit{REsampling MultilabEl datasets by Decoupling\nhighly ImbAlanced Labels}). The goal of this work is to propose a procedure to\nhybridize this method with some of the best resampling algorithms available in\nthe literature, including random oversampling, heuristic undersampling and\nsynthetic sample generation techniques. These hybrid methods are then\nempirically analyzed, determining how their behavior is influenced by the label\ndecoupling process. As a result, a noteworthy set of guidelines on the combined\nuse of these techniques can be drawn from the conducted experimentation."}, {"title": "Dealing with Difficult Minority Labels in Imbalanced Mutilabel Data Sets", "abstract": "Multilabel classification is an emergent data mining task with a broad range\nof real world applications. Learning from imbalanced multilabel data is being\ndeeply studied latterly, and several resampling methods have been proposed in\nthe literature. The unequal label distribution in most multilabel datasets,\nwith disparate imbalance levels, could be a handicap while learning new\nclassifiers. In addition, this characteristic challenges many of the existent\npreprocessing algorithms. Furthermore, the concurrence between imbalanced\nlabels can make harder the learning from certain labels. These are what we call\n\\textit{difficult} labels. In this work, the problem of difficult labels is\ndeeply analyzed, its influence in multilabel classifiers is studied, and a\nnovel way to solve this problem is proposed. Specific metrics to assess this\ntrait in multilabel datasets, called \\textit{SCUMBLE} (\\textit{Score of\nConcUrrence among iMBalanced LabEls}) and \\textit{SCUMBLELbl}, are presented\nalong with REMEDIAL (\\textit{REsampling MultilabEl datasets by Decoupling\nhighly ImbAlanced Labels}), a new algorithm aimed to relax label concurrence.\nHow to deal with this problem using the R mldr package is also outlined."}, {"title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms", "abstract": "In continuous action domains, standard deep reinforcement learning algorithms\nlike DDPG suffer from inefficient exploration when facing sparse or deceptive\nreward problems. Conversely, evolutionary and developmental methods focusing on\nexploration like Novelty Search, Quality-Diversity or Goal Exploration\nProcesses explore more robustly but are less efficient at fine-tuning policies\nusing gradient descent. In this paper, we present the GEP-PG approach, taking\nthe best of both worlds by sequentially combining a Goal Exploration Process\nand two variants of DDPG. We study the learning performance of these components\nand their combination on a low dimensional deceptive reward problem and on the\nlarger Half-Cheetah benchmark. We show that DDPG fails on the former and that\nGEP-PG improves over the best DDPG variant in both environments. Supplementary\nvideos and discussion can be found at http://frama.link/gep_pg, the code at\nhttp://github.com/flowersteam/geppg."}, {"title": "Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners", "abstract": "In real-world applications of education, an effective teacher adaptively\nchooses the next example to teach based on the learner's current state.\nHowever, most existing work in algorithmic machine teaching focuses on the\nbatch setting, where adaptivity plays no role. In this paper, we study the case\nof teaching consistent, version space learners in an interactive setting. At\nany time step, the teacher provides an example, the learner performs an update,\nand the teacher observes the learner's new state. We highlight that adaptivity\ndoes not speed up the teaching process when considering existing models of\nversion space learners, such as \"worst-case\" (the learner picks the next\nhypothesis randomly from the version space) and \"preference-based\" (the learner\npicks hypothesis according to some global preference). Inspired by human\nteaching, we propose a new model where the learner picks hypotheses according\nto some local preference defined by the current hypothesis. We show that our\nmodel exhibits several desirable properties, e.g., adaptivity plays a key role,\nand the learner's transitions over hypotheses are smooth/interpretable. We\ndevelop efficient teaching algorithms and demonstrate our results via\nsimulation and user studies."}, {"title": "Stronger generalization bounds for deep nets via a compression approach", "abstract": "Deep nets generalize well despite having more parameters than the number of\ntraining samples. Recent works try to give an explanation using PAC-Bayes and\nMargin-based analyses, but do not as yet result in sample complexity bounds\nbetter than naive parameter counting. The current paper shows generalization\nbounds that're orders of magnitude better in practice. These rely upon new\nsuccinct reparametrizations of the trained net --- a compression that is\nexplicit and efficient. These yield generalization bounds via a simple\ncompression-based framework introduced here. Our results also provide some\ntheoretical justification for widespread empirical success in compressing deep\nnets. Analysis of correctness of our compression relies upon some newly\nidentified \\textquotedblleft noise stability\\textquotedblright properties of\ntrained deep nets, which are also experimentally verified. The study of these\nproperties and resulting generalization bounds are also extended to\nconvolutional nets, which had eluded earlier attempts on proving\ngeneralization."}, {"title": "Semi-Supervised Learning on Graphs Based on Local Label Distributions", "abstract": "Most approaches that tackle the problem of node classification consider nodes\nto be similar, if they have shared neighbors or are close to each other in the\ngraph. Recent methods for attributed graphs additionally take attributes of\nneighboring nodes into account. We argue that the class labels of the neighbors\nbear important information and considering them helps to improve classification\nquality. Two nodes which are similar based on class labels in their\nneighborhood do not need to be close-by in the graph and may even belong to\ndifferent connected components. In this work, we propose a novel approach for\nthe semi-supervised node classification. Precisely, we propose a new node\nembedding which is based on the class labels in the local neighborhood of a\nnode. We show that this is a different setting from attribute-based embeddings\nand thus, we propose a new method to learn label-based node embeddings which\ncan mirror a variety of relations between the class labels of neighboring\nnodes. Our experimental evaluation demonstrates that our new methods can\nsignificantly improve the prediction quality on real world data sets."}, {"title": "Gradient Boosting With Piece-Wise Linear Regression Trees", "abstract": "Gradient Boosted Decision Trees (GBDT) is a very successful ensemble learning\nalgorithm widely used across a variety of applications. Recently, several\nvariants of GBDT training algorithms and implementations have been designed and\nheavily optimized in some very popular open sourced toolkits including XGBoost,\nLightGBM and CatBoost. In this paper, we show that both the accuracy and\nefficiency of GBDT can be further enhanced by using more complex base learners.\nSpecifically, we extend gradient boosting to use piecewise linear regression\ntrees (PL Trees), instead of piecewise constant regression trees, as base\nlearners. We show that PL Trees can accelerate convergence of GBDT and improve\nthe accuracy. We also propose some optimization tricks to substantially reduce\nthe training time of PL Trees, with little sacrifice of accuracy. Moreover, we\npropose several implementation techniques to speedup our algorithm on modern\ncomputer architectures with powerful Single Instruction Multiple Data (SIMD)\nparallelism. The experimental results show that GBDT with PL Trees can provide\nvery competitive testing accuracy with comparable or less training time."}, {"title": "Learning Determinantal Point Processes by Corrective Negative Sampling", "abstract": "Determinantal Point Processes (DPPs) have attracted significant interest from\nthe machine-learning community due to their ability to elegantly and tractably\nmodel the delicate balance between quality and diversity of sets. DPPs are\ncommonly learned from data using maximum likelihood estimation (MLE). While\nfitting observed sets well, MLE for DPPs may also assign high likelihoods to\nunobserved sets that are far from the true generative distribution of the data.\nTo address this issue, which reduces the quality of the learned model, we\nintroduce a novel optimization problem, Contrastive Estimation (CE), which\nencodes information about \"negative\" samples into the basic learning model. CE\nis grounded in the successful use of negative information in machine-vision and\nlanguage modeling. Depending on the chosen negative distribution (which may be\nstatic or evolve during optimization), CE assumes two different forms, which we\nanalyze theoretically and experimentally. We evaluate our new model on\nreal-world datasets; on a challenging dataset, CE learning delivers a\nconsiderable improvement in predictive performance over a DPP learned without\nusing contrastive information."}, {"title": "MPC-Inspired Neural Network Policies for Sequential Decision Making", "abstract": "In this paper we investigate the use of MPC-inspired neural network policies\nfor sequential decision making. We introduce an extension to the DAgger\nalgorithm for training such policies and show how they have improved training\nperformance and generalization capabilities. We take advantage of this\nextension to show scalable and efficient training of complex planning policy\narchitectures in continuous state and action spaces. We provide an extensive\ncomparison of neural network policies by considering feed forward policies,\nrecurrent policies, and recurrent policies with planning structure inspired by\nthe Path Integral control framework. Our results suggest that MPC-type\nrecurrent policies have better robustness to disturbances and modeling error."}, {"title": "Constrained Convolutional-Recurrent Networks to Improve Speech Quality with Low Impact on Recognition Accuracy", "abstract": "For a speech-enhancement algorithm, it is highly desirable to simultaneously\nimprove perceptual quality and recognition rate. Thanks to computational costs\nand model complexities, it is challenging to train a model that effectively\noptimizes both metrics at the same time. In this paper, we propose a method for\nspeech enhancement that combines local and global contextual structures\ninformation through convolutional-recurrent neural networks that improves\nperceptual quality. At the same time, we introduce a new constraint on the\nobjective function using a language model/decoder that limits the impact on\nrecognition rate. Based on experiments conducted with real user data, we\ndemonstrate that our new context-augmented machine-learning approach for speech\nenhancement improves PESQ and WER by an additional 24.5% and 51.3%,\nrespectively, when compared to the best-performing methods in the literature."}, {"title": "Variance-based Gradient Compression for Efficient Distributed Deep Learning", "abstract": "Due to the substantial computational cost, training state-of-the-art deep\nneural networks for large-scale datasets often requires distributed training\nusing multiple computation workers. However, by nature, workers need to\nfrequently communicate gradients, causing severe bottlenecks, especially on\nlower bandwidth connections. A few methods have been proposed to compress\ngradient for efficient communication, but they either suffer a low compression\nratio or significantly harm the resulting model accuracy, particularly when\napplied to convolutional neural networks. To address these issues, we propose a\nmethod to reduce the communication overhead of distributed deep learning. Our\nkey observation is that gradient updates can be delayed until an unambiguous\n(high amplitude, low variance) gradient has been calculated. We also present an\nefficient algorithm to compute the variance with negligible additional cost. We\nexperimentally show that our method can achieve very high compression ratio\nwhile maintaining the result model accuracy. We also analyze the efficiency\nusing computation and communication cost models and provide the evidence that\nthis method enables distributed deep learning for many scenarios with commodity\nenvironments."}, {"title": "An Alternative View: When Does SGD Escape Local Minima?", "abstract": "Stochastic gradient descent (SGD) is widely used in machine learning.\nAlthough being commonly viewed as a fast but not accurate version of gradient\ndescent (GD), it always finds better solutions than GD for modern neural\nnetworks.\n  In order to understand this phenomenon, we take an alternative view that SGD\nis working on the convolved (thus smoothed) version of the loss function. We\nshow that, even if the function $f$ has many bad local minima or saddle points,\nas long as for every point $x$, the weighted average of the gradients of its\nneighborhoods is one point convex with respect to the desired solution $x^*$,\nSGD will get close to, and then stay around $x^*$ with constant probability.\nMore specifically, SGD will not get stuck at \"sharp\" local minima with small\ndiameters, as long as the neighborhoods of these regions contain enough\ngradient information. The neighborhood size is controlled by step size and\ngradient noise.\n  Our result identifies a set of functions that SGD provably works, which is\nmuch larger than the set of convex functions. Empirically, we observe that the\nloss surface of neural networks enjoys nice one point convexity properties\nlocally, therefore our theorem helps explain why SGD works so well for neural\nnetworks."}, {"title": "Sequence-to-Sequence Prediction of Vehicle Trajectory via LSTM Encoder-Decoder Architecture", "abstract": "In this paper, we propose a deep learning based vehicle trajectory prediction\ntechnique which can generate the future trajectory sequence of surrounding\nvehicles in real time. We employ the encoder-decoder architecture which\nanalyzes the pattern underlying in the past trajectory using the long\nshort-term memory (LSTM) based encoder and generates the future trajectory\nsequence using the LSTM based decoder. This structure produces the $K$ most\nlikely trajectory candidates over occupancy grid map by employing the beam\nsearch technique which keeps the $K$ locally best candidates from the decoder\noutput. The experiments conducted on highway traffic scenarios show that the\nprediction accuracy of the proposed method is significantly higher than the\nconventional trajectory prediction techniques."}, {"title": "Inductive Framework for Multi-Aspect Streaming Tensor Completion with Side Information", "abstract": "Low rank tensor completion is a well studied problem and has applications in\nvarious fields. However, in many real world applications the data is dynamic,\ni.e., new data arrives at different time intervals. As a result, the tensors\nused to represent the data grow in size. Besides the tensors, in many real\nworld scenarios, side information is also available in the form of matrices\nwhich also grow in size with time. The problem of predicting missing values in\nthe dynamically growing tensor is called dynamic tensor completion. Most of the\nprevious work in dynamic tensor completion make an assumption that the tensor\ngrows only in one mode. To the best of our Knowledge, there is no previous work\nwhich incorporates side information with dynamic tensor completion. We bridge\nthis gap in this paper by proposing a dynamic tensor completion framework\ncalled Side Information infused Incremental Tensor Analysis (SIITA), which\nincorporates side information and works for general incremental tensors. We\nalso show how non-negative constraints can be incorporated with SIITA, which is\nessential for mining interpretable latent clusters. We carry out extensive\nexperiments on multiple real world datasets to demonstrate the effectiveness of\nSIITA in various different settings."}, {"title": "Online convex optimization for cumulative constraints", "abstract": "We propose the algorithms for online convex optimization which lead to\ncumulative squared constraint violations of the form\n$\\sum\\limits_{t=1}^T\\big([g(x_t)]_+\\big)^2=O(T^{1-\\beta})$, where\n$\\beta\\in(0,1)$. Previous literature has focused on long-term constraints of\nthe form $\\sum\\limits_{t=1}^Tg(x_t)$. There, strictly feasible solutions can\ncancel out the effects of violated constraints. In contrast, the new form\nheavily penalizes large constraint violations and cancellation effects cannot\noccur.\n  Furthermore, useful bounds on the single step constraint violation\n$[g(x_t)]_+$ are derived.\n  For convex objectives, our regret bounds generalize existing bounds, and for\nstrongly convex objectives we give improved regret bounds.\n  In numerical experiments, we show that our algorithm closely follows the\nconstraint boundary leading to low cumulative violation."}, {"title": "EA-CG: An Approximate Second-Order Method for Training Fully-Connected Neural Networks", "abstract": "For training fully-connected neural networks (FCNNs), we propose a practical\napproximate second-order method including: 1) an approximation of the Hessian\nmatrix and 2) a conjugate gradient (CG) based method. Our proposed approximate\nHessian matrix is memory-efficient and can be applied to any FCNNs where the\nactivation and criterion functions are twice differentiable. We devise a\nCG-based method incorporating one-rank approximation to derive Newton\ndirections for training FCNNs, which significantly reduces both space and time\ncomplexity. This CG-based method can be employed to solve any linear equation\nwhere the coefficient matrix is Kronecker-factored, symmetric and positive\ndefinite. Empirical studies show the efficacy and efficiency of our proposed\nmethod."}, {"title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization", "abstract": "Conventional wisdom in deep learning states that increasing depth improves\nexpressiveness but complicates optimization. This paper suggests that,\nsometimes, increasing depth can speed up optimization. The effect of depth on\noptimization is decoupled from expressiveness by focusing on settings where\nadditional layers amount to overparameterization - linear neural networks, a\nwell-studied model. Theoretical analysis, as well as experiments, show that\nhere depth acts as a preconditioner which may accelerate convergence. Even on\nsimple convex problems such as linear regression with $\\ell_p$ loss, $p>2$,\ngradient descent can benefit from transitioning to a non-convex\noverparameterized objective, more than it would from some common acceleration\nschemes. We also prove that it is mathematically impossible to obtain the\nacceleration effect of overparametrization via gradients of any regularizer."}, {"title": "Deep Echo State Networks for Diagnosis of Parkinson's Disease", "abstract": "In this paper, we introduce a novel approach for diagnosis of Parkinson's\nDisease (PD) based on deep Echo State Networks (ESNs). The identification of PD\nis performed by analyzing the whole time-series collected from a tablet device\nduring the sketching of spiral tests, without the need for feature extraction\nand data preprocessing. We evaluated the proposed approach on a public dataset\nof spiral tests. The results of experimental analysis show that DeepESNs\nperform significantly better than shallow ESN model. Overall, the proposed\napproach obtains state-of-the-art results in the identification of PD on this\nkind of temporal data."}, {"title": "Leveraged volume sampling for linear regression", "abstract": "Suppose an $n \\times d$ design matrix in a linear regression problem is\ngiven, but the response for each point is hidden unless explicitly requested.\nThe goal is to sample only a small number $k \\ll n$ of the responses, and then\nproduce a weight vector whose sum of squares loss over all points is at most\n$1+\\epsilon$ times the minimum. When $k$ is very small (e.g., $k=d$), jointly\nsampling diverse subsets of points is crucial. One such method called volume\nsampling has a unique and desirable property that the weight vector it produces\nis an unbiased estimate of the optimum. It is therefore natural to ask if this\nmethod offers the optimal unbiased estimate in terms of the number of responses\n$k$ needed to achieve a $1+\\epsilon$ loss approximation.\n  Surprisingly we show that volume sampling can have poor behavior when we\nrequire a very accurate approximation -- indeed worse than some i.i.d. sampling\ntechniques whose estimates are biased, such as leverage score sampling. We then\ndevelop a new rescaled variant of volume sampling that produces an unbiased\nestimate which avoids this bad behavior and has at least as good a tail bound\nas leverage score sampling: sample size $k=O(d\\log d + d/\\epsilon)$ suffices to\nguarantee total loss at most $1+\\epsilon$ times the minimum with high\nprobability. Thus, we improve on the best previously known sample size for an\nunbiased estimator, $k=O(d^2/\\epsilon)$.\n  Our rescaling procedure leads to a new efficient algorithm for volume\nsampling which is based on a determinantal rejection sampling technique with\npotentially broader applications to determinantal point processes. Other\ncontributions include introducing the combinatorics needed for rescaled volume\nsampling and developing tail bounds for sums of dependent random matrices which\narise in the process."}, {"title": "Multi-resolution Tensor Learning for Large-Scale Spatial Data", "abstract": "High-dimensional tensor models are notoriously computationally expensive to\ntrain. We present a meta-learning algorithm, MMT, that can significantly speed\nup the process for spatial tensor models. MMT leverages the property that\nspatial data can be viewed at multiple resolutions, which are related by\ncoarsening and finegraining from one resolution to another. Using this\nproperty, MMT learns a tensor model by starting from a coarse resolution and\niteratively increasing the model complexity. In order to not \"over-train\" on\ncoarse resolution models, we investigate an information-theoretic fine-graining\ncriterion to decide when to transition into higher-resolution models. We\nprovide both theoretical and empirical evidence for the advantages of this\napproach. When applied to two real-world large-scale spatial datasets for\nbasketball player and animal behavior modeling, our approach demonstrate 3 key\nbenefits: 1) it efficiently captures higher-order interactions (i.e., tensor\nlatent factors), 2) it is orders of magnitude faster than fixed resolution\nlearning and scales to very fine-grained spatial resolutions, and 3) it\nreliably yields accurate and interpretable models."}, {"title": "Online Learning with an Unknown Fairness Metric", "abstract": "We consider the problem of online learning in the linear contextual bandits\nsetting, but in which there are also strong individual fairness constraints\ngoverned by an unknown similarity metric. These constraints demand that we\nselect similar actions or individuals with approximately equal probability\n(arXiv:1104.3913), which may be at odds with optimizing reward, thus modeling\nsettings where profit and social policy are in tension. We assume we learn\nabout an unknown Mahalanobis similarity metric from only weak feedback that\nidentifies fairness violations, but does not quantify their extent. This is\nintended to represent the interventions of a regulator who \"knows unfairness\nwhen he sees it\" but nevertheless cannot enunciate a quantitative fairness\nmetric over individuals. Our main result is an algorithm in the adversarial\ncontext setting that has a number of fairness violations that depends only\nlogarithmically on $T$, while obtaining an optimal $O(\\sqrt{T})$ regret bound\nto the best fair policy."}, {"title": "Constant Regret, Generalized Mixability, and Mirror Descent", "abstract": "We consider the setting of prediction with expert advice; a learner makes\npredictions by aggregating those of a group of experts. Under this setting, and\nfor the right choice of loss function and \"mixing\" algorithm, it is possible\nfor the learner to achieve a constant regret regardless of the number of\nprediction rounds. For example, a constant regret can be achieved for\n\\emph{mixable} losses using the \\emph{aggregating algorithm}. The\n\\emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of\nalgorithms parameterized by convex functions on simplices (entropies), which\nreduce to the aggregating algorithm when using the \\emph{Shannon entropy}\n$\\operatorname{S}$. For a given entropy $\\Phi$, losses for which a constant\nregret is possible using the \\textsc{GAA} are called $\\Phi$-mixable. Which\nlosses are $\\Phi$-mixable was previously left as an open question. We fully\ncharacterize $\\Phi$-mixability and answer other open questions posed by\n\\cite{Reid2015}. We show that the Shannon entropy $\\operatorname{S}$ is\nfundamental in nature when it comes to mixability; any $\\Phi$-mixable loss is\nnecessarily $\\operatorname{S}$-mixable, and the lowest worst-case regret of the\n\\textsc{GAA} is achieved using the Shannon entropy. Finally, by leveraging the\nconnection between the \\emph{mirror descent algorithm} and the update step of\nthe GAA, we suggest a new \\emph{adaptive} generalized aggregating algorithm and\nanalyze its performance in terms of the regret bound."}, {"title": "The Description Length of Deep Learning Models", "abstract": "Solomonoff's general theory of inference and the Minimum Description Length\nprinciple formalize Occam's razor, and hold that a good model of data is a\nmodel that is good at losslessly compressing the data, including the cost of\ndescribing the model itself. Deep neural networks might seem to go against this\nprinciple given the large number of parameters to be encoded.\n  We demonstrate experimentally the ability of deep neural networks to compress\nthe training data even when accounting for parameter encoding. The compression\nviewpoint originally motivated the use of variational methods in neural\nnetworks. Unexpectedly, we found that these variational methods provide\nsurprisingly poor compression bounds, despite being explicitly built to\nminimize such bounds. This might explain the relatively poor practical\nperformance of variational methods in deep learning. On the other hand, simple\nincremental encoding methods yield excellent compression values on deep\nnetworks, vindicating Solomonoff's approach."}, {"title": "Local Differential Privacy for Evolving Data", "abstract": "There are now several large scale deployments of differential privacy used to\ncollect statistical information about users. However, these deployments\nperiodically recollect the data and recompute the statistics using algorithms\ndesigned for a single use. As a result, these systems do not provide meaningful\nprivacy guarantees over long time scales. Moreover, existing techniques to\nmitigate this effect do not apply in the \"local model\" of differential privacy\nthat these systems use.\n  In this paper, we introduce a new technique for local differential privacy\nthat makes it possible to maintain up-to-date statistics over time, with\nprivacy guarantees that degrade only in the number of changes in the underlying\ndistribution rather than the number of collection periods. We use our technique\nfor tracking a changing statistic in the setting where users are partitioned\ninto an unknown collection of groups, and at every time period each user draws\na single bit from a common (but changing) group-specific distribution. We also\nprovide an application to frequency and heavy-hitter estimation."}, {"title": "On Equitable Coloring Parameters of Certain Cycle Related Graphs", "abstract": "Coloring the vertices of a graph G subject to given conditions can be\nconsidered as a random experiment and corresponding to this experiment, a\ndiscrete random variable X can be defined as the colour of a vertex chosen at\nrandom, with respect to the given type of colouring of G and a probability mass\nfunction for this random variable can be defined accordingly. A proper coloring\nC of a graph G, which assigns colors to the vertices of G such that the numbers\nof vertices in any two colour classes differ by at most one, is called an\nequitable colouring of G. In this paper, we study two statistical parameters of\ncertain cycle related graphs, with respect to their equitable colorings."}, {"title": "Differential Equation over Banach Algebra", "abstract": "In the book, I considered differential equations of order $1$ over Banach\n$D$\\Hyph algebra: differential equation solved with respect to the derivative;\nexact differential equation; linear homogeneous equation. I considered examples\nof differential equations in quaternion algebra. In order to study homogeneous\nsystem of linear differential equations, I considered vector space over\ndivision $D$-algebra, solving of linear equations over division $D$-algebra and\nthe theory of eigenvalues in non commutative division $D$-algebra."}, {"title": "Quadrature rules for $C^0$, $C^1$ splines, the real line, and the five (5) families", "abstract": "The five (5) families of quadrature rules with periods of one or two\nintervals for the real line and spline classes $C^0$, $C^1$ are presented. The\nformulae allow one to calculate the points or weights of these quadrature rules\nin a very simple manner as for the classical Gauss-Legendre rules."}, {"title": "On Monotonous Separately Continuous Functions", "abstract": "Let ${\\mathbb T}=({\\bf T},\\leq)$ and ${\\mathbb T}_{1}=({\\bf T}_{1},\\leq_{1})$\nbe linearly ordered sets and $\\mathscr{X}$ be a topological space. The main\nresult of the paper is the following:\n  If function $\\boldsymbol{f}(t,x):{\\bf T}\\times\\mathscr{X}\\mapsto{\\bf T}_{1}$\nis continuous in each variable (\"$t$\"and \"$x$\") separately and function\n$\\boldsymbol{f}_{x}(t)=\\boldsymbol{f}(t,x)$ is monotonous on ${\\bf T}$ for\nevery $x\\in\\mathscr{X}$, then $\\boldsymbol{f}$ is continuous mapping from ${\\bf\nT}\\times\\mathscr{X}$ to ${\\bf T}_{1}$, where ${\\bf T}$ and ${\\bf T}_{1}$ are\nconsidered as topological spaces under the order topology and ${\\bf\nT}\\times\\mathscr{X}$ is considered as topological space under the Tychonoff\ntopology on the Cartesian product of topological spaces ${\\bf T}$ and\n$\\mathscr{X}$."}, {"title": "Fixed points of multivalued contractions via generalized class of simulation functions", "abstract": "In this paper, considering a wider class of simulation functions some fixed\npoint results for multivalued mappings in $\\alpha$-complete metric spaces have\nbeen presented. Results obtained in this paper extend and generalize some\nwell-known fixed point results of the literature. Some examples and consequence\nare given to illustrate the usability of the theory."}, {"title": "Flat ideals in the unit interval with the canonical fuzzy order", "abstract": "A characterization of flat ideals in the unit interval with the canonical\nfuzzy order is obtained with the help of the ordinal sum decomposition of\ncontinuous t-norms. This characterization will be useful in the study of\ntopological and domain theoretic properties of fuzzy orders."}, {"title": "Postulating the theory of experience and chance as a theory of co~events (co~beings)", "abstract": "The paper aim is the axiomatic justification of the theory of experience and\nchance, one of the dual halves of which is the Kolmogorov probability theory.\nThe author's main idea was the natural inclusion of Kolmogorov's axiomatics of\nprobability theory in a number of general concepts of the theory of experience\nand chance. The analogy between the measure of a set and the probability of an\nevent has become clear for a long time. This analogy also allows further\nevolution: the measure of a set is completely analogous to the believability of\nan event. In order to postulate the theory of experience and chance on the\nbasis of this analogy, you just need to add to the Kolmogorov probability\ntheory its dual reflection - the believability theory, so that the theory of\nexperience and chance could be postulated as the certainty\n(believability-probability) theory on the Cartesian product of the probability\nand believability spaces, and the central concept of the theory is the new\nnotion of co~event as a measurable binary relation on the Cartesian product of\nsets of elementary incomes and elementary outcomes. Attempts to build the\nfoundations of the theory of experience and chance from this general point of\nview are unknown to me, and the whole range of ideas presented here has not yet\nacquired popularity even in a narrow circle of specialists; in addition, there\nwas still no complete system of the postulates of the theory of experience and\nchance free from unnecessary complications. The main result of this work is the\naxiom of co~event, intended for the sake of constructing a theory formed by\ndual theories of believabilities and probabilities, each of which itself is\npostulated by its own Kolmogorov system of axioms. Here a preference is given\nto a system of postulates that is able to describe in a simple manner the\nresults of what I call an experienced-random experiment."}, {"title": "Extending the Algebraic Manipulability of Differentials", "abstract": "Treating differentials as independent algebraic units have a long history of\nuse and abuse. It is generally considered problematic to treat the derivative\nas a fraction of differentials rather than as a holistic unit acting as a\nlimit, though for practical reasons it is often done for the first derivative.\nHowever, using a revised notation for the second and higher derivatives will\nallow for the ability to treat differentials as independent units for a much\nlarger number of cases."}, {"title": "Generalizations of Banach and Kannan Fixed point theorems in b_{v}(s) metric spaces", "abstract": "Generalizations of a metric space is one of the most important research areas\nin mathematics. In literature ,there are several generalized metric spaces. The\nlatest generalized metric space is b_{v}(s) metric space which is introduced by\nMitrovic and Radenovic in 2017. In this paper, we prove Kannan fixed point\ntheorem and generalize Banach fixed point theorem for weakly contractive\nmappings in b_{v}(s) metric spaces. Our results extend and generalize some\ncorresponding result."}, {"title": "One categorization of microtonal scales", "abstract": "This study considers rational approximations of musical constant\n$\\beta=\\log_2(3/2)$, which defines perfect fifth. This constant has been the\nsubject of the numerous studies, and this paper determines quality of rational\napproximations in regards to absolute error. We analysed convergents and\nsecondary convergents (some of these are the best Huygens approximations).\nEspecially, we determined quality of the secondary convergents which are not\nthe best Huygens approximations - in this paper we called them non-convergents\napproximations. Some of the microtonal scales have been positioned and\ndetermined by using non-convergents approximation of music constant which\ndefines perfect fifth."}, {"title": "Independence of the grossone-based infinity methodology from non-standard analysis and comments upon logical fallacies in some texts asserting the opposite", "abstract": "This commentary considers non-standard analysis and a recently introduced\ncomputational methodology based on the notion of \\G1 (this symbol is called\n\\emph{grossone}). The latter approach was developed with the intention to allow\none to work with infinities and infinitesimals numerically in a unique\ncomputational framework and in all the situations requiring these notions.\nNon-standard analysis is a classical purely symbolic technique that works with\nultrafilters, external and internal sets, standard and non-standard numbers,\netc. In its turn, the \\G1-based methodology does not use any of these notions\nand proposes a more physical treatment of mathematical objects separating the\nobjects from tools used to study them. It both offers a possibility to create\nnew numerical methods using infinities and infinitesimals in floating-point\ncomputations and allows one to study certain mathematical objects dealing with\ninfinity more accurately than it is done traditionally. In these notes, we\nexplain that even though both methodologies deal with infinities and\ninfinitesimals, they are independent and represent two different philosophies\nof Mathematics that are not in a conflict. It is proved that texts\n\\cite{Flunks, Gutman_Kutateladze_2008, Kutateladze_2011} asserting that the\n\\G1-based methodology is a part of non-standard analysis unfortunately contain\nseveral logical fallacies. Their attempt to prove that the \\G1-based\nmethodology is a part of non-standard analysis is similar to trying to show\nthat constructivism can be reduced to the traditional mathematics."}, {"title": "An elementary conjecture which implies the Goldbach conjecture", "abstract": "Let $p_{1}$, ..., $p_{k}$ be the first $k$ odd primes in succession. Let $n$\nbe an even integer such that $n > p_{k}$. We conjecture that if none of $n -\np_{1}$, ..., $n - p_{k}$ are prime, then at least one of them has a prime\nfactor which is greater than or equal to $p_{k}$. In this brief note, we\nobserve that Goldbach's conjecture follows from this conjecture."}, {"title": "Some Properties of Fibonacci-Sum Set-Graphs", "abstract": "In this paper we study some properties of Fibonacci-sum set-graphs. The\naforesaid graphs are an extension of the notion of Fibonacci-sum graphs to the\nnotion of set-graphs. The colouring of Fibonacci-sum graphs is also discussed.\nA number of challenging research problems are posed in the conclusion."}, {"title": "Calculus without Limit Theory", "abstract": "This paper establishes calculus upon two physical facts: (1) any average\nvelocity is always between two instantaneous velocities, and (2) the motion of\nan object is determined once its velocity has been determined. It directly\ndefines derivative and definite integral on an ordered field, proves the\nfundamental theorem of calculus with no auxiliary conditions, easily reveals\nthe common properties of derivatives, and obtains differentiation formulas for\nelementary functions. Further discussion shows that the new definitions are in\naccord with the traditional concepts for continuously differentiable functions.\nThis is a result of the authors' research in the field of educational\nmathematics, which hopes to provide a more elementary and effective way to\nteach calculus."}, {"title": "Some remarks on metrics induced by a fuzzy metric", "abstract": "We introduce a crisp metric $d_M$ as the common limit of two different nets\n$(\\Delta_{M,\\lambda})$ and $(\\delta_{M,\\lambda})$ of crisp metrics induced by a\nfuzzy metric $M$ and prove that the existence of each of these limits is\nequivalent to that of the other and it is characterized by another condition on\nthe original fuzzy metric $M$. We also derive some of the properties of these\napproximate metrics $\\Delta_\\lambda$ and $\\delta_\\lambda$. On the other hand,\nfor a given a crisp metric $d$, establish that the fuzzy metric representing\n$M_d$ with values in $\\{0,1\\}$ and $d$ are compatible with the same topology.\nFurther, we prove that if a crisp metric $d$ induces a fuzzy metric $M_d$, then\nall the approximate crisp metrics $\\Delta_{M,\\lambda}$ and $\\delta_{M,\\lambda}$\ninduced by this fuzzy metric are equal to the original metric $d$."}, {"title": "A Smooth Curve as a Fractal Under the Third Definition", "abstract": "It is commonly believed in the literature that smooth curves, such as\ncircles, are not fractal, and only non-smooth curves, such as coastlines, are\nfractal. However, this paper demonstrates that a smooth curve can be fractal,\nunder the new, relaxed, third definition of fractal - a set or pattern is\nfractal if the scaling of far more small things than large ones recurs at least\ntwice. The scaling can be rephrased as a hierarchy, consisting of numerous\nsmallest, a very few largest, and some in between the smallest and the largest.\nThe logarithmic spiral, as a smooth curve, is apparently fractal because it\nbears the self-similar property, or the scaling of far more small squares than\nlarge ones recurs multiple times, or the scaling of far more small bends than\nlarge ones recurs multiple times. A half-circle or half-ellipse and the UK\ncoastline (before or after smooth processing) are fractal, if the scaling of\nfar more small bends than large ones recurs at least twice.\n  Keywords: Third definition of fractal, head/tail breaks, bends, ht-index,\nscaling hierarchy"}, {"title": "The BiEntropy of Some Knots on the Simple Cubic Lattice", "abstract": "Binary representations of the trefoil and other knots of up to ten crossings\nin the simple cubic lattice were created. The BiEntropy of each knot was\ncomputed using a variety of binary encodings and compared against controls.\nThis showed that binary encoded knots are highly disordered information\nobjects. The BiEntropy of knots on the simple cubic lattice increases slightly\nas the number of crossings and length of encoding increases. We show that the\nnon-alternating knots of nine and ten crossings are more disordered than the\nalternating knots of nine and ten crossings."}, {"title": "Arithmetic summable sequence space over non-Newtonian field", "abstract": "Recently Ruckle \\cite{RuckleArithmeticalSummability} introduced the theory of\narithmetical summability suggested by the sum $ \\sum_{k|m}f(k) $ as $ k $\nranges over the divisors of $m$ including $ 1 $ and $ m .$ Following Ruckle\n\\cite{RuckleArithmeticalSummability} we construct the sequence space $ AS(G) $\nand $ AC(G) $ of arithmetic summable and arithmetic convergent sequences in the\nsense of geometric calculus and derive interesting results in the geometric\nfield."}, {"title": "On a generalized theorem of de Bruijn and Erdös in d-dimensional Fuzzy Linear Spaces", "abstract": "In this study we follow a new framework for the theory that offers us, other\nthan traditional, a new angle to observe and investigate some relations between\nfinite sets, F-lattice L and their elements. The theory is based on the Fuzzy\nLinear Spaces (FLS) S=(N,D). In this case, to operate on these spaces the\nnecessary preliminaries, concepts and operations in lattices relative to FLS\nare introduced. Some definitions, such that k-fuzzy point, k-fuzzy line are\ngiven. Then we correspond these definitions to the definitions in usually\nlinear spaces. We investigate some combinatorics properties of FLS. In some\nexamples in the case where ILI=3*. We see some differences. In general, taking\nan ordered lattice Ln={0,a1,a2,...,an,1} we observe how some combinatorics\nformulas and properties are changed. In FLS the dimension concept is a set. We\nproduce some general formulas by using some trivial examples. Furthermore, we\ngeneralize de Bruijn-Erd\\\"os Theorem in [2]."}, {"title": "Frechet bounds of the 1-st kind for sets of half-rare events", "abstract": "Frechet bounds of the 1-st kind for sets of events and its main properties\nare considered. The lemma on not more than two nonzero values of lower\nFrechet-bounds of the 1-st kind for a set of half-rare events is proved with\nthe corollary on the analogous assertion for sets of events with arbitrary\nevent-probability distributions."}, {"title": "Complete sets", "abstract": "In this paper we introduce the concept of completeness of sets. We study this\nproperty on the set of integers. We examine how this property is preserved as\nwe carry out various operations compatible with sets. We also introduce the\nproblem of counting the number of complete subsets of any given set. That is,\ngiven any interval of integers $\\mathcal{H}:=[1,N]$ and letting\n$\\mathcal{C}(N)$ denotes the complete set counting function, we establish the\nlower bound $\\mathcal{C}(N)\\gg N\\log N$."}, {"title": "On involution $le$-semigroups", "abstract": "We deal with involution ordered semigroups possessing a greatest element, we\nintroduce the concepts of $*$-regularity, $*$-intra-regularity, $*$-bi-ideal\nelement and $*$-quasi-ideal element in this type of semigroups and, using the\nright and left ideal elements, we give relations between the regularity and\n$*$-regularity, between intra-regularity and $*$-intra-regularity. Finally, we\nprove that in an involution $*$-regular $\\vee e$-semigroup every $*$-bi-ideal\nelement can be considered as a product of a right and a left ideal element, we\ndescribe the form of the filter generated by an element of an involution\n$*$-intra-regular $poe$-semigroup $S$, showing that every $\\cal N$-class of $S$\nhas a greatest element."}, {"title": "On a certain identity involving the Gamma function", "abstract": "The goal of this paper is to prove the identity \\begin{align}\\sum\n\\limits_{j=0}^{\\lfloor\ns\\rfloor}\\frac{(-1)^j}{s^j}\\eta_s(j)+\\frac{1}{e^{s-1}s^s}\\sum\n\\limits_{j=0}^{\\lfloor\ns\\rfloor}(-1)^{j+1}\\alpha_s(j)+\\bigg(\\frac{1-((-1)^{s-\\lfloor s\\rfloor\n+2})^{1/(s-\\lfloor s\\rfloor +2)}}{2}\\bigg)\\nonumber \\\\ \\bigg(\\sum\n\\limits_{j=\\lfloor s\\rfloor\n+1}^{\\infty}\\frac{(-1)^j}{s^j}\\eta_s(j)+\\frac{1}{e^{s-1}s^s}\\sum\n\\limits_{j=\\lfloor s\\rfloor\n+1}^{\\infty}(-1)^{j+1}\\alpha_s(j)\\bigg)=\\frac{1}{\\Gamma(s+1)},\\nonumber\n\\end{align}where \\begin{align}\\eta_s(j):=\\bigg(e^{\\gamma (s-j)}\\prod\n\\limits_{m=1}^{\\infty}\\bigg(1+\\frac{s-j}{m}\\bigg)\\nonumber\n\\\\e^{-(s-j)/m}\\bigg)\\bigg(2+\\log s-\\frac{j}{s}+\\sum\n\\limits_{m=1}^{\\infty}\\frac{s}{m(s+m)}-\\sum\n\\limits_{m=1}^{\\infty}\\frac{s-j}{m(s-j+m)}\\bigg), \\nonumber \\end{align}and\n\\begin{align}\\alpha_s(j):=\\bigg(e^{\\gamma (s-j)}\\prod\n\\limits_{m=1}^{\\infty}\\bigg(1+\\frac{s-j}{m}\\bigg)e^{-(s-j)/m}\\bigg)\\bigg(\\sum\n\\limits_{m=1}^{\\infty}\\frac{s}{m(s+m)}-\\sum\n\\limits_{m=1}^{\\infty}\\frac{s-j}{m(s-j+m)}\\bigg),\\nonumber \\end{align}where\n$\\Gamma(s+1)$ is the Gamma function defined by $\\Gamma(s):=\\int\n\\limits_{0}^{\\infty}e^{-t}t^{s-1}dt$ and $\\gamma =\\lim\n\\limits_{n\\longrightarrow \\infty}\\bigg(\\sum \\limits_{k=1}^{n}\\frac{1}{k}-\\log\nn\\bigg)=0.577215664\\cdots $ is the Euler-Mascheroni constant."}, {"title": "Analytic Continuation of $ζ(s)$ Violates the Law of Non-Contradiction (LNC)", "abstract": "The Dirichlet series of $\\zeta(s)$ was long ago proven to be divergent\nthroughout half-plane $\\text{Re}(s)\\le1$. If also Riemann's proposition is\ntrue, that there exists an \"expression\" of $\\zeta(s)$ that is convergent at all\n$s$ (except at $s=1$), then $\\zeta(s)$ is both divergent and convergent\nthroughout half-plane $\\text{Re}(s)\\le1$ (except at $s=1$). This result\nviolates all three of Aristotle's \"Laws of Thought\": the Law of Identity (LOI),\nthe Law of the Excluded Middle (LEM), and the Law of Non-Contradition (LNC). In\nclassical and intuitionistic logics, the violation of LNC also triggers the\n\"Principle of Explosion\" / \\textit{Ex Contradictione Quodlibet} (ECQ). In\naddition, the Hankel contour used in Riemann's analytic continuation of\n$\\zeta(s)$ violates Cauchy's integral theorem, providing another proof of the\ninvalidity of Riemann's $\\zeta(s)$. Riemann's $\\zeta(s)$ is one of the\n$L$-functions, which are all invalid due to analytic continuation. This result\nrenders unsound all theorems (e.g. Modularity, Fermat's last) and conjectures\n(e.g. BSD, Tate, Hodge, Yang-Mills) that assume that an $L$-function (e.g.\nRiemann's $\\zeta(s)$) is valid. We also show that the Riemann Hypothesis (RH)\nis not \"non-trivially true\" in classical logic, intuitionistic logic, or\nthree-valued logics (3VLs) that assign a third truth-value to paradoxes\n(Bochvar's 3VL, Priest's $LP$)."}, {"title": "From Vectors to Geometric Algebra", "abstract": "Geometric algebra is the natural outgrowth of the concept of a vector and the\naddition of vectors. After reviewing the properties of the addition of vectors,\na multiplication of vectors is introduced in such a way that it encodes the\nfamous Pythagorean theorem. Synthetic proofs of theorems in Euclidean geometry\ncan then be replaced by powerful algebraic proofs. Whereas we largely limit our\nattention to 2 and 3 dimensions, geometric algebra is applicable in any number\nof dimensions, and in both Euclidean and non-Euclidean geometries."}, {"title": "Studies in Tours of Knight on Rectangular Boards", "abstract": "The author has constructed and enumerated tours of knight having various\nmagic properties on 4 x n and 6 x n boards. 16 magic tours of knight have been\ndiscovered on 4 x 18 board, 88 on 4 x 20 board, 464 on 4 x 22 board, 2076 on 4\nx 24 board, 9904 on 4 x 26 board and 47456 on 4 x 28 board. Magic tours exist\non all boards of size 4 x 2k for k > 8. Quasi-magic tour exists on 6 x 11\nboard. 8 magic tours of knight have been discovered on 6 x 12 board and magic\ntours exist on all boards of size 6 x 4k for k > 2."}, {"title": "Differential Operator Method of Finding A Particular Solution to An Ordinary Nonhomogeneous Linear Differential Equation with Constant Coefficients", "abstract": "We systematically introduce the idea of applying differential operator method\nto find a particular solution of an ordinary nonhomogeneous linear differential\nequation with constant coefficients when the nonhomogeneous term is a\npolynomial function, exponential function, sine function, cosine function or\nany possible product of these functions. In particular, different from the\ndifferential operator method introduced in literature, we propose and highlight\nutilizing the definition of the inverse of differential operator to determine a\nparticular solution. We suggest that this method should be introduced in\ntextbooks and widely used for determining a particular solution of an ordinary\nnonhomogeneous linear differential equation with constant coefficients in\nparallel to the method of undetermined coefficients."}, {"title": "Optimal Solution of Nonlinear Fuzzy Optimization Problem under Linear Order Relation", "abstract": "Multi-variable nonlinear fuzzy optimization problem is considered under\nlinear order relation on fuzzy numbers. Using gH-differentiability of a\nfuzzy-valued function $\\tilde{f}$, new necessary and sufficient optimality\nconditions are proposed. The optimality conditions are obtained without putting\nadditional conditions on fuzzy-valued functions like, convexity,\nquasi-convexity, pseudo-convexity. Optimum solution of the fuzzy optimization\nproblem is obtained based on the optimality conditions. Illustrations and a\ncase study are given to explain the numerical applications of the proposed\nresults. Comparison of optimality conditions from existing literature is given."}, {"title": "Oriented Convex Containers of Polygons", "abstract": "We consider the optimal containment of polygonal regions within convex\ncontainers with the special property of 'orientedness' - an oriented region\nenables us to choose a preferred direction on the plane (this direction is not\nnecessarily an axis of symmetry) - and derive preliminary results."}, {"title": "Fixed points for $G$-cyclic $(φ-ψ)$-Kannan and $G$-cyclic $(φ -ψ)$-Chatterjea contractions in $G$-metric spaces", "abstract": "Definitions of what are called $G$-cyclic $\\left( \\phi -\\psi \\right)$-Kannan\ncontraction and $G$-cyclic $\\left( \\phi -\\psi\\right)$-Chatterjea contraction\nare introduced in this paper. We use these new concepts to establish new fixed\npoint results in the context of complete generalized metric spaces. These\nresults are new generalizations and extensions of the Kannan and Chatterjea\nfixed point theorems and are generalized versions of some fixed point results\nproved in the literature. The analysis and theory are illustrated by some\nexamples."}, {"title": "Evaluation of weighted Fibonacci sums of a certain type", "abstract": "We derive a formula for the evaluation of weighted generalized Fibonacci sums\nof the type $S_k^n (w,r) = \\sum_{j = 0}^k {w^j j^r G_j{}^n }$. Several explicit\nevaluations are presented as examples."}, {"title": "New Proofs of Triangle Inequalities", "abstract": "We give three new proofs of the triangle inequality in Euclidean Geometry.\nThere seems to be only one known proof at the moment. It is due to properties\nof triangles, but our proofs are due to circles or ellipses. We aim to prove\nthe triangle inequality as simple as possible without using properties of\ntriangles."}, {"title": "On Chromatic Core Subgraph of Simple Graphs", "abstract": "If distinct colours represent distinct technology types that are placed at\nthe vertices of a simple graph in accordance to a minimum proper colouring, a\ndisaster recovery strategy could rely on an answer to the question: \"What is\nthe maximum destruction, if any, the graph (a network) can undergo while\nensuring that at least one of each technology type remain, in accordance to a\nminimum proper colouring of the remaining induced subgraph.\" In this paper, we\nintroduce the notion of a chromatic core subgraph $H$ of a given simple graph\n$G$ in answer to the stated problem. Since for any subgraph $H$ of $G$ it holds\nthat $\\chi(H) \\leq \\chi(G)$, the problem is well defined."}, {"title": "A Note on the Classification of Permutation Matrix", "abstract": "This paper is concentrated on the classification of permutation matrix with\nthe permutation similarity relation, mainly about the canonical form of a\npermutational similar equivalence class, the cycle matrix decomposition of a\npermutation matrix and the cycle factorization of a permutation matrix or\nmonomial matrix."}, {"title": "Qualitative analysis of differential equations", "abstract": "Here I introduce basic methods of qualitative analysis of differential\nequations used in mathematical biology for people with minimal mathematical\nbackground."}, {"title": "The diagonalization method and Brocard's problem", "abstract": "In this paper we introduce and develop the method of diagonalization of\nfunctions $f:\\mathbb{N}\\longrightarrow \\mathbb{R}$. We apply this method to\nshow that the equations of the form $\\Gamma_r(n)+k=m^2$ has a finite number of\nsolutions $n\\in \\mathbb{N}$ with $n>r$ for any fixed $k,r\\in \\mathbb{N}$, where\n$\\Gamma_r(n)=n(n-1)\\cdots (n-r)$ denotes the $r^{th}$ truncated Gamma function."}, {"title": "Homological Solution of the Riemann-Lanczos and Weyl-Lanczos Problems in Arbitrary Dimension", "abstract": "When ${\\cal{D}}$ is a linear partial differential operator of any order, a\ndirect problem is to look for an operator ${\\cal{D}}_1$ generating the\ncompatibility conditions (CC) ${\\cal{D}}_1\\eta=0$ of ${\\cal{D}}\\xi=\\eta$. We\nmay thus construct a differential sequence with successive operators\n${\\cal{D}},{\\cal{D}}_1,{\\cal{D}}_2, ...$, where each operator is generating the\nCC of the previous one. Introducing the formal adjoint $ad( )$, we have\n${\\cal{D}}_i\\circ {\\cal{D}}_{i-1}=0 \\Rightarrow ad({\\cal{D}}_{i-1}) \\circ\nad({\\cal{D}}_i)=0$ but $ad({\\cal{D}}_{i-1})$ may not generate all the CC of\n$ad({\\cal{D}}_i)$. When $D=K[d_1,...,d_n]=K[d]$ is the (non-commutative) ring\nof differential operators with coefficients in a differential field $K$, it\ngives rise by residue to a differential module $M$ over $D$. The homological\nextension modules $ext^i(M)=ext^i_D(M,D)$ with $ext^0(M)=hom_D(M,D)$ only\ndepend on $M$ and are measuring the above gaps, independently of the previous\ndifferential sequence.The purpose of this rather technical paper is to compute\nthem for certain Lie operators involved in the formal theory of Lie\npseudogroups in arbitrary dimension $n$. In particular, we prove that the\nextension modules highly depend on the Vessiot structure constants $c$. When\none is dealing with a Lie group of transformations or, equivalently, when\n${\\cal{D}}$ is a Lie operator of finite type, then we shall prove that\n$ext^i(M)=0, \\forall 0\\leq i \\leq n-1$. It will follow that the Riemann-Lanczos\nand Weyl-Lanczos problems just amount to prove such a result for $i=2$ and\narbitrary $n$ when ${\\cal{D}}$ is the Killing or conformal Killing operator. We\nfinally prove that ${ext}^i(M)=0, \\forall i\\geq 1$ for the Lie operator of\ninfinitesimal contact transformations with arbitrary $n=2p+1$. Most of these\nnew results have been checked by means of computer algebra."}, {"title": "Fuzzy soft seperation axioms with sense of Ganguly and Saha", "abstract": "Tanay and Kandemir <cite>TK</cite> introduced the topological structure of\nfuzzy soft sets. In 2013, Manatha and Das <cite>md</cite> defined seperation\naxioms on fuzzy soft topological spaces. In this paper, we generalized form of\nthe seperation axioms.using fuzzy soft quasi-coincidence with sense of Ganguly\nand Saha <cite>GS</cite>. By using this notions, we also give some basic\ntheorems of seperation axioms in classical topological spaces."}, {"title": "On the Consistency of the Arithmetic System", "abstract": "In this paper we establish that the well-known Arithmetic System is\nconsistent in the traditional sense. The proof is done within this Arithmetic\nSystem."}, {"title": "Soft elementary compact in soft elementary topology", "abstract": "In a recent paper, Chiney and Samanta have introduced a new definition of\nsoft topology, using the soft elementary intersection and union. In this paper,\nbasing at this approach, we introduce a definition of soft elementary compact\nset, and space. We introduce also some proprieties of the soft elementary\ncompactness, and we prove the soft elementary version of Baire theorem."}, {"title": "Pythagoras, Binomial, and de Moivre Revisited Through Differential Equations", "abstract": "The classical Pythagoras theorem, binomial theorem, de Moivre's formula, and\nnumerous other deductions are made using the uniqueness theorem for the initial\nvalue problems in linear ordinary differential equations."}, {"title": "Primes of the form $p=1+n!\\sum n,$ for some $n\\in\\mathbb{N}^{+}$", "abstract": "The purpose of this note is to report on the discovery of the primes of the\nform $p=1+n!\\sum n$, for some natural numbers $n>0$. The number of digits in\nthe prime p are approximately equal to $\\lfloor log_{10}(1+n!\\sum n)\\rceil+1$."}, {"title": "Projective Line Revisited", "abstract": "This article provides a new perspective on the geometry of a projective line,\nwhich helps clarify and illuminate some classical results about projective\nplane. As part of the same train of ideas, the article also provides a proof of\nthe nine-point circle theorem valid for any affine plane over any field of\ncharacteristic different from 2."}, {"title": "On Chromatic Curling Number of Graphs", "abstract": "The curling number of a graph G is defined as the number of times an element\nin the degree sequence of G appears the maximum. Graph colouring is an\nassignment of colours, labels or weights to the vertices or edges of a graph. A\ncolouring $\\mathcal{C}$ of colours $c_1,c_2,\\ldots,c_l$ is said to be a minimum\nparameter colouring if C consists of a minimum number of colours with smallest\nsubscripts. In this paper, we study colouring version of curling number of\ncertain graphs, with respect to their minimum parameter colourings."}, {"title": "Francis Guthrie's approach to The Four Color Problem", "abstract": "The odd wheel is the only type of 4-critical graph in which one vertex always\ngets a unique color. This supports Frederic Guthrie's approach to the Four\nColor Problem."}, {"title": "An investigation of the non-trivial zeros of the Riemann zeta function", "abstract": "While many zeros of the Riemann zeta function are located on the critical\nline $\\Re(s)=1/2$, the non-existence of zeros in the remaining part of the\ncritical strip $\\Re(s) \\in \\, ]0, 1[$ is the main scope to be proven for the\nRiemann hypothesis. The Riemann zeta functional leads to a relation between the\nzeros on either sides of the critical line. Given $s$ a complex number and\n$\\bar{s}$ its complex conjugate, if $s$ is a zero of the Riemann zeta function\nin the critical strip $\\Re(s) \\in \\, ]0, 1[$, then $\\zeta(s) =\n\\zeta(1-\\bar{s})$, as a key proposition to prove the Riemann hypothesis."}, {"title": "Lowen type multi-fuzzy topological spaces", "abstract": "In this paper Lowen type multi-fuzzy topological space has been introduced\nand characterization of topology by its nbd system is studied. Also the product\nmulti-fuzzy topological space has been introduced and it has been investigated\nthat 2nd countability and compactness are finitely productive in multi-fuzzy\ntopological spaces."}, {"title": "Hyperreal delta functions as a new general tool for modeling physical states with infinitely high densities", "abstract": "This paper introduces the expanded real numbers as an ordered subring of the\nhyperreal number field that does not contain any infinitesimals, and defines\nthe set of all integrable functions from the real numbers to the expanded real\nnumbers. This allows to identify the Dirac delta with a special\nhyperreal-valued function of a real variable: the Dirac delta function thus\ndefined is a general tool, applicable for the mathematical modeling of physical\nsystems in which infinitely high densities occur."}, {"title": "A hybrid natural transform homotopy perturbation method for solving fractional partial differential equations", "abstract": "A hybrid analytical method for solving linear and nonlinear fractional\npartial differential equations is presented. The proposed analytical method is\nan elegant combination of the Natural Transform Method (NTM) and a well-known\nmethod, Homotopy Perturbation Method (HPM). In this analytical method, the\nfractional derivative is computed in Caputo sense and the nonlinear terms are\ncalculated using He's polynomials. The proposed analytical method reduces the\ncomputational size, avoids round-off errors. Exact solutions of linear and\nnonlinear fractional partial differential equations is successfully obtained\nusing the analytical method."}, {"title": "General Methods For Solving Ordinary Differential Equations 1", "abstract": "The method of this paper is my original creation. A new method for solving\nlinear differential equations is proposed in this paper. The important\nconclusion of this paper is that arbitrary order linear ordinary differential\nequations with variable coefficients can be solved by the method of recursion\nand reduction of order under some conditions which easily be satisfied in\npractical applications."}, {"title": "A study of divergence from randomness in the distribution of prime numbers within the arithmetic progressions 1+6n and 5+6n", "abstract": "In this article I present results from a statistical study of prime numbers\nthat shows a behaviour that is not compatible with the thesis that they are\ndistributed randomly. The analysis is based on studying two arithmetical\nprogressions defined by the following polynomials: ($1+6n$, $5+6n$, $n\\in{N}$)\nwhose respective numerical sequences have the characteristic of containing all\nthe prime numbers except $3$ and $2$. If prime numbers were distributed\nrandomly, we would expect the two polynomials to generate the same number of\nprimes. Instead, as the reported findings show, we note that the polynomial\n$5+6n$ tends to generate many more primes, and that this divergence grows\nprogressively as more prime numbers are considered. A possible explanation for\nthis phenomenon can be found by calculating the number of products that\ngenerate composite numbers which are expressible by the two polynomials. This\nanalysis reveals that the number of products that generate composite numbers\nexpressible by the polynomial $1+6n$ is $(n+1)^{ 2}$, while the number of\nproducts that generate composites expressible by the polynomial $5+6n$ is\n$(n+1)n$, con $n\\in{N}$. As a composite number is a non-prime number, this\ndifference incited me to analyse the distribution of prime numbers generated by\nthe two polynomials. The results, based on studying the first (approx.) 500\nmillion prime numbers, confirm the fact that the number of primes that can be\nwritten using the polynomial $1+6n$ is lower than the number of primes that can\nwritten using the polynomial $5+6n$, and that this divergence grows\nprogressively with the number of primes considered."}, {"title": "Quelques remarques sur les vari{é}t{é}s, fonctions de Green et formule de Stokes", "abstract": "We give some remarks on some manifolds K3 surfaces, Complex projective\nspaces, real projective space and Torus and the classification of two\ndimensional Riemannian surfaces, Green functions and the Stokes formula. We\nalso, talk about traces of Sobolev spaces, the distance function, the notion of\ndegree and a duality theorem, the variational formulation and conformal map in\ndimension 2, the metric on the boundary of a Lipschitz domain and polar\ngeodesic coordinates and the Gauss-Bonnet formula and the positive mass theorem\nin dimension 3 and in the locally conformally flat case. And the Ricci flow.\nAnd fields and their relation to the equations.And obstructions in astronomy.\nAnd on strings, superstrings and D-branes. And topological solutions in the\nnegative case, critical, supercritical and superstrings."}, {"title": "On homeomorphisms and $C^{1}$ maps", "abstract": "Our purpose in this article is first, following [8], to prove that if $\\alpha\n$, $\\beta $ are any points of the open unit disc $D(0;1)$ in the complex plane\n${\\bf C}$ and $r$, $s$ are any positive real numbers such that ${\\overline{D}}(\n\\alpha ;r) \\subseteq D(0;1)$ and ${\\overline{D}}( \\beta ;s) \\subseteq D(0;1)$,\nthen there exist $t \\in (0,1)$ and a homeomorphism $h : {\\overline{D}}(0;1)\n\\rightarrow {\\overline{D}}(0;1)$ such that ${\\overline{D}}( \\alpha ;r)\n\\subseteq D(0;t)$, ${\\overline{D}}( \\beta ;s) \\subseteq D(0;t)$, $h \\left[\n{\\overline{D}}( \\alpha ;r) \\right] = {\\overline{D}}( \\beta ;s)$ and $h = id$ on\n${\\overline{D}}(0;1) \\setminus D(0;t)$, and second, following [9], to prove\nthat if $q \\in {\\bf N} \\setminus \\{ 0, 1 \\} $ and ${\\bf B}({\\bf 0};1)$ is the\nopen unit ball in ${\\bf R}^{q}$, while for any $t>0$, we set $f^{(t)}( {\\bf x}\n) = \\frac{ t {\\bf x} }{ 1 + (t-1) \\Vert {\\bf x} \\Vert }$, whenever ${\\bf x} \\in\n{\\overline{\\bf B}}({\\bf 0};1)$, then $f^{(t)} \\rightarrow id$ in $C^{1} \\left(\n{\\overline{\\bf B}}({\\bf 0};1) , {\\bf R}^{q} \\right) $ as $t \\rightarrow 1^{+}$."}, {"title": "Triangle Inscribed-Triangle Picking", "abstract": "Given a triangle ABC, we derive the probability distribution function and the\nmoments of the area of an inscribed triangle RST whose vertices are uniformly\ndistributed on AB, BC, and CA. The theoretical results are confirmed by a Monte\nCarlo simulation."}, {"title": "Hyperreal Numbers for Infinite Divergent Series", "abstract": "Treating divergent series properly has been an ongoing issue in mathematics.\nHowever, many of the problems in divergent series stem from the fact that\ndivergent series were discovered prior to having a number system which could\nhandle them. The infinities that resulted from divergent series led to\ncontradictions within the real number system, but these contradictions are\nlargely alleviated with the hyperreal number system. Hyperreal numbers provide\na framework for dealing with divergent series in a more comprehensive and\ntractable way."}, {"title": "Development of the matrix of primes and proof of an infinite number of primes-twins", "abstract": "This paper is devoted to the theory of prime numbers. In this paper we first\nintroduce the notion of a matrix of prime numbers. Then, in order to\ninvestigate the density of prime numbers in separate rows of the matrix under\nconsideration, we propose a number of lemmas and theorems that, together with\nthe Dirichlet and Euler theorems, make it possible to prove the infinity of\nprime twins."}, {"title": "Derivatives of flat functions", "abstract": "We remark that there is no smooth function $f(x)$ on $[0, 1]$ which is flat\nat $0$ such that the derivative $f^{(n)}$ of any order $n\\geq 0$ is positive on\n$(0,1]$. Moreover, the number of zeros of the $n$-th derivative $f^{(n)}$ grows\nto the infinity and the zeros accumulate to $0$ when $n \\to \\infty$."}, {"title": "Some New Results on Proper Colouring of Edge-set Graphs", "abstract": "In this paper, we present a foundation study for proper colouring of edge-set\ngraphs. The authors consider that a detailed study of the colouring of edge-set\ngraphs corresponding to the family of paths is best suitable for such\nfoundation study. The main result is deriving the chromatic number of the\nedge-set graph of a path, $P_{n+1}$, $n \\geq 1$. It is also shown that edge-set\ngraphs for paths are perfect graphs."}, {"title": "A Proof of the Riemann Hypothesis Through the Nicolas Inequality", "abstract": "A work by Nicolas has shown that if it can be proven that a certain\ninequality holds for all $n$, the Riemann hypothesis is true. This inequality\nis associated with the Mertens theorem, and hence the Euler totient at\n$\\prod_{k=1}^n p_k$, where $n$ is any integer and $p_n$ is the $n$-th prime. We\nshall show that indeed the Nicolas inequality holds for all $n$."}, {"title": "Topology and Higher Concurrencies", "abstract": "We formulate a general approach to higher concurrencies in general and neural\ncodes in particular, and suggest how the higher order aspects may be dealt with\nin using topology."}, {"title": "Identities for second order recurrence sequences", "abstract": "We derive several identities for arbitrary homogeneous second order\nrecurrence sequences with constant coefficients. The results are then applied\nto present a unified study of six well known integer sequences, namely the\nFibonacci sequence, the sequence of Lucas numbers, the Jacobsthal sequence, the\nJacobsthal-Lucas sequence, the Pell sequence and the Pell-Lucas sequence."}, {"title": "A few results on the infimum of regular polygons equal-size split line", "abstract": "If an n-side unit regular polygon is divided into m equal sized parts, then\nwhat is the minimum length of the split line ${l_{m,n}}$? This problem has its\npractical application in real world. This paper proved that ${l_{2,3}} = \\sqrt\n{\\frac{{\\sqrt 3 \\pi }}{{12}}} $, ${l_{3,3}} = \\frac{{\\sqrt 3 }}{2}$, and\n$\\frac{1}{2}\\sqrt {n\\pi {\\rm{ctan}}\\frac{\\pi }{n}} \\le \\mathop {\\lim\n}\\limits_{m \\to \\infty } \\frac{{{l_{m,n}}}}{{\\sqrt m }} \\le \\sqrt {\\frac{{\\sqrt\n3 }}{2}n{\\rm{ctan}}\\frac{\\pi }{{\\rm{n}}}} $"}, {"title": "Nonlinear Invariants of Planar Point Clouds Transformed by Matrices", "abstract": "The goal of this paper is to present invariants of planar point clouds, that\nis functions which take the same value before and after a linear transformation\nof a planar point cloud via a $2 \\times 2$ invertible matrix. In the approach\nwe adopt here, these invariants are functions of two variables derived from the\nleast squares straight line of the planar point cloud under consideration. A\nlinear transformation of a point cloud induces a nonlinear transformation of\nthese variables. The said invariants are solutions to certain Partial\nDifferential Equations, which are obtained by employing Lie theory. We find\ncloud invariants in the general case of a four$-$parameter transformation\nmatrix, as well as, cloud invariants of various one$-$parameter sets of\ntransformations which can be practically implemented. Case studies and\nsimulations which verify our findings are also provided."}, {"title": "100% of the zeros of the Riemann zeta-function are on the critical line", "abstract": "We consider a specific family of analytic functions $g_{\\alpha,T}(s)$,\nsatisfying certain functional equations and approximating to linear\ncombinations of the Riemann zeta-function and its derivatives of the form\n  $c_0\\zeta(s)+c_1\\frac{\\zeta'(s)}{\\log T}+c_2\\frac{\\zeta''(s)}{(\\log\nT)^2}+\\dots+c_{K}\\frac{\\zeta^{(K)}(s)}{(\\log T)^{K}}$.\n  We also consider specific mollifiers of the form $M(s)D(s)$ for these linear\ncombinations, where $M(s)$ is the classical mollifier, that is, a short\nDirichlet polynomial for $1/\\zeta(s)$, and the Dirichlet polynomial $D(s)$ is\nalso short but with large and irregular Dirichlet coefficients, and arises from\nsubstitution for $w$, in Runge's complex approximation polynomial for\n$f(w)=\\frac1{c_0+w}$, of the Selberg approximation for\n  $\\frac{c_1}{\\log T}\\frac{\\zeta'}{\\zeta}(s)+\\frac{c_2}{(\\log\nT)^2}\\frac{\\zeta''}{\\zeta}(s)+\\dots+\\frac{c_{K}}{(\\log\nT)^{K}}\\frac{\\zeta^{(K)}}{\\zeta}(s)$\n  (analogous to Selberg's classical approximation for\n$\\frac{\\zeta'}{\\zeta}(s)$).\n  Exploiting the functional equations previously mentioned (concerning\ntranslation of the variable $s$), together with the mean-square asymptotics of\nthe Levinson-Conrey method and the Selberg approximation theory (with some\nadditional results) we show that almost all of the zeros of the Riemann\nzeta-function are on the critical line."}, {"title": "On the Frankl's union-closed conjecture", "abstract": "A celebrated unresolved conjecture of Peter Frankl states that every finite\ncollection of sets, with finite universe, admits an abundant element. In this\npaper, we prove Frankl's union-closed conjecture(FC). We provide an induction\nproof based on a key result that every candidate collection, $\\Omega$, admits\nan irreducible form. The concept of irreducible collections is one of the key\ncontributions of this paper.\n  We show that the conjecture is true if it holds for a class of irreducible\nfinite collections. Then we show that the conjecture holds for all irreducible\nfinite collections, with non-empty universe."}, {"title": "A Fast Algorithm to Calculate Power Sum of Natural Numbers", "abstract": "Permutations can be represented as linear combinations of natural numbers\nwith different powers. In this paper, its coefficient matrix and inverse matrix\nis derived, and the results show the coefficient matrix is a lower triangular\nmatrix while the inverse matrix is upper triangular. Permutations of n-th order\nare used to generate the inverse matrix. The generation function of natural\nnumbers' power sum is derived to calculate the power sum."}, {"title": "On the Philosophy of Higher Structures", "abstract": "The purpose of this paper is to describe and elaborate the philosophical\nideas behind hyperstructures and structure formation in general and emphasize\nthe key ideas of the Hyperstructure Program."}, {"title": "On the Mathematics of Higher Structures", "abstract": "In this paper we will relate hyperstructures and the general\n$\\mathscr{H}$-principle to known mathematical structures, and also discuss how\nthey may give rise to new mathematical structures. The main purpose is to point\nout new ideas and directions of investigation."}, {"title": "Roots of polynomials and the Sendov's conjecture", "abstract": "In this paper we first prove that a simple root of a polynomial satisfies the\nSendov's conjecture. As the multiple roots trivially satisfy the Sendov's\nconjecture we conclude that the Sendov's conjecture holds true."}, {"title": "On Studying the Phase Behavior of the Riemann Zeta Function Along the Critical Line", "abstract": "The critical line of the Riemann zeta function is studied from a new\nviewpoint. It is found that the ratio between the zeta function at any zero and\nthe corresponding one at a conjugate point has a certain phase and its absolute\nvalue is unity. This fact is valid along the whole critical line and only\nthere. The common functional equation is used with the aid of the function\nratio between any zero and its negative side pair, a complex conjugate. As a\nresult, an equation is obtained for solving the phase along the critical line."}, {"title": "Primitive Roots In Short Intervals", "abstract": "Let $p\\geq 2$ be a large prime, and let $N\\gg ( \\log p)^{1+\\varepsilon}$.\nThis note proves the existence of primitive roots in the short interval\n$[M,M+N]$, where $M \\geq 2$ is a fixed number, and $ \\varepsilon>0$ is a small\nnumber. In particular, the least primitive root $g(p)= O\\left ((\\log\np)^{1+\\varepsilon} \\right)$, and the least prime primitive root $g^*(p)= O\\left\n((\\log p)^{1+\\varepsilon} \\right)$ unconditionally."}, {"title": "Encoding discrete quantum algebras in a hierarchy of binary words", "abstract": "It is shown how to endow a hierarchy of sets of binary patterns with the\nstructure of an abstract,normed C*-algebra. In the course we also recover an\nintermediate connection with the words of a Dyck language and Tempereley-Lieb\nalgebras for which we also find that an effective arithmetic code is possible\nalbeit of greater complexity. We also discuss possible applications associated\nwith signal theory and waveform engineering on possible ways to embed discrete\ncomputational structures in an analog continuum substrate."}, {"title": "Several Conclusions on another site setting problem", "abstract": "Let $S = \\{ {A_1},{A_2}, \\cdots ,{A_n}\\} $ be a finite point set in\nm-dimensional Euclidean space ${E^m}$, and$\\left\\| {{A_i}{A_j}} \\right\\|$ be\nthe distance between $A_i$ and $A_j$. Define $\\sigma (S) = \\sum\\limits_{1 \\le i\n< j \\le n} {\\left\\| {{A_i}{A_j}} \\right\\|} $, $D(S) = \\mathop {\\max }\\limits_{1\n\\le i < j \\le n} \\left\\{ {\\left\\| {{A_i}{A_j}} \\right\\|} \\right\\}$, $\\omega\n(m,n) = \\frac{{\\sigma (S)}}{{D(S)}}$, $\\sup \\omega (m,n) = \\max \\left\\{ {\\left.\n{\\frac{{\\sigma (S)}}{{D(S)}}} \\right|S \\subset {E^m},\\left| S \\right| = n}\n\\right\\}$. This paper proves that, for any point P in an n-dimensional simplex\n${A_1}{A_2} \\cdots {A_{n + 1}}$ in Euclidean space, $\\sum\\limits_{i = 1}^{n +\n1} {\\left\\| {P{A_i}} \\right\\|} $ <= $\\mathop {\\sup }\\limits_{{i_t},{j_t} \\in \\{\n1,2, \\cdots ,n + 1\\} } \\left\\{ {\\sum\\limits_{t = 1}^n {\\left\\|\n{{A_{{i_t}}}{A_{{j_t}}}} \\right\\|} } \\right\\}$ By using this inequality and\nseveral results in differential geometry this paper also proves that $\\sup\n\\omega (2,4) = 4 + 2\\sqrt {2 - \\sqrt 3 } $, $\\sup \\omega (n,n + 2)$ >= $C_{n +\n1}^2 + 1 + n\\sqrt {2\\left( {1 - \\sqrt {{\\textstyle{{n + 1} \\over {2n}}}} }\n\\right)} $."}, {"title": "Fuzzy $α$-cut and related structures", "abstract": "This paper deals with a new notion called fuzzy $\\alpha$-cut and its\nproperties. A notion called localic frame is also introduced. Algebraic\nstructures arising out of the family of fuzzy $\\alpha$-cuts have been\ninvestigated. It will be seen that this family forms a localic frame. Some\nsignificance and usefulness of fuzzy $\\alpha$-cuts are discussed."}, {"title": "Extending the Calculus of Moving Surfaces to Higher Orders", "abstract": "In 2010, a book published on the work of Jaques Hadamard, entitled\n\"Introduction to Tensor Analysis and the Calculus of Moving Surfaces\" by Dr.\nPavel Grinfeld, proposed an extension of Hadamard's work to ultimately allow\nprinciples of tensorial invariance on surfaces to be extended to notions of\ntime-dependent and moving surfaces. Coined \"The Calculus of Moving Surfaces\"\n(CMS), notions of Invariant Time Derivatives on Arbitrary Surface/Ambient\nTensors, Surface Velocities, and time derivatives of time-dependent Volume &\nSurface Integrals were introduced. This paper focuses on extending concepts\nfound within CMS to other Surface Objects, to Higher Orders, and to further\nuncover Tensors which are powerful at representing Commutations and Curvature\npresent on Surfaces which are moving in Time."}, {"title": "On the Factorization of Two Adjacent Numbers in Multiplicatively Closed Sets Generated by Two Elements", "abstract": "For two natural numbers $1<p_1<p_2$, with $\\alpha =\n\\frac{\\log(p_1)}{\\log(p_2)}$ irrational, we describe, in main Theorem $\\Omega$\nand in Note $1.5$, the factorization of two adjacent numbers in the\nmultiplicatively closed subset $S = \\{p_1^ip_2^j\\mid i,j\\in\n\\mathbb{N}\\cup\\{0\\}\\}$ using primary and secondary convergents of $\\alpha$.\nThis suggests general Question $1.2$ for more than two generators which is\nstill open."}, {"title": "A marriage of category theory and set theory: a finitely axiomatized nonstandard first-order theory implying ZF", "abstract": "It is well known that ZFC, despite its usefulness as a foundational theory\nfor mathematics, has two unwanted features: it cannot be written down\nexplicitly due to its infinitely many axioms, and it has a countable model due\nto the L\\\"{o}wenheim-Skolem theorem. This paper presents the axioms one has to\naccept to get rid of these two features. For that matter, some twenty axioms\nare formulated in a nonstandard first-order language with countable many\nconstants: to this collection of axioms is associated a universe of discourse\nconsisting of a class of objects, each of which is a set, and a class of\narrows, each of which is a function. The axioms of ZF are derived from this\nfinite axiom scheme, and it is shown that it does not have a countable\nmodel--if it has a model at all, that is. Furthermore, the axioms of category\ntheory are proven to hold: the present universe may therefore serve as an\nontological basis for category theory. However, it has not been investigated\nwhether any of the soundness and completeness properties hold for the present\ntheory: the inevitable conclusion is therefore that only further research can\nestablish whether the present results indeed constitute an advancement in the\nfoundations of mathematics."}, {"title": "A Mathematical Approach to the Hierarchical Structure of Languages", "abstract": "In this paper we suggest how the mathematical concept of hyperstructures may\nbe a useful tool in the study of the higher, hierachical structure of\nlanguages."}, {"title": "Golden ratios, Lucas Sequences and the Quadratic Family", "abstract": "It is conjectured that there is a converging sequence of some generalized\nFibonacci ratios, given the difference between consecutive ratios, such as the\nGolden Ratio, $\\varphi^1$, and the next golden ratio $\\varphi^2$. Moreover, the\ngraphic depiction of those ratios show some overlap with the quadratic family,\nand some numerical evidence suggest that everyone of those ratios in the finite\nset obtained, belong to at least one quadratic family, and finally a proof is\npresented that the converging sequence of some generalized Fibonacci ratios\nbelong to at least one quadratic family."}, {"title": "Pythagorean fuzzy graphs: Some results", "abstract": "Graph theory has successfully used to solve a wide range of problems\nencountered in diverse fields such as medical sciences, neural networks,\ncontrol theory, transportation, clustering analysis, expert systems, image\ncapturing, and network security. In past few years, a number of generalizations\nof graph theoretical concepts have developed to model the impreciseness and\nuncertainties in graphical network problems. A Pythagorean fuzzy set is a\npowerful tool for describing the vague concepts more precisely. The Pythagorean\nfuzzy set-based models provide more flexibility in handling the human judgment\ninformation as compared to other fuzzy models. The objective of this paper is\nto apply the concept of Pythagorean fuzzy sets to graph theory. This work\nintroduces the notion of Pythagorean fuzzy graphs (PFGs) and describes a number\nof methods for their construction. We then define some basic operations on PFGs\nand prove some of their important properties. The work also discusses the\nnotion of isomorphism between Pythagorean fuzzy graphs with a numerical\nexample. Further, we introduce the concept of the strong Pythagorean fuzzy\ngraph and the complete Pythagorean fuzzy graph. In addition, the paper also\nproves some results on self-complementary, self-weak complementary with\nPythagorean fuzzy strong graphs and Pythagorean fuzzy complete graphs."}, {"title": "Riemann Hypothesis: a GGC factorisation", "abstract": "A GGC (Generalized Gamma Convolution) representation of Riemann's Xi-function\nis constructed."}, {"title": "Existence and Smoothness of Navier-Stokes Equations", "abstract": "In this paper we propose new method for proving of global solutions for 3D\nNavier-Stokes equations. This complies an application to the Clay Institute\nMillennium Prize Navier Stokes Problem. The proposed method can be applied for\ninvestigation of global solutions for other classes of PDEs."}, {"title": "A Note on $J$-Colouring of Jahangir Graphs", "abstract": "In this paper, we discuss $J$-colouring of the family of Jahangir graphs.\nNote that the family of Jahangir graphs is a wide-ranging family of graphs\nwhich by a generalised definition includes wheel graphs. We characterise the\nsubset of Jahangir graphs which admit a $J$-colouring."}, {"title": "Goldbach and Twin Prime Pairs: A Sieve Method to Connect the Two", "abstract": "This paper proposes, and demonstrates the efficacy of, an elementary method\nfor establishing a lower bound for cardinalities of selected sets of twin\nprimes, and shows that the proof employed may be modified for selected sets of\nGoldbach pairs. Our sieve method is centred on the restrictive properties of\nintervals, specifically regarding divisibility distributions. We implicitly use\nthe Chinese Remainder Theorem by way of the use of the midpoint in our\nintervals, and consider the sieve of Eratosthenes in such a way as to find a\nset of primes whose distribution is mirror-symmetrical about that midpoint.\nBounds are established through the use of the formulae closely associated with\nthe Prime Number theorem and the Mertens theorem. We show that the Goldbach\nconjecture is true if the Riemann hypothesis is true."}, {"title": "Synopsis of Duality", "abstract": "Properties and examples of the dual transformation between two planes, which\nis such that the coordinates of a point in the original plane give the\ncoefficients of a line in the dual plane and the coefficients of a line in the\noriginal plane give the coordinates of a point in the dual plane, are examined."}, {"title": "Geometric Matrices and the Symmetric Group", "abstract": "We construct real and complex matrices in terms of Kronecker products of a\nWitt basis of 2n null vectors in the geometric algebra over the real and\ncomplex numbers. In this basis, every matrix is represented by a unique sum of\nproducts of null vectors. The complex matrices provide a direct matrix\nrepresentation for geometric algebras with signatures p+q <= 2n+1. Properties\nof irreducible representations of the symmetric group are presented in this\ngeometric setting."}, {"title": "Application of Renormalization Techniques to the Classical Arbelos Problem", "abstract": "This application of renormalization techniques offers a modern take on the\nclassical Arbelos geometry problem. Keeping within the context of the original\nproblem, two semicircles, meeting at chord T, are together circumscribed by a\nthird semicircle. Separate from the original Arbelos result, both circumscribed\nsemicircle areas are found in terms of chord T and the third circumscribing\nsemicircle radius R. This approach eliminates the additional variables of the\ncircumscribed semicircle radii."}, {"title": "Inequalities For The Primes Counting Function", "abstract": "The prime counting function inequality $\\pi(x+y) < \\pi(x)+\\pi(y)$, which is\nknown as Hardy-Littlewood conjecture, has been established for a variety of\ncases such as $ \\delta x \\leq y \\leq x$, where $0< \\delta \\leq 1$, and $x \\leq\ny\\leq x \\log x \\log \\log x$ as $ x \\to \\infty$. The goal in note is to extend\nthe inequality to the new larger ranges $\\geq x \\log^{-c}x\\leq y \\leq x$, where\n$c\\geq 0$ is a constant, unconditionally; and for $\\geq x^{1/2} \\log^3x\\leq y\n\\leq x$, conditional on a standard conjecture."}, {"title": "Rectifying curves on a smooth surface immersed in the euclidean space", "abstract": "The main objective of the present paper is to investigate a sufficient\ncondition for which a rectifying curve on a smooth surface remains invariant\nunder isometry of surfaces, and also it is shown that under such an isometry\nthe component of the position vector of a rectifying curve on a smooth surface\nalong the normal to the surface is invariant."}, {"title": "Introducing fully UP-semigroups", "abstract": "In this paper, we introduce some new classes of algebras related to\nUP-algebras and semigroups, called a left UP-semigroup, a right UP-semigroup, a\nfully UP-semigroup, a left-left UP-semigroup, a right-left UP-semigroup, a\nleft-right UP-semigroup, a right-right UP-semigroup, a fully-left UP-semigroup,\na fully-right UP-semigroup, a left-fully UP-semigroup, a right-fully\nUP-semigroup, a fully-fully UP-semigroup, and find their examples."}, {"title": "The UP-Isomorphism Theorems for UP-algebras", "abstract": "In this paper, we construct the fundamental theorem of UP-homomorphisms in\nUP-algebras. We also give an application of the theorem to the first, second,\nthird and fourth UP-isomorphism theorems in UP-algebras."}, {"title": "On the resummation of series of fuzzy numbers via generalized Dirichlet and generalized factorial series", "abstract": "We introduce semicontinuous summation methods for series of fuzzy numbers and\ngive Tauberian conditions under which summation of a series of fuzzy numbers\nvia generalized Dirichlet series and via generalized factorial series implies\nits convergence. Besides, we define the concept of level Fourier series of\nfuzzy valued functions and obtain results concerning the summation of level\nFourier series."}, {"title": "Properties of Lerch Sums and Ramanujan's Mock Theta Functions", "abstract": "In this article we study properties of Ramanujan's mock theta functions that\ncan be expressed in Lerch sums. We mainly show that each Lerch sum is actually\nthe integral of a Jacobian theta function (here we show that for\n$\\vartheta_3(t,q)$ and $\\vartheta_4(t,q)$) and the $\\sec-$function. We also\nprove some modular relations and evaluate the Fourier coefficients of a class\nof Lerch sums."}, {"title": "On $J$-Colouring of Chithra Graphs", "abstract": "The family of Chithra graphs is a wide ranging family of graphs which\nincludes any graph of size at least one. Chithra graphs serve as a graph\ntheoretical model for genetic engineering techniques or for modelling natural\nmutation within various biological networks found in living systems. In this\npaper, we discuss recently introduced $J$-colouring of the family of Chithra\ngraphs."}, {"title": "Some properties of substochastic matrices", "abstract": "We establish some properties of substochastic matrices that we haven't been\nable to find in the literature"}, {"title": "Defining the prime numbers prior to the integers: A first-principles approach to the distribution of primes", "abstract": "While the prime numbers have been subject to mathematical inquiry since the\nancient Greeks, the accumulated effort of understanding these numbers has - as\nMarcus du Sautoy recently phrased it - 'not revealed the origins of what makes\nthe primes tick.' Here, we suggest that a resolution to this long-standing\nconundrum is attainable by defining the primes prior to the natural numbers -\nas opposed to the standard number theoretical definition of primes where these\nnumbers derive from the natural numbers. The result is a first-principles\nperspective on the primes that exposes and explains the 'origins' of their\ndistribution and their mathematical properties and provides an intuitive as\nwell as pedagogical approach to the primes with the potential to impact our\nthinking about these age-old mathematical objects. A few immediate outcomes of\nthis perspective are another proof of the fundamental theorem of arithmetic, a\nprobabilistic model of primes sharing as well as explaining their subrandom\ncorrelation structure, and an equivalent formulation of the Riemann hypothesis."}, {"title": "The Exterior Derivative - A direct approach", "abstract": "In this note we provide a direct approach to the most basic operator in this\ntheory namely the exterior derivative. The crucial ingredient is a calculus\nlemma based on determinants. We maintain the view that in a first course at\nleast this direct approach is preferable to the more abstract one based on\ncharacterization of the exterior derivative in terms of its properties."}, {"title": "A disproof of the Riemann hypothesis on zeros of $ζ-$function", "abstract": "Applying the known Nyman--Beurling criterion, it is disproved the Riemann\nhypothesis on zeros of $\\zeta -$function."}, {"title": "On generalized Berwald surfaces with locally symmetric fourth root metrics", "abstract": "Let $m=2l$ be a positive natural number, $l=1, 2, \\ldots. $ A Finslerian\nmetric $F$ is called an $m$-th root metric if its $m$-th power $F^m$ is of\nclass $C^{m}$ on the tangent manifold $TM$. Using some homogenity properties,\nthe local expression of an $m$-th root metric is a polynomial of degree $m$ in\nthe variables $y^1$, $\\ldots$, $y^n$, where $\\dim M=n$. $F$ is locally\nsymmetric if each point has a coordinate neighbourhood such that $F^m$ is a\nsymmetric polynomial of degree $m$ in the variables $y^1$, $\\ldots$, $y^n$ of\nthe induced coordinate system on the tangent manifold. Using the fundamental\ntheorem of symmetric polynomials, the reduction of the number of the\ncoefficients depending on the position makes the computational processes more\neffective and simple. In the paper we present some general observations about\nlocally symmetric $m$-th root metrics. Especially, we are interested in\ngeneralized Berwald surfaces with locally symmetric fourth root metrics. The\nmain result (Theorem 1) is their intrinsic characterization in terms of the\nbasic notions of linear algebra. We present a one-parameter family of examples\nas well. The last section contains some computations in 3D. They are supported\nby the MAPLE mathematics softwer (LinearAlgebra)."}, {"title": "On the distribution of composite odd numbers", "abstract": "We study odd numbers through a straightforward indexing. We focus in\nparticular on odd prime and composite numbers and their distribution. With a\ncounting argument, we calculate the limit of two sums and compare their\nconvergence rate."}, {"title": "Quantum Vacuum and the Structure of \"Empty\" Space-Time", "abstract": "We have considered the possibility of formation a massless particles with\nspin 1 in the region of negative energies, within the framework of the Weyl\ntype equation for neutrinos. It is proved that, unlike quantum electrodynamics,\nthe developed approach allows in the \"ground state\" the formation of such\nstable particles. The structure and properties of this vector-boson are studied\nin detail. The problem of entangling two vector bosons with projections of\nspins +1 and -1 and, accordingly, the formation of a zero-spin boson is studied\nwithin the framework of a complex stochastic equation of the Langevin type. The\npaper discusses the structure of the Bose particle of a scalar field and the\nspace-time's properties of an empty space quantum vacuum."}, {"title": "Do we need to modify Maxwell's equations?", "abstract": "Maxwell's equations are modified to incorporate a scalar field to account for\nthe London's superconductivity. Assuming the electromagnetic field is described\nby the Klein-Gordon equation, London's equations of superconductivity are then\nderived, which are invariant under a new set of transformations. The invariance\nof the modified Maxwell's equations under these transformations requires the\nelectromagnetic field and the scalar field to be scale-invariant. Relying on\nthese transformations, a quantized Josephson-like current is derived. This\ncurrent gives rise to a residual magnetic field. The spatial and temporal\nvariations of the scalar field are linked to the electric polarization such\nthat the polarization vector is curl-less."}, {"title": "Moving unstable particles and special relativity", "abstract": "In Poincare-Wigner-Dirac theory of relativistic interactions, boosts are\ndynamical. This means that - just like time translations - boost\ntransformations have non-trivial effect on internal variables of interacting\nsystems. This is different from space translations and rotations, whose actions\nare always universal, trivial and interaction-independent. Applying this theory\nto unstable particles viewed from a moving reference frame, we prove that the\ndecay probability cannot be invariant with respect to boosts. Different moving\nobservers may see different internal compositions of the same unstable\nparticle. Unfortunately, this effect is too small to be noticeable in modern\nexperiments."}, {"title": "Constructive Foundation of Quantum Mechanics", "abstract": "I describe a constructive foundation for Quantum Mechanics, based on the\ndiscreteness of the degrees of freedom of quantum objects and on the Principle\nof Relativity. Taking Einstein's historical construction of Special Relativity\nas a model, the construction is carried out in close contact with a simple\nquantum mechanical Gedanken experiment. This leads to the standard axioms of\nQuantum Mechanics. The quantum mechanical description is identified as a tool\nthat allows describing objects with discrete degrees of freedom in space-time\ncovariant with respect to coordinate transformations. An inherent property of\nthis description is a quantum mechanical interaction mechanism. The\nconstruction gives detailed answers to controversial questions, such as the\nmeasurement problem, the informational content of the wave function, and the\ncompleteness of Quantum Mechanics."}, {"title": "Anisotropic generalization of well-known solutions describing relativistic self-gravitating fluid systems: An algorithm", "abstract": "We present an algorithm to generalize a plethora of well-known solutions to\nEinstein field equations describing spherically symmetric relativistic fluid\nspheres by relaxing the pressure isotropy condition on the system. By suitably\nfixing the model parameters in our formulation, we generate closed-form\nsolutions which may be treated as anisotropic generalization of a large class\nof solutions describing isotropic fluid spheres. From the resultant solutions,\na particular solution is taken up to show its physical acceptability. Making\nuse of the current estimate of mass and radius of a known pulsar, the effects\nof anisotropic stress on the gross physical behaviour of a relativistic compact\nstar is also highlighted."}, {"title": "The Real Quaternion Relativity", "abstract": "In this work, we use real quaternions and the basic concept of the final\nspeed of light in an attempt to enhance the standard description of special\nrelativity. First, we demonstrate that it is possible to introduce a quaternion\ntime domain where a coordinate point is described by a quaternion time. We show\nthat the time measurement is a function of the observer location, even for\nstationary frames of reference. We introduce a moving observer, which leads to\nthe traditional Lorentz relation for the time interval. We show that the\npresent approach can be used in stationary, moving, or rotating frames of\nreference, unlike the traditional special relativity, which applies only to the\ninertial moving frames. Then, we use the quaternion formulation of space-time\nand mass-energy equivalence to extend the quaternion relativity to space, mass,\nand energy. We demonstrate that the transition between the particle and\nobserver reference frames is equivalent to space inversion and can be described\nmathematically by quaternion conjugation. On the other hand, physical\nmeasurements are described by the quaternion norm and consequently are\nindependent of the conjugation, which seems to be the quaternion formulation of\nthe relativity principle."}, {"title": "Cosmic Microwave Background and the issue of a fundamental preferred frame", "abstract": "Correlating ether-drift measurements in laboratory and CMB observations in\nspace would confirm the existence of a preferred reference frame. To this end,\nhowever, the velocity of light in the interferometers cannot be the same\nparameter 'c' of Lorentz transformations. Thus, for the earth velocity of 370\nkm/s, a fundamental 10^(-15) light anisotropy, as presently observed in vacuum\nand in solid dielectrics, could reveal a 10^(-9) difference in the vacuum\nrefractivity between an ideal freely-falling frame and an apparatus on the\nearth surface. In this perspective, the stochastic nature of the vacuum could\nexplain the irregular character of the signal and the substantial reduction of\nits statistical average (10^(-18) or smaller). For the same v=370 km/s the\ndifferent refractivity, about 10^(-4) and 10^(-5) for air or helium at\natmospheric pressure, could also explain the observed anisotropy, respectively\nabout 10^(-10) and 10^(-11). The mechanism enhancing the signal in gaseous\nmatter, but ineffective in solid dielectrics, is naturally identified in a\nnon-local, temperature gradient of a fraction of millikelvin. This is found in\nall classical experiments and might ultimately reflect the CMB temperature\ndipole or the fundamental energy flow in a Lorentz-non-invariant vacuum.\nClarification requires dedicated experiments and improvements in the data\nanalysis."}, {"title": "The chain of quantum mechanics equations", "abstract": "The chain of quantum mechanics equations is constructed for wave functions\ndepending on coordinate and time as well as on velocity, acceleration and\nacceleration of higher orders."}, {"title": "From Duffin-Kemmer-Petiau to Tzou algebras in relativistic wave equations", "abstract": "We study relation between the Duffin-Kemmer-Petiau algebras and some\nrepresentations of Tzou algebras. Working in the setting of relativistic wave\nequations we reduce, via a similarity transformation, five and ten dimensional\nDuffin-Kemmer-Petiau algebras to three and seven dimensional Tzou algebras,\nrespectively."}, {"title": "On axiomatic formulation of gravity and matter field theories with MDRs and Finsler-Lagrange-Hamilton geometry on (co)tangent Lorentz bundles", "abstract": "We develop an axiomatic geometric approach and provide an unconventional\nreview of modified gravity theories, MGTs, with modified dispersion relations,\nMDRs, encoding Lorentz invariance violations, LIVs, classical and quantum\nrandom effects, anisotropies etc. There are studied Lorentz-Finsler like\ntheories elaborated as extensions of general relativity, GR, and quantum\ngravity, QG, models and constructed on (co) tangent Lorentz bundles, i.e.\n(curved) phase spaces or locally anisotropic spacetimes. An indicator of MDRs\nis considered as a functional on various type functions depending on phase\nspace coordinates and physical constants. It determines respective generating\nfunctions and fundamental physical objects (generalized metrics, connections\nand nonholonomic frame structures) for relativistic models of Finsler, Lagrange\nand/or Hamilton spaces. We show that there are canonical almost symplectic\ndifferential forms and adapted (non) linear connections which allow us to\nformulate equivalent almost K\\\"{a}hler-Lagrange / - Hamilton geometries. This\nway, it is possible to unify geometrically various classes of (non) commutative\nMGTs with locally anisotropic gravitational, scalar, non-Abelian gauge field,\nand Higgs interactions. We elaborate on theories with Lagrangian densities\ncontaining massive graviton terms and bi-connection and bi-metric modifications\nwhich can be modelled as Finsler-Lagrange-Hamilton geometries. An appendix\ncontains historical remarks on elaborating Finsler MGTs and a summary of\nauthor's results in twenty directions of research on (non) commutative/\nsupersymmetric Finsler geometry and gravity; nonholonomic geometric flows,\nlocally anisotropic superstrings and cosmology, etc."}, {"title": "Dissipative Viscous Cylindrical Collapse in $f(R)$ Gravity With Full Causal Approach", "abstract": "The plan of this study is to inspect the effects of dynamics of dissipative\ngravitational collapse in cylindrical symmetric non-static spacetime by using\nMisner-Sharp concept in context of metric $f(R)$ theory of gravity. For more\ngeneric isotropic fluid distribution of cylindrical object by dissipative\nnature of dark source of the fluid due to energy matter tensor,the Misner-Sharp\napproach technique has been used to illustrate the heat flux with free\nradiating out flow, bulk and shear viscosity. Furthermore, dynamical equation\nhave been associated with full casual heat transportation equations in\nframework of $M\\ddot{u}ller$-Israel-Stewart formalism. The present study\nexplain the effects of thermodynamics viscid/heat radiating coupling factors on\ngravitational collapse in $M\\ddot{u}ller$-Israel-Stewart notion and matches\nwith the consequences of prior astrophysicists by excluding such coefficients\nand viscosity variables."}, {"title": "Thermodynamics of Gravitationally Induced Particle Creation Scenario in DGP Braneworld", "abstract": "In this paper, we discuss the thermodynamical analysis for gravitationally\ninduced particle creation scenario in the framework of DGP braneworld model.\nFor this purpose, we consider apparent horizon as the boundary of the universe.\nWe take three types of entropy such as Bakenstein entropy, logarithmic\ncorrected entropy and power law corrected entropy with ordinary creation rate\n$\\Gamma$. We analyze the first law and generalized second law of thermodynamics\nanalytically for these entropies which hold under some constraints. The\nbehavior of total entropy in each case is also discussed which implies the\nvalidity of generalized second law of thermodynamics. Also, we check the\nthermodynamical equilibrium condition for two phases of creation rate, that is\nconstant and variable $\\Gamma$ and found its validity in all cases of entropy."}, {"title": "A model to inter-relate the values of the quantum electrodynamic, gravitational and cosmological constants", "abstract": "The fundamental constants of electromagnetism, gravity and quantum mechanics\ncan be related empirically by the numerical approximation $\\ln(V_e/V_P)\\approx\n\\alpha^{-1}$, where $\\alpha$ is the low energy value of the electromagnetic\nfine structure constant and $V_e$ and $V_P$ are volumes corresponding to the\nclassical electron radius, $r_e$, and the Planck length respectively. This\nlogarithmic relation is used in an ideal gas model to determine the work, $W$,\ndone when a hypothetical vacuum fluctuation expands relativistically from $V_P$\nto $V_e$ in a time limited by the uncertainty principle. It is proposed that\nthe expansion is a phenomenological representation of a quantum transition from\na Planck-scale initial state into a final virtual photonic state of energy\n$W\\simeq \\hbar c/2r_e$ and lifetime $\\simeq r_e/c$, occupying a volume $\\simeq\nV_e$. The magnitude of the negative gravitational self-energy density,\n$\\rho_G$, of this virtual state is found to be within $\\sim 10\\%$ of the\nmeasured value of the positive \"dark energy\" density, $\\rho_\\Lambda$. It is\nproposed that this is not merely an \"accidental\" numerical coincidence but has\nphysical significance, namely that the sum of the two energy densities is zero,\ni.e. $\\rho_\\Lambda+\\rho_G=0$. This relation gives a value of the cosmological\nconstant, $\\Lambda$, in agreement with astronomical measurements. The\nimplications of these inter-relations between $\\Lambda$, the gravitational\nconstant, $G$, and $\\alpha$ are outlined."}, {"title": "Charge conservation in a gravitational field in the scalar ether theory", "abstract": "A modification of the Maxwell equations due to the presence of a\ngravitational field was formerly proposed for a scalar theory with a preferred\nreference frame. With this modification, the electric charge is not conserved.\nThe aim of the present work was to numerically assess the amount of charge\nproduction or destruction. We propose an asymptotic scheme for the\nelectromagnetic field in a weak and slowly varying gravitational field. This\nscheme is valid independently of the theory and the \"gravitationally-modified\"\nMaxwell equations. Then we apply this scheme to plane waves and to a group of\nHertzian dipoles in the scalar ether theory. The predicted amounts of charge\nproduction/destruction discard the formerly proposed gravitationally-modified\nMaxwell equations. The theoretical reason for that is the assumption that the\ntotal energy tensor is the sum of the energy tensor of the medium producing the\nelectromagnetic (e.m.) field and the e.m. energy tensor. This means that an\nadditional, \"interaction\" tensor has to be present. With this assumption, the\nstandard Maxwell equations in a curved spacetime, which predict charge\nconservation, are compatible with the investigated theory. We find that the\ninteraction energy might contribute to the dark matter."}, {"title": "$4$-index theory of gravity and its relation with the violation of the energy-momentum conservation law", "abstract": "Recently, a $4$-index generalization of the Einstein theory is proposed by\nMoulin (Eur. Phys. J. C 77, 878 (2017)). Using this method, we find the most\ngeneral $2$-index field equations derivable from the Einstein-Hilbert action.\nThe application of Newtonian limit, the role of gravitational coupling constant\nand the effects of the properties of ordinary energy-momentum tensor in\nobtaining a $4$-index gravity theory have been studied. We also address the\nresults of building Weyl free $4$-index gravity theory. Our study displays that\nboth the Einstein and Rastall theories can be obtained as the subclasses of a\n$4$-index gravity theory which shows the power of $4$-index method in unifying\nvarious gravitational theories. It is also obtained that the violation of the\nenergy-momentum conservation law may be allowed in $4$-index gravity theory,\nand moreover, the contraction of $4$-index theory generally admits a\nnon-minimal coupling between geometry and matter field in the Rastall way. This\nstudy also shows that, unlike the Einstein case, the gravitational coupling\nconstant of $4$-index Rastall theory generally differs from that of the\nordinary $2$-index Rastall theory."}, {"title": "Ideal gas with a varying (negative absolute) temperature: An alternative to dark energy?", "abstract": "The present work is an attempt to investigate whether the evolutionary\nhistory of the Universe from the offset of inflation can be described by\nassuming the cosmic fluid to be an ideal gas with a specific gas constant but a\nvarying negative absolute temperature (NAT). The motivation of this work is to\nsearch for an alternative to the \"exotic\" and \"supernatural\" dark energy (DE).\nIn fact, the NAT works as an \"effective quintessence\" and there is need to deal\nneither with exotic matter like DE nor with modified gravity theories. For the\nsake of completeness, we release some clarifications on NATs in Section 3 of\nthe paper."}, {"title": "Free Fall in Gravitational Theory", "abstract": "Einstein's explanation of Mercury's perihelion motion has been verified by\nastronomical observations. His formula could also be obtained in Schwarzschild\nmetric and was published already in 1898. Motion along a straight geodesic,\nhowever, namely, free fall into a gravitational center with vanishing angular\nmomentum, is incorrectly described both by Einstein's and by Schwarzschild's\nequation of motion. A physical solution for free fall may be obtained by taking\ninto account the dependence of mass on velocity in Newton's gravitational law\nas adopted in the physics of accelerators."}, {"title": "Strong Electric Field in 2D Graphene: The Integer Quantum Hall regime from a different (many-body) perspective", "abstract": "We investigate the emerging consequences of an applied strong in-plane\nelectric field on a macroscopically large graphene sheet subjected to a\nperpendicular magnetic field, by determining in exact analytical form various\nmany-body thermodynamic properties and the Hall coefficient. The results\nsuggest exotic possibilities that necessitate very careful experimental\ninvestigation. In this alternate form of Quantum Hall Effect, non-linear\nphenomena related to the global magnetization, energy and Hall conductivity\n(the latter depending on the strengths of magnetic B- and electric E-fields)\nemerge without using perturbation methods, to all orders of E-field and B-field\nstrengths. Interestingly enough, when the value of the electric field is\nsufficiently strong, fractional quantization also emerges, whose topological\nstability has to be verified."}, {"title": "Nonlinear Schrodinger equation and classical-field description of the Lamb-Retherford experiment", "abstract": "I show that Lamb-Retherford experiment can be fully described within the\nframework of classical field theory without using concepts such as the discrete\nstates of the atom and jump-like electron transitions between them. The rate of\nstimulated decay of the metastable state of a hydrogen atom in an external\nperiodic electric field is determined. The dependence of this rate on the\nfrequency and amplitude of the external electric field, as well as on the\nparameters of the atom, has been obtained. It is shown that the maximum value\nof the stimulated decay rate of the metastable state of a hydrogen atom is\nachieved at an external electric field frequency equal to the frequency shift\ncorresponding to either the fine structure of the hydrogen atom or the Lamb\nshift."}, {"title": "From Elasticity to Electromagnetism: Beyond the Mirror", "abstract": "The first purpose of this short but striking paper is to revisit Elasticity\n(EL) and Electromagnetism (EM) by comparing the structure of these two theories\nand examining with details their well known couplings, in particular\npiezoelectricity and photoelasticity. Despite the strange Helmholtz and\nMach-Lippmann analogies existing between them, no classical technique may\nprovide a common setting. However, unexpected arguments discovered\nindependently by the brothers E. and F. Cosserat in 1909 for EL and by H. Weyl\nin 1918 for EM are leading to construct a new differential sequence called\nSpencer sequence in the framework of the formal theory of Lie pseudogroups and\nto introduce it for the conformal group of space-time with 15 parameters. Then,\nall the previous explicit couplings can be deduced abstractly and one must just\ngo to a laboratory in order to know about the coupling constants on which they\nare depending, like in the Hooke or Minkowski constitutive relations existing\nrespectively in EL or EM separately. We finally provide a new combined\nexperimental and theoretical proof of the fact that any 1-form with value in\nthe second order jets (elations) of the conformal group of space-time can be\nuniquely decomposed into the direct sum of the Ricci tensor R and the\nelectromagnetic field F. This result questions the mathematical foundations of\nboth General Relativity (GR) and Gauge Theory (GT). In particular, the Einstein\noperator (6 terms) must be thus replaced by the formal adjoint of the Ricci\noperator (4 terms only) in the study of gravitational waves."}, {"title": "Group theoretical formulation of free fall and projectile motion", "abstract": "In this work we formulate the group theoretical description of free fall and\nprojectile motion. We show that the kinematic equations for constant\nacceleration form a one parameter group acting on a phase space. We define the\ngroup elements $\\phi_t$ by their action on the points in the phase space. We\nalso generalize this approach to projectile motion. We evaluate the group\norbits regarding their relations to the physical orbits of particles and\nunphysical solutions. We note that the group theoretical formulation does not\napply to more general cases involving a time dependent acceleration. This\nmethod improves our understanding of the constant acceleration problem with its\nglobal approach. It is especially beneficial for students who want to pursue a\ncareer in theoretical physics."}, {"title": "Shadow of a noncommutative geometry inspired Ayón Beato García black hole", "abstract": "We introduce the noncommutative geometry inspired Ay\\'on Beato Garc\\'ia black\nhole metric and study various properties of this metric by which we try to\nprobe the allowed values of the noncommutative parameter $\\vartheta$ under\ncertain conditions. We then construct the shadow (apparent shape) cast by this\nblack hole. We derive the corresponding photon orbits and explore the effects\nof noncommutative spacetime on them. We then study the effects of\nnoncommutative parameter $\\vartheta$, smeared mass $m(r)$, smeared charge\n$q(r)$ on the silhouette of the shadow analytically and present the results\ngraphically. We then discuss the deformation which arises in the shape of the\nshadow under various conditions. Finally, we introduce a plasma background and\nobserve how the shadow behaves in this scenario."}, {"title": "Mechanism for Resolving Gauge Hierarchy and Large Vacuum Energy", "abstract": "Alternative forms of the solutions to the quantum field equations and their\nimplications for physical theory are considered. Incorporation of these\nalternative solution forms, herein deemed \"supplemental solutions\", into the\ndevelopment of quantum field theory leads to a unique class of particle states,\nwhich may provide simple resolutions of more than one extant problem in high\nenergy physics. The symmetry between the traditional and supplemental solutions\nresults in a direct and natural zero-point energy value of zero, and, as well,\na possible mechanism for cancelling the Higgs condensate energy, thereby\nproviding a potential resolution of the large cosmological constant problem.\nFurther, this symmetry may also resolve the Higgs gauge hierarchy problem.\nResolutions of seeming theoretic impediments to supplemental fields, in\nparticular, non-positive definite Fock space metric and vacuum decay, are\npresented, and concomitant implications for unitarity are considered. As\nsupplemental solutions are already inherent in quantum field theory, little\nchange is required to the fundamental mathematics of the theory."}, {"title": "The weak-field-limit solution for Kerr black hole in radiation gauge", "abstract": "In this work we present the solution for a rotating Kerr black hole in the\nweak-field limit under the radiation gauge proposed by Chen and Zhu [Phys. Rev.\nD83, 061501(R) (2011)], with which the two physical components of the\ngravitational wave can be picked out exactly."}, {"title": "Exact solution for Schwarzschild black hole in radiation gauge", "abstract": "Recently Chen and Zhu propose a true radiation gauge for gravity [Phys. Rev.\nD 83, 061501(R) (2011)]. This work presents a general solution for the metric\nof Schwarzschild black hole in this radiation gauge."}, {"title": "On a nonstatic Painleve-Gullstrand spacetime", "abstract": "A time dependent geometry outside a spherically symmetric mass is proposed.\nThe source has zero energy density but nonzero radial and tangential pressures.\nThe time variable is interpreted as the duration of measurement performed upon\nthe physical system. For very short time intervals, the effect of the mass\nsource is much reduced, going to zero when $t \\rightarrow 0$. All physical\nquantities are finite when $t \\rightarrow 0$ and $r \\rightarrow 0$ and also at\ninfinity. The total energy flux measured on a hypersurface of constant $r$ is\nvanishing."}, {"title": "A semi-analytical approach to black body radiation", "abstract": "We describe a semi-analytical method to calculate the total radiance received\nform a black body, between two frequencies. As has been done before, the method\ntakes advantage of the fact that the solution simplifies with the use of\npolylogarithm functions. We then use it to study the amount of radiation from\nthe sun received by bodies at Earths surface."}, {"title": "Field theoretic model for the Josephson effect", "abstract": "The Josephson effect is found to stem from the quantum behavior of massive\nphotons existing in a superconducting medium. Accordingly, the Josephson\ncoupling energy is found to be equal to the rest mass energy of these photons.\nThe Josephson effect is described by propagation of massive photon field\nfollowing the universal quantum equation instead of being due to quantum\ntunnelling. The mass of the photon is found to be dependent on the electric\nproperties of the junction. A characteristic (critical) quantized capacitance\nof the junction is found to be inversely related to the critical current. The\nquantum (kinetic) inductance induced in the junction is found to be equal to\n$L_q=\\mu_0\\lambda_J$\\,, where $\\lambda_J$ is the Josephson penetration depth,\nand $\\mu_0$ is the free space permeability."}, {"title": "Derivations of the Planck Blackbody Spectrum from Thermodynamic Ideas in Classical Physics with Classical Zero-Point Radiation", "abstract": "Based upon thermodynamic ideas, two new derivations of the Planck blackbody\nspectrum are given within classical physics which includes classical zero-point\nradiation. The first and second laws of thermodynamics, applied to a harmonic\noscillator or a radiation normal mode, require that the canonical potential\n$\\phi(\\omega/T)$ is a function of a single variable corresponding to the ratio\nof the oscillation frequency to the temperature. The second law of\nthermodynamics involves extremum ideas which may be applied to thermal\nradiation. Our first derivation of the Planck spectrum is based upon the idea\nthat the canonical potential $\\phi(\\omega/T)$ is a monotonic function and all\nits derivatives are monotonic when interpolating between zero-point energy at\nlow temperature and energy equipartition at high temperature; the monotonic\nbehavior precludes the canonical potential from giving a preferred value for\nthe ratio $\\omega/T.$ Our second derivation of the Planck spectrum is based\nupon the requirement that the change in the Helmholtz free energy of the\nradiation in a partitioned box held at constant temperature should be a minimum\nat thermal equilibrium. Finally, the change in Casimir energy with change in\npartition position for the radiation in a partitioned box is shown to\ncorrespond at high temperature to the absence of zero-point energy when the\nspectral energy per normal mode is chosen as the traditional Planck spectrum\nwhich omits zero-point energy at low temperature; thus the idea of zero-point\nenergy is embedded in the traditional Planck spectrum. It is emphasized that\nthermal radiation is intimately connected with zero-point radiation and the\nstructure of spacetime in classical physics."}, {"title": "Three-Dimensional Nonlinear Stokes - Mueller Polarimetry", "abstract": "The formalism is developed for a tree-dimensional ($3D$) nonlinear\nStokes-Mueller polarimetry. The expressions are derived for the generalized\n$3D$ linear and nonlinear Stokes vectors, and the corresponding nonlinear\nMueller matrix. The coherency-like Hermitian square matrix $X$ of\nsusceptibilities is introduced, which is derived from the nonlinear Mueller\nmatrix. The $X$-matrix is characterized by the index of depolarization. Several\ndecompositions of the $X$-matrix are introduced. The $3D$ nonlinear\nStokes-Mueller polarimetry formalism can be applied for three and higher wave\nmixing processes. The $3D$ polarimetric measurements can be used for structural\ninvestigations of materials, including heterogeneous biological structures. The\n$3D$ polarimetry is applicable for nonlinear microscopy with high numerical\naperture objectives."}, {"title": "On the Fundamentality of Meaning", "abstract": "The mainstream view of meaning is that it is emergent, not fundamental, but\nsome have disputed this, asserting that there is a more fundamental level of\nreality than that addressed by current physical theories, and that matter and\nmeaning are in some way entangled. In this regard there are intriguing\nparallels between the quantum and biological domains, suggesting that there may\nbe a more fundamental level underlying both. I argue that the organisation of\nthis fundamental level is already to a considerable extent understood by\nbiosemioticians, who have fruitfully integrated Peirce's sign theory into\nbiology; things will happen there resembling what happens with familiar life,\nbut the agencies involved will differ in ways reflecting their fundamentality,\nin other words they will be less complex, but still have structures complex\nenough for what they have to do. According to one approach involving a\ncollaboration with which I have been involved, a part of what they have to do,\nalong with the need to survive and reproduce, is to stop situations becoming\ntoo chaotic, a concept that accords with familiar 'edge of chaos' ideas. Such\nan extension of sign theory (semiophysics?) needs to be explored by physicists,\npossible tools being computational models, existing insights into complexity,\nand dynamical systems theory. Such a theory will not be mathematical in the\nsame way that conventional physics theories are mathematical: rather than being\nfoundational, mathematics will be 'something that life does', something that\nsufficiently evolved life does because in the appropriate context so doing is\nof value to life."}, {"title": "New way of second quantized theory of fermions with either Clifford or Grassmann coordinates and spin-charge-family theory", "abstract": "Fermions with the internal degrees of freedom described in Clifford space\ncarry in any dimension a half integer spin. There are two kinds of spins in\nClifford space. The spin-charge-family theory,assuming even d=13+1, uses one\nkind of spins to describe in d=3+1 spins and charges of quarks and leptons and\nantiquarks and antileptons, while the other kind is used to describe families.\nThe new way of second quantization, suggested by the spin-charge-family theory,\nis presented. It is shown that the creation and annihilation operators of\n1-fermion states, written as products of nilpotents and projectors of an odd\nClifford character, fulfill the anticommutation relations as required in the\nsecond quantization procedure for fermions: 1-fermion states are in Clifford\nspace already second quantized, the creation operators for any n-fermion second\nquantized vectors are products of one fermion creation operators, operating on\nthe empty vacuum state. It is demonstrated that also in Grassmann space there\nexist the creation and annihilation operators of an odd Grassmann character,\ngenerating \"fermions\", which fulfill as well the anticommutation relations for\nfermions, representing correspondingly the second quantized 1-\"fermion\" states,\nin this case with integer spins. Grassmann space offers no families. We discuss\nthe new second quantization procedure of the fields in both spaces. For the\nGrassmann case we present the action, basic states, solutions of the Weyl\nequation for free massless \"fermions\" and discrete symmetry operators. A short\noverview of the achievements of the spin-charge-family theory is done, and open\nproblems of this theory still waiting to be solved are presented. The Grassmann\nand the Clifford case are compared in order to better understand open questions\nin physics of elementary fermion and boson fields and in cosmology."}, {"title": "Unified Classical and `Quantum Mechanical' Gravity!", "abstract": "It is shown that a unified description of classical and `quantum mechanical'\ngravity in its linearized form is possible."}, {"title": "Non-relativistic Arbitrary l-states of Quarkonium through Asymptotic Iteration Method", "abstract": "The energy eigenvalues with any l-states and mass of heavy quark- antiquark\nsystem (quarkonium) are obtained by using Asymptotic Iteration Method in the\nview of non-relativistic quantum chromodynamics, in which the quarks are\nconsidered as spinless for easiness, and are bounded by Cornell potential. A\nsemi-analytical formula for energy eigenvalues and mass is achieved via the\nmethod in scope of the perturbation theory. The accuracy of this formula is\nchecked by comparing the eigenvalues with the ones numerically obtained in this\nstudy, and with exact ones in literature. Furthermore, semi-analytical formula\nis applied to some meson systems for comparing the masses with the experimental\ndata."}, {"title": "The fallacy of Schott energy-momentum", "abstract": "The incompatibility between Larmor's formula for radiation losses (at a rate\nproportional to square of the acceleration of the electric charge) and the\nradiation reaction (the rate of loss of momentum of the accelerated charge\nproportional to its rate of change of acceleration) was recently shown to arise\nbecause a proper distinction is not kept between radiation losses calculated in\nterms of a retarded time and those expressed in terms of a \"real time\".\nHowever, the occurrence of this disparity between two formulations is usually\nreconciled in literature by proposing an acceleration-dependent Schott energy\nlying somewhere in the nearby electromagnetic fields of an accelerated charge.\nBut nobody has yet unambiguously demonstrated where the Schott energy actually\nlies in the fields. By scrutinizing electromagnetic fields of a uniformly\naccelerated charge, a mathematically tractable case, we show that contrary to\nthe ideas prevalent in the literature, there is no evidence of any\nacceleration-dependent Schott energy-momentum in the electromagnetic fields,\nanywhere in the near vicinity of the charge or elsewhere. Accordingly, we\nexpose the fallacy of the Schott energy-momentum term, which should henceforth\nbe abandoned, in the electromagnetic radiation formulation."}, {"title": "Precursors of gate oxide degradation in SiC power MOSFETs", "abstract": "Gate oxide degradation is more critical in Silicon-Carbide (SiC) MOSFETs than\nin Silicon (Si) MOSFETs. This is because of the smaller gate oxide thickness\nand the higher electric field that develops across the gate oxide in SiC\nMOSFETs. While multiple precursors have been identified for monitoring the gate\noxide degradation in Si MOSFETs, very few precursors have been identified for\nSiC MOSFETs. The purpose of this paper is to demonstrate that gate oxide\ndegradation precursors used in Si MOSFETs: a) threshold voltage, b) gate\nplateau voltage and c) gate plateau time, can also be used as precursors for\nSiC MOSFETS. Moreover, all three precursors are found to exhibit a simultaneous\nincreasing trend (during the stress time) leading to an increase in on-state\nloss, switching loss and switching time of the SiC MOSFET. The existing studies\nof gate oxide degradation mechanisms in SiC MOSFETs, and their effects on\nthreshold voltage and mobility were extended to correlate a variation of all\nthree precursors using analytical expressions. The increasing trends of\nprecursors were experimentally confirmed by inducing gate oxide degradation in\ncommercial SiC MOSFET samples."}, {"title": "Fractional Cassini Coordinates", "abstract": "Introducing a set $\\{\\alpha_i\\} \\in R$ of fractional exponential powers of\nfocal distances an extension of symmetric Cassini-coordinates on the plane to\nthe asymmetric case is proposed which leads to a new set of fractional\ngeneralized Cassini-coordinate systems. Orthogonality and classical limiting\ncases are derived. An extension to cylindrically symmetric systems in $R^3$ is\ninvestigated. The resulting asymmetric coordinate systems are well suited to\nsolve corresponding two- and three center problems in physics."}, {"title": "Nanocircuits in loop structures: continuous waves preclude gauge invariant wavelengths", "abstract": "Tunnel junctions for quantum computing require discrete spectra from\ncontinuous waves on a doubly connected coordinate or loop. For an electron on a\nmetal ring discrete spectra follow from discontinuous Bloch waves. Can both\npropositions be true? We find using a gauge function originating in the\nLagrangian that continuity on a ring or loop violates gauge invariance of the\nde Broglie wavelength. This same gauge function shows that Lagrangians for the\nelectron on a ring and the charge on a junction are mutual transforms. Thus\npersistent current on a metal ring and the Coulomb blockade on a tunnel\njunction seem to be the same dynamical theory based on discontinuous Bloch\nwaves on the compact perimeter of a circle"}, {"title": "A novel connection between scalar field theories and quantum mechanics", "abstract": "This work deals with scalar field theories and supersymmetric quantum\nmechanics. The investigation is inspired by a recent result, which shows how to\nuse the reconstruction mechanism to describe two distinct field theories from\nthe very same quantum mechanics potential, and by an older work, which\ndescribes the deformation procedure that offers an interesting way to generate\nand solve new scalar field theories, starting from a given model of current\ninterest. We use the procedure to unveil a new route, from which one can\ndescribe families of scalar field theories that engender the very same quantum\nmechanics potential. The approach can be applied algorithmically, and\nimplemented to generate models that give rise to distinct quantum mechanics\nsystems as well."}, {"title": "Generalized boosts with shell structure of the parameter space", "abstract": "A modification of boost transformation in arbitrary pseudo-Euclidean space is\nsuggested, which in the case of the Minkowski space admits the existence of\ninertial reference frames moving with velocities taking values in a certain\nbounded interval. The velocity space may be partitioned by hypersurfaces\n\\beta^2=\\beta^2_k=const, k=1,2,3,..., into a finite or countable number of\ndomains (shells), each of which has own class of inertial \"reference frames\"\nand the velocity composition law. These shells are in one-to-one\ncorrespondence. A set of mappings of shells to each other forms the group,\nisomorphic to permutation group in the case of finite number of shells, or the\ngroup of integers in the case of countable number of shells in the velocity\nspace"}, {"title": "Quantum models based on finite groups", "abstract": "We consider a constructive modification of quantum-mechanical formalism.\nReplacement of a general unitary group by unitary representations of finite\ngroups makes it possible to reproduce quantum formalism without loss of its\nempirical content. Since any linear representation of a finite group can be\nimplemented as a subrepresentation of a permutation representation,\nquantum-mechanical problems can be formulated in terms of permutation groups.\nReproducing quantum behavior in the framework of permutation representations of\nfinite groups makes it possible to clarify the meaning of a number of physical\nconcepts."}, {"title": "Possible scheme for observing acceleration (Unruh) radiation", "abstract": "In an FEL the electrons traveling through the undulator are surrounded in\ntheir own reference frame by Unruh radiation at a temperature of order 8,000\nKelvin. When these virtual photons scatter from the beam electrons they become\nreal and can be detected. Because of the microbunching of the FEL electron beam\nthis process proceeds coherently for a fraction of the electrons in the\nmicrobunch. This enhances the Unruh radiation which is still dominated by the\ncopious spontaneous radiation in the same energy range. We discuss the\nparticular case of the Stanford LCLS, as well as the case of extreme\nacceleration, when the x-ray beam is brought into collision with the 14 GeV\nelectron beam."}, {"title": "Magnetically charged black hole in framework of nonlinear electrodynamics model", "abstract": "A model of nonlinear electrodynamics is proposed and investigated in general\nrelativity. We consider the magnetic black hole and find a regular solution\nwhich gives corrections into the Reissner-Nordstr\\\"{o}m solution. At\n$r\\rightarrow\\infty$ the asymptotic spacetime becomes flat. The magnetic mass\nof the black hole is calculated and the metric function is obtained. At some\nvalues of the model parameter there can be one, two or no horizons.\nThermodynamics of black holes is studied and we calculate the Hawking\ntemperature and heat capacity of black holes. It is demonstrated that there is\na phase transition of second order. At some parameters of the model black holes\nare thermodynamically stable."}, {"title": "Thermodynamic approach to holographic dark energy and the Rényi entropy", "abstract": "Using the first law of thermodynamics, we propose a relation between the\nsystem entropy ($S$) and its IR ($L$) and UV ($\\Lambda$) cutoffs. In addition,\napplying this relation to the apparent horizon of flat FRW universe, whose\nentropy meets the R\\'{e}nyi entropy, a new holographic dark energy model is\naddressed. Thereinafter, the evolution of the flat FRW universe, filled by a\npressureless source and the obtained dark energy candidate, is studied. In our\nmodel, there is no mutual interaction between the cosmos sectors. We find out\nthat the obtained model is theoretically powerful to explain the current\naccelerated phase of the universe. This result emphasizes that the generalized\nentropy formalism is suitable for describing systems including the long-range\ninteractions such as gravity."}, {"title": "Hubble Expansion as an Einstein Curvature", "abstract": "Extending the spacetime manifold of general relativity (GR) to incorporate\nthe Hubble expansion of space as a specific curvature, generates a modified\nsolution with three additional non-zero Christoffel symbols and a reformulated\nRicci tensor and curvature. The observational consequences of this\nreformulation are compared with the $\\Lambda$CDM model for luminosity distance\nusing the extensive type~Ia supernovae (SNe~1a) data with redshift corrected to\nthe CMB, and for angular diameter distance using the recent baryonic acoustic\noscillation (BAO) data. For the SNe~1a data, the modified GR and $\\Lambda$CDM\nmodels differ by $^{+0.11}_{-0.15}~\\mu_B$~mag. over $z_{cmb}=0.01-1.3$, with\noverall weighted RMS errors of $\\pm0.136$ $\\mu_B$~mag for modified GR and\n$\\pm0.151$ $\\mu_B$~mag for $\\Lambda$CDM respectively. The BAO measures span a\nrange $z=0.106-2.36$, with weighted RMS errors of $\\pm0.034$~Mpc with\n$H_0=67.6\\pm0.25$ for the modified GR model, and $\\pm0.085$~Mpc with\n$H_0=70.0\\pm0.25$ for the $\\Lambda$CDM model. The derived GR metric for this\nnew solution describes both the SNe~1a and the BAO observations with comparable\naccuracy to the $w'\\Lambda$CDM model. By incorporating the Hubble expansion of\nspace within general relativity as a specific curvature term, these\nobservations may be described without requiring additional parameters for\neither dark matter or accelerating dark energy."}, {"title": "Braids, normed division algebras, and Standard Model symmetries", "abstract": "This paper represents a first attempt at unifying two promising models that\nattempt to explain the origin of the internal symmetries of leptons and quarks.\nIt is shown that each of the four normed division algebras over the reals\nadmits a representation of a circular braid group. For the complex numbers and\nthe quaternions, the represented circular braid groups are $B_2$ and $B_3^c$,\nprecisely those used to construct leptons and quarks as framed braids in the\nHelon model of Bilson-Thompson. It is then shown that these framed braids\ncoincide with the states that span the minimal left ideals of the complex\n(chained) octonions, shown by Furey to describe one generation of leptons and\nquarks with unbroken $SU(3)_{c}$ and $U(1)_{em}$ symmetry.\n  The identification of basis states of minimal ideals with certain framed\nbraids is possible because the braiding in $B_2$ and $B_3^c$ in the Helon model\nare interchangeable. It is shown that the framed braids in the Helon model can\nbe written as pure braid words in $B_3^c$ with trivial braiding in $B_2$,\nsomething which is not possible for framed braids in general."}, {"title": "Massive electrodynamics for London's superconductivity and Josephson effect", "abstract": "Massive electrodynamics for London's superconductivity and Josephson effect\nare derived. The propagation of massive boson inside a medium yields electric\nphenomena that are reflected in the Josephson effect. Critical force, magnetic\nfield and temperature are found to be related to the critical current of the\njunction. The mass of the boson depends only on the critical current of the\njunction. The electromagnetic interaction between the Cooper pairs in the two\nsides of the superconductor in the josephson junction is mediated by a massive\nboson. The propagation of the electromagnetic waves mediated by the massive\nbosons gives rise to the electric properties of the Josephson junction. Of\nthese properties are a quantized resistance of Hall type corresponding to a\nnon-quantized magnetic flux, and a quantized capacitance. A non zero magnetic\nflux encompassing a magnetic charge is found to arise despite the fact that it\nis not a priori assumed."}, {"title": "Evolving topologically deformed wormholes supported in the dark matter halo", "abstract": "In this paper, we construct an evolving wormhole in the dark matter halo.\nThis work is relevant since matter has two components: (i) cosmological part\n(only time-dependent) and (ii) wormhole part (only space-dependent). In order\nto implement this, we use the Chaplygin gas as an equation of state for the\ncosmic part and Navarro-Frenk-White dark matter density profile as well as\nThomas-Fermi (TF) profile in order to form a dark wormhole. The flare-out\ncondition of wormhole is also satisfied by violating the null energy condition\n(NEC) for some specific values of quantities. Furthermore, we reveal more\ninteresting results regarding how an topologically deformation parameter\n$\\alpha$ affects the evolving wormhole sourced with some dark matter models\nbased on the physically motivated shape function."}, {"title": "Space-Time Quasicrystal Structures and Inflationary and Late Time Evolution Dynamics in Accelerating Cosmology", "abstract": "We construct new classes of cosmological solutions in modified and Einstein\ngravity theories encoding space-time quasicrystal, STQC, configurations\nmodelled by nonlinear self-organized and pattern forming quasi-periodic\nstructures. Such solutions are defined by generic off-diagonal locally\nanisotropic and inhomogeneous metrics depending via generating and integration\nfunctions on all spacetime coordinates. There are defined nonholonomic\nvariables and conditions for the generating/integration functions and sources\nfor effective descriptions, or approximations, as \"quasi\"\nFriedmann-Lama\\^itre-Robertson-Walker (FLRW) metrics. Such (off-) diagonal\nSTQC-FLRW configurations contain memory on nonlinear classical and/or quantum\ninteractions and may describe new acceleration cosmology scenarios. For special\ntime-periodic conditions on nonlinear gravitational and matter field\ninteractions, we can model at cosmological scales certain analogous of time\ncrystal like structures originally postulated by Frank Wilczek in condensed\nmatter physics. We speculate how STQC quasi-FLRW configurations could explain\nmodern cosmology data and provide viable descriptions for the inflation and\nstructure formation in our Universe. Finally, it is discussed systematically\nand critically how a unified description of inflation with dark energy era can\nbe explained by (modified) cosmological STQC-scenarios."}, {"title": "Resolution of the 300-Year-Old Vibrating String Controversy", "abstract": "The dispute about the well-known 1D vibrating string model and its solutions,\nknown as The Vibrating String Controversy, spanned the whole of 1700s and\ninvolved a group of the most eminent scientists of the time. After that, the\nmodel stood undisputed for over two centuries. In this study, it is shown that\nnot only this 300-year-old model cannot correspond to reality, but it is\ntheoretically not quite plausible, either. A new 2D model is developed removing\nall the assumptions of the classical model. The result is a pair of non-linear\npartial differential equations modeling 2D motions of a finite 1D string. A\ntheorem that can be used to determine the initial displacement functions from\nthe initial shape of the string is proven. The new model is capable of\nrepresenting initial conditions that cannot be handled in the classical model.\nIt also allows initially non-taut/non-slack strings and self-intersecting\nshapes. The classical model and the non-taut strings emerge as special limit\ncases. It is proven that pure transverse motions of a 1D string are possible\nonly in very rare cases. A theorem that sets the conditions for pure transverse\nmotions is also presented. Numerical studies of interesting cases are presented\nin support of the new model. High-speed camera experiments are also conducted,\nthe results of which also support the new theory."}, {"title": "Cosmological models with a hybrid scale factor in an extended gravity theory", "abstract": "A general formalism to investigate Bianchi type $VI_h$ universes is developed\nin an extended theory of gravity. A minimally coupled geometry and matter field\nis considered with a rescaled function of $f(R,T)$ substituted in place of the\nRicci scalar $R$ in the geometrical action. Dynamical aspects of the models are\ndiscussed by using a hybrid scale factor that behaves as power law in an\ninitial epoch and as an exponential form at late epoch. The power law behaviour\nand the exponential behaviour appear as two extreme cases of the present model."}, {"title": "Has the density of sources of gamma-ray burts been constant over the last ten billion years ?", "abstract": "A generic tired-light mechanism is examined in which a photon, like any\nparticle moving in a medium, experiences friction, that is, a force resisting\nits motion. If the velocity of light is assumed to be constant, this hypothesis\nyields a Hubble-like law which is also a consequence of the Rh = ct cosmology.\nHerein, it is used for estimating matter density as a function of redshift,\nallowing to show that the density of sources of long gamma-ray bursts appears\nto be nearly constant, up to z $\\approx$ 4. Assuming that the later is a fair\nprobe of the former, this means that matter density has been roughly constant\nover the last ten billion years, implying that, at least over this period,\nmatter has been in an overall state of equilibrium."}, {"title": "Detection of Exoplanets Using the Transit Method", "abstract": "I conducted differential photometry on a star GSC 3281-0800, a known host to\nexoplanet HAT-P-32b, using analysis software AstroImageJ. I plotted the\nmeasurements from a series of images taken during the transit, via ADU count\ngiven from an earth-based digital CCD camera. I was able to establish a\ndefinite light curve and learn more about the properties of this exoplanet."}, {"title": "Conception of Brownian coil", "abstract": "This article proposes a conception of Brownian coil. Brownian coil is a tiny\ncoil with the same size of pollen. Once immersed into designed magnetic field\nand liquid, the coil will be moved and deformed macroscopically, due to the\nmicroscopic thermodynamic molecular collisions. Such deformation and movement\nwill change the magnetic flux through the coil, by which an ElectroMotive Force\n(EMF) is produced. In this work, Brownian heat exchanger and Brownian generator\nare further designed to transform the internal energy of liquid into other\nform: 1) the internal energy of the resistance; 2) the constant electric\nenergy. The two forms accord with the Clausius' statement and Kelvin's\nstatement respectively. If the ideas can be realized, the second law of\nthermodynamics and the second kind of perpetual-motion machine should be\nunderstood again."}, {"title": "Towards differential elimination of spinor field from spinor electrodynamics", "abstract": "A system of PDEs for the electromagnetic field and one real component of the\nspinor field is generally equivalent to spinor electrodynamics. There are\nreasons to believe that the component can be differentially eliminated from the\nsystem. A Lagrangian depending on the electromagnetic field and one real\ncomponent of the spinor field generally describes the same physics as spinor\nelectrodynamics."}, {"title": "An Introduction to Influence Theory: Kinematics and Dynamics", "abstract": "Influence theory is a foundational theory of physics that is not based on\ntraditional empirically defined concepts, such as positions in space and time,\nmass, energy, or momentum. Instead, the aim is to derive these concepts, and\ntheir empirically determined relationships, from a more primitive model. It is\npostulated that there exist things, which we call particles, that influence one\nanother in a discrete and directed fashion resulting in a partially ordered set\nof influence events. We consider the problem of consistent quantification of\nthe influence events. Observers are modeled as particle chains (observer\nchains) as if an observer were able to track a particle and quantify the\ninfluence events that the particle experiences. From these quantified influence\nevents, we study consistent quantification of the universe of events based on\nthe observer chains. In this paper, we both review and further develop the\nkinematics and dynamics of particles from the perspective of influence theory."}, {"title": "Rotating planets in Newtonian gravity", "abstract": "Variational techniques have been used in applications of hydrodynamics in\nspecial cases but an action that is general enough to deal with both potential\nflows and solid-body flows, such as cylindrical Couette flow and rotating\nplanets, has been proposed only recently. This paper is one of a series that\naims to test and develop the new Action Principle. We study a model of rotating\nplanets, a compressible fluid in a stationary state of motion, under the\ninfluence of a fixed or mutual gravitational field. The main problem is to\naccount for the shape and the velocity fields, given the size of the equatorial\nbulges, the angular velocity at equator and the density profiles. The theory is\napplied to the principal objects in the solar system, from Earth and Mars to\nSaturn with fine details of its hexagonal flow and to Haumea with its odd\nshape. With only 2 parameters the model gives a fair fit to the shapes and the\nangular velocity field near the surface. Planetary rings are an unforeseen, but\na natural and inevitable feature of the dynamics; no cataclysmic event need be\ninvoked to justify them. The simple solutions that have been studied so far are\nmost suitable for the hard planets, and for them the predicted density profiles\nare reasonable. The effect of precession was not taken into account, nor were\nentropic forces, so far. There has not yet been a systematic search for truly\nrealistic solutions. The intention is to test the versatility of the action\nprinciple; the indications are are very encouraging."}, {"title": "A classical mistake and what it tells us. How to do better with an action principle for Hydro and Thermodynamics", "abstract": "Rayleigh's stability analysis of cylindrical Couette flow, of 1889 and 1916,\nis in contradiction with observation. The analysis is repeated in many\ntextbooks and reviews up to 2017, and its failure to agree with observation was\nduly noted. More successful approaches have been found, but little was done to\ndiscover the weak point of Rayleigh's argument, what is the reason that it\nfails. This paper identifies the mistake as one that is endemic in the\nliterature. Since the physics of the problem remains poorly understood, a\ndiscussion of this paradox should prove useful. Briefly, the argument depends\non the Navier-Stokes equation and on the assumption that a certain expression\ncalled \"energy density\" or \"kinetic potential\" can be interpreted and used as\nsuch. It is shown here that %no energy density is compatible with the\nNavier-Stokes equation, in the context of Couette flow or in general, and that\nthe use of any expression as a kinetic potential is in conflict with the\nNavier-Stokes equation, in all but a very limited context.\n  An alternative analysis of basic Couette flow, based on an action principle\nfor compressible fluids, provides a Hamiltonian density as well as a kinetic\npotential. The two are not the same, even in the simplest cases. The action\nprinciple provides a kinetic potential; a new criterion for stability\nrecognizes the profound effect of the surface adhesion and the tensile strength\nof water. It is in full agreement with observation.\n  Several new experiments are suggested."}, {"title": "Gravitational attraction until relativistic equipartition of internal and translational kinetic energies", "abstract": "Translational ordering of the internal kinematic chaos provides the Special\nRelativity referents for the geodesic motion of warm thermodynamical bodies.\nTaking identical mathematics, relativistic physics of the low speed transport\nof time-varying heat-energies differs from Newton's physics of steady masses\nwithout internal degrees of freedom. General Relativity predicts geodesic\nchanges of the internal heat-energy variable under the free gravitational fall\nand the geodesic turn in the radial field center. Internal heat variations\nenable cyclic dynamics of decelerated falls and accelerated takeoffs of\ninertial matter and its structural self-organization. The coordinate speed of\nthe ordered spatial motion takes maximum under the equipartition of\nrelativistic internal and translational kinetic energies. Observable\npredictions are discussed for verification/falsification of the principle of\nequipartition as a new basic for the ordered motion and self-organization in\nexternal fields, including gravitational, electromagnetic, and thermal ones."}, {"title": "Volume and Boundary Face Area of a Regular Tetrahedron in a Constant Curvature Space", "abstract": "An example of the volume and boundary face area of a curved polyhedron for\nthe case of regular spherical and hyperbolic tetrahedron is discussed. An exact\nformula is explicitly derived as a function of the scalar curvature and the\nedge length. This work can be used in loop quantum gravity and Regge calculus\nin the context of a non-vanishing cosmological constant."}, {"title": "Holographic Renormalization with Machine learning", "abstract": "At low energies, the microscopic characteristics and changes of physical\nsystems as viewed at different distance scales are described by universal scale\ninvariant properties investigated by the Renormalization Group (RG) apparatus,\nan efficient tool used to deal with scaling problems in effective field\ntheories. We employ an information-theoretic approach in a deep learning setup\nby introducing an artificial neural network algorithm to map and identify new\nphysical degrees of freedom. Using deep learning methods mapped to a genuine\nfield theory, we develop a mechanism capable to identify relevant degrees of\nfreedom and induce scale invariance without prior knowledge about a system. We\nshow that deep learning algorithms that use an RG-like scheme to learn relevant\nfeatures from data could help to understand the nature of the holographic\nentanglement entropy and the holographic principle in context of the AdS/CFT\ncorrespondence."}, {"title": "The Structure of Matter in Spacetime from the Substructure of Time", "abstract": "The nature of the change in perspective that accompanies the proposal of a\nunified physical theory deriving from the single dimension of time is\nelaborated. On expressing a temporal interval in a multi-dimensional form, via\na direct arithmetic decomposition, both the geometric structure of\n4-dimensional spacetime and the physical structure of matter in spacetime can\nbe derived from the substructure of time. While reviewing this construction,\nhere we emphasise how the new conceptual picture differs from the more typical\nviewpoint in theoretical physics of accounting for the properties of matter by\nfirst postulating entities on top of a given spacetime background or by\ngeometrically augmenting 4-dimensional spacetime itself. With reference to\nhistorical and philosophical sources we argue that the proposed perspective,\ncentred on the possible arithmetic forms of time, provides an account for how\nthe mathematical structures of the theory can relate directly to the physical\nstructures of the empirical world."}, {"title": "Riemannian geometry without the hypotheses of homogeneity and symmetry", "abstract": "A generalisation of Riemannian geometry is considered, based exclusively on\nthe minimal assumptions that the line element $ds$ is a regular function of\nposition and direction and that the distance of every point from itself is\nequal to zero. Besides the Riemannian line element, also Riemann's residual\nhypotheses of homogeneity and symmetry are dropped. Surprisingly, the\ninfinitesimal Pythagorean distance formula reemerges, without the need of being\npostulated, as a first approximation to $almost$ $every$ geometry that is\ninvariant with respect to direction reversal. More in general, the first\napproximation to $almost$ $every$ geometry is a one-parameter family of\nhomogeneous Riemann-Randers line elements, naturally providing the geometrical\nframework for a unified theory of the classical electromagnetic and\ngravitational fileds. Geometry naturally accounts for the hierarchy between\nelectromagnetic and gravitational interactions, their different\nattractive/repulsive nature, electric charge, CPT symmetry, Maxwell and\nEinstein equations. Within this framework higher order terms could describe new\nspacetime degrees of freedom of possible large-scale relevance."}, {"title": "Quantum interference without quantum mechanics", "abstract": "A recently proposed model of the Dirac electron, which describes observed\nproperties of the particle correctly, is in the present paper shown to be also\nable to explain quantum interference by classical probabilities. According to\nthis model, the electron is not point-like, but rather an \"entity\" formed by a\nfast periodic motion of a quantum whose energy is equal to the rest energy of\nthe electron. Only after a time span equal to the period of that periodic\nmotion the \"entity\" becomes the electron, with its properties, mass, spin,\ncharge, etc.. When in motion with respect to the observer, the \"dynamic\nsubstructure\" of the electron described in this way, leads to a certain time\nstructure of its detection probability, if the space-time point of detection is\ntaken as the space-time point of the quantum. In the typical \"two slit\"\nexperimental situation, this leads to a periodic detection probability with a\nfrequency of twice the De Broglie frequency. This result is identical to the\nresult obtained by the quantum mechanical description of the moving electron by\nthe free particle wave function. The different interpretations of the\nestablished interference pattern, inherent in the two alternative theoretical\ndescriptions is outlined, and the relation between the two descriptions\ndiscussed. It is concluded that quantum interference is well explained with\nclassical probabilities, without quantum mechanics and without paradoxes. In\nview of the demonstrated merits of the model on the one hand, and the new\naspects regarding the established theories it implies on the other, a more\nthorough investigation of its role in relation to relativistic quantum\nmechanics, and to quantum field theory is suggested. Some of the interesting\naspects are summarized."}, {"title": "Generalized Nordström Theory Revisited Part II: Nordström & Maxwell United", "abstract": "In 1945 Einstein concluded that [1]: \"The present theory of relativity is\nbased on a division of physical reality into a metric field (gravitation) on\nthe one hand, and into an electromagnetic field and matter on the other hand.\nIn reality space will probably be of a uniform character and the present theory\nbe valid only as a limiting case. For large densities of field and of matter,\nthe field equations and even the field variables which enter into them will\nhave no real significance.\". The dichotomy is resolved by introducing a complex\nRanders metric with a real valued scalar field and complex valued vector field,\nproviding a unified mathematical framework for gravitation & electromagnetism\nfor which the resulting theory's predictions agree with General Relativity; to\nleading order in the gravitational constant. Hence, the related experimental\nresults validate both theories; and the former theory's metric solutions are\nfree of spurious singularities, because its stress-energy tensor includes the\nenergy & momentum for the gravitational field; like e.g. Maxwell's\nstress-energy tensor contains the electromagnetic field."}, {"title": "Metriplectic Structure of a Radiation-Matter Interaction Toy Model", "abstract": "A dynamical system defined by a metriplectic structure is a dissipative model\ncharacterized by a specific pair of tensors, which defines the Leibniz\nbrackets. Generally, these tensors are Poisson brackets tensor and a symmetric\nmetric tensor that models purely dissipative dynamics. In this paper, the\nmetriplectic system describing a simplified two-photon absorption by a\ntwo-level atom is disclosed. The Hamiltonian component describes the free\nelectromagnetic radiation. The metric component encodes the radiation-matter\ncoupling, driving the system to an asymptotically stable state in which the\nexcited level of the atom is populated due to absorption. This work is intended\nas a first result to pave the way to apply the metriplectic formalism to many\nother irreversible processes in nonlinear optics."}, {"title": "Acoustically driven x-ray emission and matter collapse in lead", "abstract": "The action of focused underwater weak shock waves on a lead sample is\nrevealed to be not restricted by a mechanical influence only. A strong\nunexpected x-ray emission was registered from the lead foil exposed to shock\nwaves ({\\it sound into x-rays}) which were extremely adiabatic compared to\nprocesses of x-ray generation. The lead foil, exposed to shock waves, lost a\npart of its area having the shape of a polygonal hole of the size of $\\sim\n2mm$. The missing polygon of lead foil looks as a delicately removed part with\nno damage at the hole surroundings as it should be after a mechanical breaking.\nThis points to a non-mechanical mechanism of hole formation. That missing\npolygonal lead matter seems to be \"disappeared\" because the total lead volume\nwas reduced by that amount after exposure to acoustic waves ({\\it matter\ncollapse}). Both paradoxical phenomena cannot be explained by a combination of\nknown effects and a fundamentally new mechanism is required to underlie them.\nThe concept of electron anomalous states, which encouraged the experiments and\nspecified main features of them, is likely that mechanism."}, {"title": "Connecting the Cabbibo-Kobayashi-Maskawa matrix to quark masses", "abstract": "We show that the Cabbibo-Kobayashi-Maskawa interaction matrix may be\nconstructed with the quark masses."}, {"title": "A cylindrically symmetric, static anisotropic fluid spacetime and the naked singularity", "abstract": "In this article, a cylindrical symmetry and static solution of the Einstein's\nfield equations, was presented. The space-time is conformally flat, regular\neverywhere except on the symmetry axis where it possesses a naked curvature\nsingularity. The matter-energy source anisotropic fluids violates the weak\nenergy condition (WEC), diverge on the symmetry axis. We discuss geodesics\nmotion of free test-particles near to the singularity, and geodesic expansion\nin the metric to understand the nature of singularity which is naked or\ncovered, and finally the C-energy of the space-time."}, {"title": "The Viability of Phantom Dark Energy as a Quantum Field in 1st-Order FLRW Space", "abstract": "In the standard cosmological framework of the 0th-order FLRW metric and the\nuse of perfect fluids in the stress-energy tensor, dark energy with an\nequation-of-state parameter $w < -1$ (known as phantom dark energy) implies\nnegative kinetic energy and vacuum instability when modeled as a scalar field.\nHowever, the accepted values for present-day $w$ from Planck and WMAP9 include\na significant range of values less than $-1$. A flip of the sign in front of\nthe kinetic energy term in the Lagrangian remedies the negative kinetic energy\nbut introduces ghostlike instabilities, which perhaps may be rendered\nunobservable, but certainly not without great cost to the theory. Staying\nwithin the confines of observational constraints and general relativity, we\ntreat dark energy as a quantum scalar field in the background of this 1st-order\nFLRW space-time, find an approximation for the Green's function, and calculate\nthe expectation value of the field's kinetic energy for $w<-1$ using adiabatic\nexpansion to renormalize and obtain a finite value. We find that the kinetic\nenergy is positive for values of $w$ less than $-1$ in 0th- or 1st-order FLRW\nspace, thus giving more theoretical credence to observational values of $w<-1$\nand demonstrating that phantom dark energy does not categorically have negative\nkinetic energy. For a nonminimal coupling parameter $\\xi=0$, kinetic energy is\npositive for $w \\gtrsim -1.22$, which includes virtually all values of constant\n$w$ allowed by cosmological data constraints, and more negative values of $w$\ngive positive kinetic energy for non-zero values of $\\xi$. Also, our results\nare generally applicable for a massive free field or a field with a small\npotential in a 0th- or 1st-order FLRW background dominated by a fluid with a\nconstant $w$. [abridged]"}, {"title": "A dual first-postulate basis for special relativity", "abstract": "An overlooked straightforward application of velocity reciprocity to a\ntriplet of inertial frames in collinear motion identifies the ratio of their\ncyclic relative velocities' sum to the negative product as a cosmic invariant,\nwhose inverse square root corresponds to a universal limit speed. A logical\nindeterminacy of the ratio equation establishes the repeatedly observed\nunchanged speed of stellar light as one instance of this universal limit speed.\nThis formally renders the second postulate redundant. The ratio equation\nfurthermore enables the limit speed to be quantified, in principle,\nindependently of a limit speed signal. Assuming negligible gravitational\nfields, two deep-space vehicles in non-collinear motion could measure with only\na single clock the limit speed against the speed of light, without requiring\nthese speeds to be identical. Moreover, the cosmic invariant (from dynamics,\nequal to the mass-to-energy ratio) emerges explicitly as a function of signal\nresponse time ratios between three collinear vehicles, multiplied by the\ninverse square of the velocity of whatever arbitrary signal might be used."}, {"title": "Fermions and bosons in the expanding universe by the spin-charge-family theory", "abstract": "The spin-charge-family theory, which is a kind of the Kaluza-Klein theories\nin $d=(13+1)$ --- but with the two kinds of the spin connection fields, the\ngauge fields of the two Clifford algebra objects, $S^{ab}$ and $\\tilde{S}^{ab}$\n--- explains all the assumptions of the standard model: The origin of the\ncharges of fermions appearing in one family, the origin and properties of the\nvector gauge fields of these charges, the origin and properties of the families\nof fermions, the origin of the scalar fields observed as the Higgs's scalar and\nthe Yukawa couplings. The theory explains several other phenomena like: The\norigin of the dark matter, of the matter-antimatter asymmetry, the \"miraculous\"\ntriangle anomaly cancellation in the standard model and others. Since the\ntheory starts at $d=(13+1)$ the question arises how and at which $d$ had our\nuniverse started and how it came down to $d=(13+1)$ and further to $d=(3+1)$.\nIn this short contribution some answers to these questions are presented."}, {"title": "On the Origin and Nature of Dark Matter", "abstract": "It is discussed how the ideas of entropy and the second law of\nthermodynamics, conceived long ago during the nineteenth century, underly why\ncosmological dark matter exists and originated in the first three years of the\nuniverse in the form of primordial black holes, a very large number of which\nhave many solar masses including up to the supermassive black holes at the\ncentres of galaxies. Certain upper bounds on dark astrophysical objects with\nmany solar masses based on analysis of the CMB spectrum and published in the\nliterature are criticised. For completeness we discuss WIMPs and axions which\nare leading particle theory candidates for the constituents of dark matter. The\nPIMBHs (Primordial Intermediate Mass Black Holes) with many solar masses should\nbe readily detectable in microlensing experiments which search the Magallenic\nClouds and measure light curves with durations of from one year up to several\nyears."}, {"title": "Analysis of the Hubble diagram of type SNe Ia supernovae and of gamma-ray bursts. A comparison between two models", "abstract": "A paper by Harmut Traunm\\\"uller [1] showed from statistical studies of\nobservational data that the most adequate equation to represent observations on\nmagnitude and redshift from 892 type 1a supernovae is $\\mu =\n5\\,log[(1+z)\\,ln(1+z)] + const.$ Comparing the Hubble diagram calculated from\nthe observed redshift data of 280 supernovae with Hubble diagrams inferred on\nthe basis of two cosmological models in the range of z = 0.0104 to 8.1, Laszlo\nMarosi [2] found in a quite independant study that the best fit function to\nrepresent observations is $\\mu= 44.109769 \\,z^{0.059883}$. Noting that\ndifferences between the different cosmological models become more pronouced in\na photon time-of-fligth $t_s$ vs. $z$ repr\\'esentation, he also noted that the\nbest equation to account for observations may also be written $z =\n-1+e^{2.024\\,\\,10^{-18} \\,\\,ts}$. In the light of these observational data, we\ncompare the theoretical Hubble diagram obtained with the flat $\\Lambda CDM$\nmodel to the ones we have obtained few years ago [3, 4, 5] from a model that we\ncall here the \"light model\" for the sake of clarity. Our conclusions are that\nvalues calculated on the basis of the $\\Lambda CDM$ model exhibit poor\nagreement with the presently available data while the light model agrees\nexactly with observations and conclusions of statistical studies [1] and [2]\n(independently of the values of $\\Omega_k$, $\\Omega_M$ or $\\Omega_{\\Lambda}$).\nOur model giving no accelerating expansion of the universe, we conclude that\nthis latter is not necessary and that models can exist which lead exactly to\nobservations without having to consider any accelerating expansion of the\nuniverse. In an Appendix, we discuss some aspects of the model and we present a\nbrief overview of some of its key results."}, {"title": "GW170817 event rules out general relativity in favor of vector gravity", "abstract": "The observation of gravitational waves by the three LIGO-Virgo\ninterferometers allows the examination of the polarization of gravitational\nwaves. Here we analyze the binary neutron star event GW170817, whose source\nlocation and distance are determined precisely by concurrent electromagnetic\nobservations. Applying a signal accumulation procedure to the LIGO-Virgo strain\ndata, we find ratios of the signals detected by the three interferometers. We\nconclude that the signal ratios are inconsistent with the predictions of\ngeneral relativity, but consistent with the recently proposed vector theory of\ngravity [Phys. Scr. 92, 125001 (2017)]. Moreover, we find that vector gravity\nyields a distance to the source in agreement with the astronomical\nobservations. If our analysis is correct, Einstein's general theory of\nrelativity is ruled out in favor of vector gravity at 99% confidence level and\nfuture gravitational wave detections by three or more observatories should\nconfirm this conclusion with higher precision."}, {"title": "Contrasting Interactions Between Dipole Oscillators in Classical and Quantum Theories: Illustrations of Unretarded van der Waals Forces", "abstract": "Students encounter harmonic-oscillator models in many aspects of basic\nphysics, within widely-varying theoretical contexts. Here we highlight the\ninterconnections and varying points of view. We start with the classical\nmechanics of masses coupled by springs and trace how the same essential systems\nare reanalyzed in the unretarded van der Waals interactions between dipole\noscillators within classical and quantum theories. We note how classical\nmechanical ideas from kinetic theory lead to energy equipartition which\ndetermines the high-temperature van der Waals forces of atoms and molecules\nmodeled as dipole oscillators. In this case, colliding heat-bath particles can\nbe regarded as providing local hidden variables for the statistical mechanical\nbehavior of the oscillators. Next we note how relativistic classical\nelectrodynamical ideas conflict with the assumptions of nonrelativistic\nclassical statistical mechanics. Classical electrodynamics which includes\nclassical zero-point radiation leads to van der Waals forces between dipole\noscillators, and these classical forces agree at all temperatures with the\nforces derived from quantum theory. However, the classical theory providing\nthis agreement is not a local theory, but rather a non-local hidden-variables\ntheory. The classical theory can be regarded as involving hidden variables in\nthe random phases of the plane waves spreading throughout space which provide\nthe source-free random radiation."}, {"title": "Space-time can be neither discrete nor continuous", "abstract": "We show that our recent Bohr-like approach to black hole (BH) quantum physics\nimplies that space-time quantization could be energy-dependent. Thus, in a\ncertain sense, space-time can be neither discrete nor continuous. Our approach\npermits also to show that the \"volume quantum\" of the Schwarzschild space-time\nincreases with increasing energy during BH evaporation and arrives to a maximum\nvalue when the Planck scale is reached and the generalized uncertainty\nprinciple (GUP) prevents the total BH evaporation. Remarkably, this result does\nnot depend on the BH original mass. The interesting consequence is that the\nbehavior of BH evaporation should be the same for all Schwarzschild BHs when\nthe Planck scale is approached."}, {"title": "The Averagely Radial Speed of Light for The Rotating And Charged Black Hole", "abstract": "The Kerr-Newman metric is used to discuss the averagely measured speed of\nlight along the radial direction at the black hole from a weak-gravitation\nreference frame such as an observer on Earth. The velocity equation of light at\nthe black hole is represented in the spherical coordinate (r, thita, phi) and\nthe main parameters are the Schwarzschild radius RS, the rotation term a, and\nthe charged term RQ. From the calculations, the averagely radial speed of light\nfrom r=Rs to r= (alpha)Rs with alpha>1 is possibly exceeding the speed of light\nc in free space by an observer in a reference frame far away from the black\nhole like on Earth. The result extends to the large r region when the rotation\nof the black hole is very high or the charge is large enough. This averagely\nradial speed finally goes to c in a large distance away from the black hole. We\nalso propose a new explanation based on our results that the observation of the\nfaster-than-light particle is due to the light bending near the black hole or\nsupermassive star with very strong gravity. Finally, we also give explanation\nthat the propagation speed of gravity shall not be faster than the\ncorresponding speed of light."}, {"title": "Impact models of gravitational and electrostatic forces: Potential energies, atomic clocks, gravitational anomalies and redshift", "abstract": "The far-reaching gravitational force is described by a heuristic impact model\nwith hypothetical massless entities propagating at the speed of light in vacuum\nand transferring momentum and energy be- tween massive bodies through\ninteractions on a local basis. In the original publication (Wilhelm et al.\n2013), a spherical symmetric emission of secondary entities had been\npostulated. The potential energy problems in gravitationally and\nelectrostatically bound two-body systems have been studied in the framework of\nthis im- pact model of gravity and of a proposed impact model of the\nelectrostatic force (Wilhelm et al. 2014). These studies have indicated that an\nanti-parallel emission of a secondary entity - now called graviton - with\nrespect to the incoming one is more appropriate. This article is based on the\nlatter choice and presents the modifications resulting from this change. The\nmodel has been applied to multiple interactions of gravitons in large mass\nconglomerations in several publications. They will be summarized here taking\nthe modified interaction process into account. In addition, the speed of\nphotons as a function of the gravitational potential are considered in this\ncontext together with the dependence of atomic clocks and the redshift on the\ngravitational potential."}, {"title": "Structural color tuning in 1D photonic crystals with electric field and magnetic field", "abstract": "A tuning of the light transmission properties of 1D photonic structures\nemploying an external stimulus is very attracting and opens the way to the\nfabrication of optical switches for colour manipulation in sensing, lighting,\nand display technology. We present the electric field-induced tuning of the\nlight transmission in a photonic crystal device, made by alternating layers of\nsilver nanoparticles and titanium dioxide nanoparticles. We show a shift of\naround 10 nm for an applied voltage of 10 V. We ascribe the shift to an\naccumulation of charges at the silver/TiO2 interface due to electric field,\nresulting in an increase of the number of charges contributing to the plasma\nfrequency in silver, giving rise to a blue shift of the silver plasmon band,\nwith concomitant blue shift of the photonic band gap. The employment of a\nrelatively low applied voltage gives the possibility to build a compact and\nlow-cost device. We also propose the fabrication of 1D photonic crystal and\nmicrocavities employing a magneto-optical material as TGG (Tb3Ga5O12). With\nthese structures we can observe a shift of 22 nm with a magnetic field of 5 T,\nat low temperature (8 K). The option to tune the colour of a photonic crystal\nwith magnetic field is interesting because of the possibility to realize\ncontactless optical switches. We also discuss the possibility to achieve the\ntuning of the photonic band gap with UV light in photonic crystals made with\nindium tin oxide (ITO)."}, {"title": "Phase transition of spacetime: particles as black holes in anti-de Sitter space", "abstract": "In this work, we re-examined the ancient complex metric in the recent quantum\npicture of black holes as Bose-Einstein condensates of gravitons. Both black\nholes and particles can be described by the complex Kerr-Newman metric in a 6-D\ncomplex space, which appears as a 4-D spacetime for a real or imaginary\nobserver because of the barrier of the horizon. As two kind of complex black\nholes, particle and black hole are complex conjugated and can convert into each\nother through a phase transition. From the view of an observer in 3-D real\nspace, an elementary particle with spin appears as an imaginary black hole in\nan anti-de Sitter space. The self-gravitational interaction of a particle as an\nimaginary black hole makes it obtain its wave-like nature in 4-D spacetime."}, {"title": "Existence of Compact Structures in $f(R,T)$ Gravity", "abstract": "The present paper is devoted to investigate the possible emergence of\nrelativistic compact stellar objects through modified $f(R,T)$ gravity. For\nanisotropic matter distribution, we used Krori and Barura solutions and two\nnotable and viable $f(R,T)$ gravity formulations. By choosing particular\nobservational data, we determine the values of constant in solutions for three\nrelativistic compact star candidates. We have presented some physical behavior\nof these relativistic compact stellar objects and some aspects like energy\ndensity, radial as well as transverse pressure, their evolution, stability,\nmeasure of anisotropy and energy conditions."}, {"title": "Dimensionally regularized Boltzmann-Gibbs Statistical Mechanics and two-body Newton's gravitation", "abstract": "It is believed that the canonical gravitational partition function $Z$\nassociated to the classical Boltzmann-Gibbs (BG) distribution $\\frac {e^{-\\beta\nH}} {{\\cal Z}}$ cannot be constructed because the integral needed for building\nup $Z$ includes an exponential and thus diverges at the origin.\n  We show here that, by recourse to 1) the analytical extension treatment\nobtained for the first time ever, by Gradshteyn and Rizhik, via an appropriate\nformula for such case and 2) the dimensional regularization approach of Bollini\nand Giambiagi's (DR), one can indeed obtain finite gravitational results\nemploying the BG distribution. The BG treatment is considerably more involved\nthan its Tsallis counterpart. The latter needs only dimensional regularization,\nthe former requires, in addition, analytical extension."}, {"title": "\"Quantum\" key distribution using weak classical light waves", "abstract": "The detection of very weak classical electromagnetic (light) waves by\nclassical macroscopic device is discussed. It is shown that the results of such\ndetection can be interpreted as a manifestation of the quantum properties of\nradiation, although in reality they are related to the peculiarities of the\ninteraction of weak classical electromagnetic waves with discrete atoms. We\nshow that the \"quantum\" key distribution protocol can be realized using very\nweak classical light waves and avalanche detectors, and it possesses all the\nproperties of the quantum cryptographic protocol E91 which based on entangled\nphotons."}, {"title": "Investigation of the Quantum Vacuum as an Energy Sink for Subcritical and Supercritical Vaporization Lasers", "abstract": "In this paper, it is shown that the quantum electrodynamic vacuum particle\nproduction rate by a vaporization laser is negligible and is not a significant\nenergy sink at electric field strengths beyond the Schwinger Limit."}, {"title": "Geometry and Physics of Sp(3)/Sp(1)^3", "abstract": "The action of $Sp(3)$ on a vector space $V_3\\in \\mathbb H^3$ is analyzed. The\ntransitive action of the group is conveyed by the flag manifold (coset space)\n$Sp(3)/Sp(1)^3\\sim G/H$, a Wallach space. The curvature two-forms are shown to\nmediate pair-wise interactions between the components of the $\\mathbb H^3$\nvector space. The root space of the flag manifold is shown to be isomorphic to\nthat of $SU(3)$, suggesting similarities between the representations of the\nflag manifold and those of $SU(3)$. The passage from $SU(3)$ to $Sp(3)$ and the\ninterpretation given here encompasses the spin of the fermionic components of\n$V_3$. Composite fermions are representable as linear combinations of product\nstates of the eigenvectors of $G/H$."}, {"title": "The Chern-Simons Current in Time Series of Knots and Links in Proteins", "abstract": "A superspace model of knots and links for DNA time series data is proposed to\ntake into account the feedback loop from docking to undocking state of\nprotein-protein interactions. In particular, the direction of interactions\nbetween the 8 hidden states of DNA is considered. It is a $E_{8}\\times E_{8}$\nunified spin model where the genotype, from active and inactive side of DNA\ntime data series, can be considered for any living organism. The mathematical\nmodel is borrowed from loop-quantum gravity and adapted to biology. It is used\nto derive equations for gene expression describing transitions from ground to\nexcited states, and for the 8 coupling states between geneon and anti-geneon\ntransposon and retrotransposon in trash DNA. Specifically, we adopt a modified\nGrothendieck cohomology and a modified Khovanov cohomology for biology. The\nresult is a Chern-Simons current in $(8+3)$ extradimensions of a given\nunoriented super manifold with ghost fields of protein structures. The $8$\ndimensions come from the 8 hidden states of spinor field of genetic code. The\nextradimensions come from the 3 types of principle fiber bundle in the\nsecondary protein."}, {"title": "An exact solution to the partition function of the finite-size Ising Model", "abstract": "There is no an exact solution to three-dimensional (3D) finite-size Ising\nmodel (referred to as the Ising model hereafter for simplicity) and even\ntwo-dimensional (2D) Ising model with non-zero external field to our knowledge.\nHere by using an elementary but rigorous method, we obtain an exact solution to\nthe partition function of the Ising model with $N$ lattice sites. It is a sum\nof $2^N$ exponential functions and holds for $D$-dimensional ($D=1,2,3,...$)\nIsing model with or without the external field. This solution provides a new\ninsight into the problem of the Ising model and the related difficulties, and\nnew understanding of the classic exact solutions for one-dimensional (1D)\n(Kramers and Wannier, 1941) or 2D Ising model (Onsager, 1944). With this\nsolution, the specific heat and magnetisation of a simple 3D Ising model are\ncalculated, which are consistent with the results from experiments and/or\nnumerical simulations. Furthermore, the solution here and the related\napproaches, can also be available to other models like the percolation and/or\nthe Potts model."}, {"title": "Supersymmetric Preons and the Standard Model", "abstract": "The experimental fact that standard model superpartners have not been\nobserved compels one to consider an alternative implementation for\nsupersymmetry. The basic supermultiplet proposed here consists of a photon and\na charged spin 1/2 preon field, and their superpartners. These fields are shown\nto yield the standard model fermions, Higgs fields and gauge symmetries.\nSupersymmetry is defined for unbound preons only. Quantum group SLq(2)\nrepresentations are introduced to classify topologically scalars, preons,\nquarks and leptons."}, {"title": "Analysis with observational constraints in $ Λ$-cosmology in $f(R,T)$ gravity", "abstract": "An exact cosmological solution of Einstein field equations (EFEs) is derived\nfor a dynamical vacuum energy in $f(R,T)$ gravity for\nFriedmann-Lemaitre-Robertson-Walker (FLRW) space-time. A parametrization of the\nHubble parameter is used to find a deterministic solution of EFE. The\ncosmological dynamics of our model is discussed in detail. We have analyzed the\ntime evolution of physical parameters and obtained their bounds analytically.\nMoreover, the behavior of these parameters are shown graphically in terms of\nredshift $`z'$. Our model is consistent with the formation of structure in the\nUniverse. The role of the $f(R,T)$ coupling constant $\\lambda$ is discussed in\nthe evolution of the equation of state parameter. The statefinder and Om\ndiagnostic analysis is used to distinguish our model with other dark energy\nmodels. The maximum likelihood analysis has been reviewed to obtain the\nconstraints on the Hubble parameter $H_0$ and the model parameter $n$ by taking\ninto account the observational Hubble data set $H(z)$, the Union 2.1\ncompilation data set $SNeIa$, the Baryon Acoustic Oscillation data $BAO$, and\nthe joint data set $H(z)$ + $ SNeIa$ and $H(z)$ + $SNeIa$ + $BAO $. It is\ndemonstrated that the model is in good agreement with various observations."}, {"title": "Axial momentum for the relativistic Majorana particle", "abstract": "The Hilbert space of states of the relativistic Majorana particle consists of\nnormalizable bispinors with real components, and the usual momentum operator $-\ni \\nabla$ can not be defined in this space. For this reason, we introduce the\naxial momentum operator, $ - i \\gamma_5 \\nabla$ as a new observable for this\nparticle. In the Heisenberg picture, the axial momentum contains a component\nwhich oscillates with the amplitude proportional to $m/E$, where $E$ is the\nenergy and $m$ the mass of the particle. The presence of the oscillations\ndiscriminates between the massive and massless Majorana particle. We show how\nthe eigenvectors of the axial momentum, called the axial plane waves, can be\nused as a basis for obtaining the general solution of the evolution equation,\nalso in the case of free Majorana field. Here a novel feature is a coupling of\nmodes with the opposite momenta, again present only in the case of massive\nparticle or field."}, {"title": "The cosmological constant problem or how the quantum vacuum drives the slow accelerating expansion of the Universe", "abstract": "I argue that a solution to the cosmological constant problem is to assume\nthat the expectation value of the quantum vacuum stress-energy tensor is\nproportional to the metric tensor with a negative energy density and positive\npressure. This assumption is confirmed by an explicit calculation of the vacuum\nexpectation for the free electromagnetic and Dirac fields of quantum\nelectrodynamics. As a consequence the metric of the universe might correspond\nto a FLRW with accelerating expansion only after averaging over large scales,\nbut at small scales it gives rise to an extremely rapid fluctuation between\nexpansion and contraction in every small region, with different phases in\ndifferent points. The vacuum stress-energy tensor has fluctuations that lead to\nshort periods of expansion. A calculation with plausible approximations leads\nto an estimate of the accelerating expansion that fits in the observed value."}, {"title": "Solutions of the fractional Schrödinger equation via diagonalization - A plea for the harmonic oscillator basis part 1: the one dimensional case", "abstract": "A covariant non-local extention if the stationary Schr\\\"odinger equation is\npresented and it's solution in terms of Heisenbergs's matrix quantum mechanics\nis proposed. For the special case of the Riesz fractional derivative, the\ncalculation of corresponding matrix elements for the non-local kinetic energy\nterm is performed fully analytically in the harmonic oscillator basis and leads\nto a new interpretation of non local operators in terms of generalized Glauber\nstates.\n  As a first application, for the fractional harmonic oscillator the potential\nenergy matrix elements are calculated and the and the corresponding\nSchr\\\"odinger equation is diagonalized. For the special case of invariance of\nthe non-local wave equation under Fourier-transforms a new symmetry is deduced,\nwhich may be interpreted as an extension of the standard parity-symmetry."}, {"title": "A Time-Dependent Model of Dark Energy Based on Four-Dimensional Continuous Deformation Theory", "abstract": "In this article, we investigate the mechanism of cosmological expansion and\ninflation by modeling dark energy as a four-dimensional continuous medium, with\nits elastic deformation described by a four-dimensional vector field. We\ndemonstrate that when the bulk modulus of this cosmological medium is $K = 1.64\n\\times 10^{109} \\text{ N}\\cdot\\text{m}^{-2}$, the dark energy density,\ncorresponding to the stress-energy associated with the deformation of the\nmedium, decreases by a factor of $\\sim 10^{122}$ while the scaling factor\nexpands from $\\sim 10^{-60}$ to $\\sim 10^{-32}$ over approximately $10^{-42}$\nseconds during cosmological inflation in the early universe. Our analysis\nsuggests three potential new physical phenomena for future investigation:\ndetecting longitudinal modes of elastic waves, examining discrepancies in the\nredshift of light from the early universe, and fitting supernova curves using\nthe parameters introduced in our model."}, {"title": "On perturbation theory for the Sturm-Liouville problem, Part II", "abstract": "I study some possibilities of analytically solving a particular\nSturm-Liouville problem with step-wise (piece-constant) coefficients with help\nof an iterative procedure mentioned in my previous paper (Green's function sum\nrules). I construct short, simple, but very accurate analytical formulae for\ncalculating the ground state eigenvalue and eigenfunction as well as for\ncalculating the first eigenfunction. I study numerical precision of the\nobtained approximations together with the perturbation theory results."}, {"title": "Can the apparent expansion of the Universe be attributed to an increasing vacuum refractive index ?", "abstract": "H.A. Wilson, then R.H. Dicke, proposed to describe gravitation by a spatial\nchange of the refractive index of the vacuum around a gravitational mass. Dicke\nextended this formalism in order to describe the apparent expansion of the\nUniverse by a cosmological time dependence of the global vacuum index. In this\npaper, we develop Dicke's formalism. The metric expansion in standard cosmology\n(the time-dependent scale factor of the Friedmann-Lema\\^itre curved spacetime\nmetric) is replaced by a flat and static Euclidean metric with a change with\ntime of the vacuum index. We show that a vacuum index increasing with time\nproduces both the cosmological redshift and time dilation, and that the\npredicted evolution of the energy density of the cosmological microwave\nbackground is consistent with the standard cosmology. We then show that the\ntype Ia supernovae data, from the joint SDSS-II and SNLS SNe-Ia samples, are\nwell modeled by a vacuum index varying exponentially as n(t)=exp(t/tau0), where\ntau0=8.0+0.2-0.8 Gyr. The main consequence of this formalism is that the\ncosmological redshift should affect any atom, with a relative decrease of the\nenergy levels of about -2 10^{-18} per second. Possibilities for an\nexperimental investigation of this prediction are discussed."}, {"title": "Gravitational Field of a Spherical Perfect Fluid", "abstract": "Analyzing the spacetime for a static spherically distributed perfect fluid we\nshow that the smooth matching of the interior and exterior metrics for a\nrealistic source is possible only for the distances from the origin that\nexceeds the photon sphere radius for this object."}, {"title": "Space-Time Geodesics and the Derivation of Schrödinger's equation", "abstract": "Using the essence of Feynman's path integral and the space-time geodesics, an\ninfinity of differentiable paths that follow the geometry of a continuous\ngeodesic are constructed, and a wave function is associated to each path as a\nprobability amplitude identical to the Feynman's probability amplitude for each\npath. We prove that each probability amplitude obeys to the Schr\\\"odinger's\nequation for a non relativistic physical system moving in a time independent\npotential, starting from the Jacobi-Hamilton equation."}, {"title": "$f(T)$ Corrected Instability of Cylindrical Collapsing Object with Harrison-Wheeler Equation of State", "abstract": "In this paper, we study the dynamical instability of a collapsing object in\nthe framework of generalized teleparallel gravity. We assume a cylindrical\nobject with a specific matter distribution. This distribution contains energy\ndensity, isotropic pressure component with heat conduction. We take oscillating\nstates scheme up to first order to check the instable behavior of the object.\nWe construct a general collapse equation for underlying case with non-diagonal\ntetrad depending on the matter, metric functions, heat conducting term and\ntorsional terms. The Harrison-Wheeler equation of state which contains\nadiabatic index is used to explore the dynamical instability ranges for\nNewtonian and post-Newtonian constraints. These ranges depend on perturbed part\nof metric coefficients, matter parts and torsion."}, {"title": "New about the wave function,\"Einstein's boxes'' and scattering a particle on a one-dimensional $δ$-potential", "abstract": "The connection between the problem of scattering a particle on a\none-dimensional $\\delta$-potential with the \"Einstein's boxes\" thought\nexperiment is shown. In both cases, the validity of the superposition principle\nis limited by Einstein's 'separation principle'. It is also shown that the\ngenerally accepted point of view, according to which \"To know the quantum\nmechanical state of a system implies, in general, only statistical restrictions\non the results of measurements\", is fundamentally wrong. First, even the square\nof the modulus of the wave function imposes more than just statistical\nrestrictions. Second, the phase of the wave function also has a physical\nmeaning -- it sets the field of pulses of the ensemble. That is, quantum\nmechanics not only does not prohibit the simultaneous measurement of the\ncoordinate and momentum of a particle, but also predicts the value of the\nmomentum at that spatial point where the particle will (accidentally) be\ndetected."}, {"title": "Implications of macroeconomic volatility in the Euro area", "abstract": "In this paper we estimate a Bayesian vector autoregressive model with factor\nstochastic volatility in the error term to assess the effects of an uncertainty\nshock in the Euro area. This allows us to treat macroeconomic uncertainty as a\nlatent quantity during estimation. Only a limited number of contributions to\nthe literature estimate uncertainty and its macroeconomic consequences jointly,\nand most are based on single country models. We analyze the special case of a\nshock restricted to the Euro area, where member states are highly related by\nconstruction. We find significant results of a decrease in real activity for\nall countries over a period of roughly a year following an uncertainty shock.\nMoreover, equity prices, short-term interest rates and exports tend to decline,\nwhile unemployment levels increase. Dynamic responses across countries differ\nslightly in magnitude and duration, with Ireland, Slovakia and Greece\nexhibiting different reactions for some macroeconomic fundamentals."}, {"title": "A Method for Winning at Lotteries", "abstract": "We report a new result on lotteries --- that a well-funded syndicate has a\npurely mechanical strategy to achieve expected returns of 10\\% to 25\\% in an\nequiprobable lottery with no take and no carryover pool. We prove that an\noptimal strategy (Nash equilibrium) in a game between the syndicate and other\nplayers consists of betting one of each ticket (the \"trump ticket\"), and extend\nthat result to proportional ticket selection in non-equiprobable lotteries. The\nstrategy can be adjusted to accommodate lottery taxes and carryover pools. No\n\"irrationality\" need be involved for the strategy to succeed --- it requires\nonly that a large group of non-syndicate bettors each choose a few tickets\nindependently."}, {"title": "Does it Pay to Buy the Pot in the Canadian 6/49 Lotto? Implications for Lottery Design", "abstract": "Despite its unusual payout structure, the Canadian 6/49 Lotto is one of the\nfew government sponsored lotteries that has the potential for a favorable\nstrategy we call \"buying the pot.\" By buying the pot we mean that a syndicate\nbuys each ticket in the lottery, ensuring that it holds a jackpot winner. We\nassume that the other bettors independently buy small numbers of tickets. This\npaper presents (1) a formula for the syndicate's expected return, (2)\nconditions under which buying the pot produces a significant positive expected\nreturn, and (3) the implications of these findings for lottery design."}, {"title": "Solving Dynamic Discrete Choice Models: Integrated or Expected Value Function?", "abstract": "Dynamic Discrete Choice Models (DDCMs) are important in the structural\nestimation literature. Since the structural errors are practically always\ncontinuous and unbounded in nature, researchers often use the expected value\nfunction. The idea to solve for the expected value function made solution more\npractical and estimation feasible. However, as we show in this paper, the\nexpected value function is impractical compared to an alternative: the\nintegrated (ex ante) value function. We provide brief descriptions of the\ninefficacy of the former, and benchmarks on actual problems with varying\ncardinality of the state space and number of decisions. Though the two\napproaches solve the same problem in theory, the benchmarks support the claim\nthat the integrated value function is preferred in practice."}, {"title": "Heterogeneous structural breaks in panel data models", "abstract": "This paper develops a new model and estimation procedure for panel data that\nallows us to identify heterogeneous structural breaks. We model individual\nheterogeneity using a grouped pattern. For each group, we allow common\nstructural breaks in the coefficients. However, the number, timing, and size of\nthese breaks can differ across groups. We develop a hybrid estimation procedure\nof the grouped fixed effects approach and adaptive group fused Lasso. We show\nthat our method can consistently identify the latent group structure, detect\nstructural breaks, and estimate the regression parameters. Monte Carlo results\ndemonstrate the good performance of the proposed method in finite samples. An\nempirical application to the relationship between income and democracy\nillustrates the importance of considering heterogeneous structural breaks."}, {"title": "Characterizing Assumption of Rationality by Incomplete Information", "abstract": "We characterize common assumption of rationality of 2-person games within an\nincomplete information framework. We use the lexicographic model with\nincomplete information and show that a belief hierarchy expresses common\nassumption of rationality within a complete information framework if and only\nif there is a belief hierarchy within the corresponding incomplete information\nframework that expresses common full belief in caution, rationality, every good\nchoice is supported, and prior belief in the original utility functions."}, {"title": "Quantifying Health Shocks Over the Life Cycle", "abstract": "We first show (1) the importance of investigating health expenditure process\nusing the order two Markov chain model, rather than the standard order one\nmodel, which is widely used in the literature. Markov chain of order two is the\nminimal framework that is capable of distinguishing those who experience a\ncertain health expenditure level for the first time from those who have been\nexperiencing that or other levels for some time. In addition, using the model\nwe show (2) that the probability of encountering a health shock first de-\ncreases until around age 10, and then increases with age, particularly, after\nage 40, (3) that health shock distributions among different age groups do not\ndiffer until their percentiles reach the median range, but that above the\nmedian the health shock distributions of older age groups gradually start to\nfirst-order dominate those of younger groups, and (4) that the persistency of\nhealth shocks also shows a U-shape in relation to age."}, {"title": "Ordered Kripke Model, Permissibility, and Convergence of Probabilistic Kripke Model", "abstract": "We define a modification of the standard Kripke model, called the ordered\nKripke model, by introducing a linear order on the set of accessible states of\neach state. We first show this model can be used to describe the lexicographic\nbelief hierarchy in epistemic game theory, and perfect rationalizability can be\ncharacterized within this model. Then we show that each ordered Kripke model is\nthe limit of a sequence of standard probabilistic Kripke models with a modified\n(common) belief operator, in the senses of structure and the\n(epsilon-)permissibilities characterized within them."}, {"title": "How Can We Induce More Women to Competitions?", "abstract": "Why women avoid participating in a competition and how can we encourage them\nto participate in it? In this paper, we investigate how social image concerns\naffect women's decision to compete. We first construct a theoretical model and\nshow that participating in a competition, even under affirmative action\npolicies favoring women, is costly for women under public observability since\nit deviates from traditional female gender norms, resulting in women's low\nappearance in competitive environments. We propose and theoretically show that\nintroducing prosocial incentives in the competitive environment is effective\nand robust to public observability since (i) it induces women who are\nintrinsically motivated by prosocial incentives to the competitive environment\nand (ii) it makes participating in a competition not costly for women from\nsocial image point of view. We conduct a laboratory experiment where we\nrandomly manipulate the public observability of decisions to compete and test\nour theoretical predictions. The results of the experiment are fairly\nconsistent with our theoretical predictions. We suggest that when designing\npolicies to promote gender equality in competitive environments, using\nprosocial incentives through company philanthropy or other social\nresponsibility policies, either as substitutes or as complements to traditional\naffirmative action policies, could be promising."}, {"title": "Hyper-rational choice theory", "abstract": "The rational choice theory is based on this idea that people rationally\npursue goals for increasing their personal interests. In most conditions, the\nbehavior of an actor is not independent of the person and others' behavior.\nHere, we present a new concept of rational choice as a hyper-rational choice\nwhich in this concept, the actor thinks about profit or loss of other actors in\naddition to his personal profit or loss and then will choose an action which is\ndesirable to him. We implement the hyper-rational choice to generalize and\nexpand the game theory. Results of this study will help to model the behavior\nof people considering environmental conditions, the kind of behavior\ninteractive, valuation system of itself and others and system of beliefs and\ninternal values of societies. Hyper-rationality helps us understand how human\ndecision makers behave in interactive decisions."}, {"title": "Structural analysis with mixed-frequency data: A MIDAS-SVAR model of US capital flows", "abstract": "We develop a new VAR model for structural analysis with mixed-frequency data.\nThe MIDAS-SVAR model allows to identify structural dynamic links exploiting the\ninformation contained in variables sampled at different frequencies. It also\nprovides a general framework to test homogeneous frequency-based\nrepresentations versus mixed-frequency data models. A set of Monte Carlo\nexperiments suggests that the test performs well both in terms of size and\npower. The MIDAS-SVAR is then used to study how monetary policy and financial\nmarket volatility impact on the dynamics of gross capital inflows to the US.\nWhile no relation is found when using standard quarterly data, exploiting the\nvariability present in the series within the quarter shows that the effect of\nan interest rate shock is greater the longer the time lag between the month of\nthe shock and the end of the quarter"}, {"title": "An Experimental Investigation of Preference Misrepresentation in the Residency Match", "abstract": "The development and deployment of matching procedures that incentivize\ntruthful preference reporting is considered one of the major successes of\nmarket design research. In this study, we test the degree to which these\nprocedures succeed in eliminating preference misrepresentation. We administered\nan online experiment to 1,714 medical students immediately after their\nparticipation in the medical residency match--a leading field application of\nstrategy-proof market design. When placed in an analogous, incentivized\nmatching task, we find that 23% of participants misrepresent their preferences.\nWe explore the factors that predict preference misrepresentation, including\ncognitive ability, strategic positioning, overconfidence, expectations, advice,\nand trust. We discuss the implications of this behavior for the design of\nallocation mechanisms and the social welfare in markets that use them."}, {"title": "Prediction of Shared Bicycle Demand with Wavelet Thresholding", "abstract": "Consumers are creatures of habit, often periodic, tied to work, shopping and\nother schedules. We analyzed one month of data from the world's largest\nbike-sharing company to elicit demand behavioral cycles, initially using models\nfrom animal tracking that showed large customers fit an Ornstein-Uhlenbeck\nmodel with demand peaks at periodicities of 7, 12, 24 hour and 7-days. Lorenz\ncurves of bicycle demand showed that the majority of customer usage was\ninfrequent, and demand cycles from time-series models would strongly overfit\nthe data yielding unreliable models. Analysis of thresholded wavelets for the\nspace-time tensor of bike-sharing contracts was able to compress the data into\na 56-coefficient model with little loss of information, suggesting that\nbike-sharing demand behavior is exceptionally strong and regular. Improvements\nto predicted demand could be made by adjusting for 'noise' filtered by our\nmodel from air quality and weather information and demand from infrequent\nriders."}, {"title": "A General Method for Demand Inversion", "abstract": "This paper describes a numerical method to solve for mean product qualities\nwhich equates the real market share to the market share predicted by a discrete\nchoice model. The method covers a general class of discrete choice model,\nincluding the pure characteristics model in Berry and Pakes(2007) and the\nrandom coefficient logit model in Berry et al.(1995) (hereafter BLP). The\nmethod transforms the original market share inversion problem to an\nunconstrained convex minimization problem, so that any convex programming\nalgorithm can be used to solve the inversion. Moreover, such results also imply\nthat the computational complexity of inverting a demand model should be no more\nthan that of a convex programming problem. In simulation examples, I show the\nmethod outperforms the contraction mapping algorithm in BLP. I also find the\nmethod remains robust in pure characteristics models with near-zero market\nshares."}, {"title": "Knowledge and Unanimous Acceptance of Core Payoffs: An Epistemic Foundation for Cooperative Game Theory", "abstract": "We provide an epistemic foundation for cooperative games by proof theory via\nstudying the knowledge for players unanimously accepting only core payoffs. We\nfirst transform each cooperative game into a decision problem where a player\ncan accept or reject any payoff vector offered to her based on her knowledge\nabout available cooperation. Then we use a modified KD-system in epistemic\nlogic, which can be regarded as a counterpart of the model for non-cooperative\ngames in Bonanno (2008), (2015), to describe a player's knowledge,\ndecision-making criterion, and reasoning process; especially, a formula called\nC-acceptability is defined to capture the criterion for accepting a core payoff\nvector. Within this syntactical framework, we characterize the core of a\ncooperative game in terms of players' knowledge. Based on that result, we\ndiscuss an epistemic inconsistency behind Debreu-Scarf Theorem, that is, the\nincrease of the number of replicas has invariant requirement on each\nparticipant's knowledge from the aspect of competitive market, while requires\nunbounded epistemic ability players from the aspect of cooperative game."}, {"title": "The dynamic impact of monetary policy on regional housing prices in the US: Evidence based on factor-augmented vector autoregressions", "abstract": "In this study interest centers on regional differences in the response of\nhousing prices to monetary policy shocks in the US. We address this issue by\nanalyzing monthly home price data for metropolitan regions using a\nfactor-augmented vector autoregression (FAVAR) model. Bayesian model estimation\nis based on Gibbs sampling with Normal-Gamma shrinkage priors for the\nautoregressive coefficients and factor loadings, while monetary policy shocks\nare identified using high-frequency surprises around policy announcements as\nexternal instruments. The empirical results indicate that monetary policy\nactions typically have sizeable and significant positive effects on regional\nhousing prices, revealing differences in magnitude and duration. The largest\neffects are observed in regions located in states on both the East and West\nCoasts, notably California, Arizona and Florida."}, {"title": "On the iterated estimation of dynamic discrete choice games", "abstract": "We study the asymptotic properties of a class of estimators of the structural\nparameters in dynamic discrete choice games. We consider K-stage policy\niteration (PI) estimators, where K denotes the number of policy iterations\nemployed in the estimation. This class nests several estimators proposed in the\nliterature such as those in Aguirregabiria and Mira (2002, 2007), Pesendorfer\nand Schmidt-Dengler (2008), and Pakes et al. (2007). First, we establish that\nthe K-PML estimator is consistent and asymptotically normal for all K. This\ncomplements findings in Aguirregabiria and Mira (2007), who focus on K=1 and K\nlarge enough to induce convergence of the estimator. Furthermore, we show under\ncertain conditions that the asymptotic variance of the K-PML estimator can\nexhibit arbitrary patterns as a function of K. Second, we establish that the\nK-MD estimator is consistent and asymptotically normal for all K. For a\nspecific weight matrix, the K-MD estimator has the same asymptotic distribution\nas the K-PML estimator. Our main result provides an optimal sequence of weight\nmatrices for the K-MD estimator and shows that the optimally weighted K-MD\nestimator has an asymptotic distribution that is invariant to K. The invariance\nresult is especially unexpected given the findings in Aguirregabiria and Mira\n(2007) for K-PML estimators. Our main result implies two new corollaries about\nthe optimal 1-MD estimator (derived by Pesendorfer and Schmidt-Dengler (2008)).\nFirst, the optimal 1-MD estimator is optimal in the class of K-MD estimators.\nIn other words, additional policy iterations do not provide asymptotic\nefficiency gains relative to the optimal 1-MD estimator. Second, the optimal\n1-MD estimator is more or equally asymptotically efficient than any K-PML\nestimator for all K. Finally, the appendix provides appropriate conditions\nunder which the optimal 1-MD estimator is asymptotically efficient."}, {"title": "Kernel Estimation for Panel Data with Heterogeneous Dynamics", "abstract": "This paper proposes nonparametric kernel-smoothing estimation for panel data\nto examine the degree of heterogeneity across cross-sectional units. We first\nestimate the sample mean, autocovariances, and autocorrelations for each unit\nand then apply kernel smoothing to compute their density functions. The\ndependence of the kernel estimator on bandwidth makes asymptotic bias of very\nhigh order affect the required condition on the relative magnitudes of the\ncross-sectional sample size (N) and the time-series length (T). In particular,\nit makes the condition on N and T stronger and more complicated than those\ntypically observed in the long-panel literature without kernel smoothing. We\nalso consider a split-panel jackknife method to correct bias and construction\nof confidence intervals. An empirical application and Monte Carlo simulations\nillustrate our procedure in finite samples."}, {"title": "Identifying the occurrence or non occurrence of cognitive bias in situations resembling the Monty Hall problem", "abstract": "People reason heuristically in situations resembling inferential puzzles such\nas Bertrand's box paradox and the Monty Hall problem. The practical\nsignificance of that fact for economic decision making is uncertain because a\ndeparture from sound reasoning may, but does not necessarily, result in a\n\"cognitively biased\" outcome different from what sound reasoning would have\nproduced. Criteria are derived here, applicable to both experimental and\nnon-experimental situations, for heuristic reasoning in an inferential-puzzle\nsituations to result, or not to result, in cognitively bias. In some\nsituations, neither of these criteria is satisfied, and whether or not agents'\nposterior probability assessments or choices are cognitively biased cannot be\ndetermined."}, {"title": "On the solution of the variational optimisation in the rational inattention framework", "abstract": "I analyse the solution method for the variational optimisation problem in the\nrational inattention framework proposed by Christopher A. Sims. The solution,\nin general, does not exist, although it may exist in exceptional cases. I show\nthat the solution does not exist for the quadratic and the logarithmic\nobjective functions analysed by Sims (2003, 2006). For a linear-quadratic\nobjective function a solution can be constructed under restrictions on all but\none of its parameters. This approach is, therefore, unlikely to be applicable\nto a wider set of economic models."}, {"title": "Synthetic Control Methods and Big Data", "abstract": "Many macroeconomic policy questions may be assessed in a case study\nframework, where the time series of a treated unit is compared to a\ncounterfactual constructed from a large pool of control units. I provide a\ngeneral framework for this setting, tailored to predict the counterfactual by\nminimizing a tradeoff between underfitting (bias) and overfitting (variance).\nThe framework nests recently proposed structural and reduced form machine\nlearning approaches as special cases. Furthermore, difference-in-differences\nwith matching and the original synthetic control are restrictive cases of the\nframework, in general not minimizing the bias-variance objective. Using\nsimulation studies I find that machine learning methods outperform traditional\nmethods when the number of potential controls is large or the treated unit is\nsubstantially different from the controls. Equipped with a toolbox of\napproaches, I revisit a study on the effect of economic liberalisation on\neconomic growth. I find effects for several countries where no effect was found\nin the original study. Furthermore, I inspect how a systematically important\nbank respond to increasing capital requirements by using a large pool of banks\nto estimate the counterfactual. Finally, I assess the effect of a changing\nproduct price on product sales using a novel scanner dataset."}, {"title": "An Note on Why Geographically Weighted Regression Overcomes Multidimensional-Kernel-Based Varying-Coefficient Model", "abstract": "It is widely known that geographically weighted regression(GWR) is\nessentially same as varying-coefficient model. In the former research about\nvarying-coefficient model, scholars tend to use multidimensional-kernel-based\nlocally weighted estimation(MLWE) so that information of both distance and\ndirection is considered. However, when we construct the local weight matrix of\ngeographically weighted estimation, distance among the locations in the\nneighbor is the only factor controlling the value of entries of weight matrix.\nIn other word, estimation of GWR is distance-kernel-based. Thus, in this paper,\nunder stationary and limited dependent data with multidimensional subscripts,\nwe analyze the local mean squared properties of without any assumption of the\nform of coefficient functions and compare it with MLWE. According to the\ntheoretical and simulation results, geographically-weighted locally linear\nestimation(GWLE) is asymptotically more efficient than MLWE. Furthermore, a\nrelationship between optimal bandwith selection and design of scale parameters\nis also obtained."}, {"title": "Pricing Mechanism in Information Goods", "abstract": "We study three pricing mechanisms' performance and their effects on the\nparticipants in the data industry from the data supply chain perspective. A\nwin-win pricing strategy for the players in the data supply chain is proposed.\nWe obtain analytical solutions in each pricing mechanism, including the\ndecentralized and centralized pricing, Nash Bargaining pricing, and revenue\nsharing mechanism."}, {"title": "A Nonparametric Approach to Measure the Heterogeneous Spatial Association: Under Spatial Temporal Data", "abstract": "Spatial association and heterogeneity are two critical areas in the research\nabout spatial analysis, geography, statistics and so on. Though large amounts\nof outstanding methods has been proposed and studied, there are few of them\ntend to study spatial association under heterogeneous environment.\nAdditionally, most of the traditional methods are based on distance statistic\nand spatial weighted matrix. However, in some abstract spatial situations,\ndistance statistic can not be applied since we can not even observe the\ngeographical locations directly. Meanwhile, under these circumstances, due to\ninvisibility of spatial positions, designing of weight matrix can not\nabsolutely avoid subjectivity. In this paper, a new entropy-based method, which\nis data-driven and distribution-free, has been proposed to help us investigate\nspatial association while fully taking the fact that heterogeneity widely\nexist. Specifically, this method is not bounded with distance statistic or\nweight matrix. Asymmetrical dependence is adopted to reflect the heterogeneity\nin spatial association for each individual and the whole discussion in this\npaper is performed on spatio-temporal data with only assuming stationary\nm-dependent over time."}, {"title": "A study of strategy to the remove and ease TBT for increasing export in GCC6 countries", "abstract": "The last technical barriers to trade(TBT) between countries are Non-Tariff\nBarriers(NTBs), meaning all trade barriers are possible other than Tariff\nBarriers. And the most typical examples are (TBT), which refer to measure\nTechnical Regulation, Standards, Procedure for Conformity Assessment, Test &\nCertification etc. Therefore, in order to eliminate TBT, WTO has made all\nmembership countries automatically enter into an agreement on TBT"}, {"title": "How Smart Are `Water Smart Landscapes'?", "abstract": "Understanding the effectiveness of alternative approaches to water\nconservation is crucially important for ensuring the security and reliability\nof water services for urban residents. We analyze data from one of the\nlongest-running \"cash for grass\" policies - the Southern Nevada Water\nAuthority's Water Smart Landscapes program, where homeowners are paid to\nreplace grass with xeric landscaping. We use a twelve year long panel dataset\nof monthly water consumption records for 300,000 households in Las Vegas,\nNevada. Utilizing a panel difference-in-differences approach, we estimate the\naverage water savings per square meter of turf removed. We find that\nparticipation in this program reduced the average treated household's\nconsumption by 18 percent. We find no evidence that water savings degrade as\nthe landscape ages, or that water savings per unit area are influenced by the\nvalue of the rebate. Depending on the assumed time horizon of benefits from\nturf removal, we find that the WSL program cost the water authority about $1.62\nper thousand gallons of water saved, which compares favorably to alternative\nmeans of water conservation or supply augmentation."}, {"title": "Business Cycles in Economics", "abstract": "The business cycles are generated by the oscillating macro-/micro-/nano-\neconomic output variables in the economy of the scale and the scope in the\namplitude/frequency/phase/time domains in the economics. The accurate forward\nlooking assumptions on the business cycles oscillation dynamics can optimize\nthe financial capital investing and/or borrowing by the economic agents in the\ncapital markets. The book's main objective is to study the business cycles in\nthe economy of the scale and the scope, formulating the Ledenyov unified\nbusiness cycles theory in the Ledenyov classic and quantum econodynamics."}, {"title": "Testing for Unobserved Heterogeneous Treatment Effects with Observational Data", "abstract": "Unobserved heterogeneous treatment effects have been emphasized in the recent\npolicy evaluation literature (see e.g., Heckman and Vytlacil, 2005). This paper\nproposes a nonparametric test for unobserved heterogeneous treatment effects in\na treatment effect model with a binary treatment assignment, allowing for\nindividuals' self-selection to the treatment. Under the standard local average\ntreatment effects assumptions, i.e., the no defiers condition, we derive\ntestable model restrictions for the hypothesis of unobserved heterogeneous\ntreatment effects. Also, we show that if the treatment outcomes satisfy a\nmonotonicity assumption, these model restrictions are also sufficient. Then, we\npropose a modified Kolmogorov-Smirnov-type test which is consistent and simple\nto implement. Monte Carlo simulations show that our test performs well in\nfinite samples. For illustration, we apply our test to study heterogeneous\ntreatment effects of the Job Training Partnership Act on earnings and the\nimpacts of fertility on family income, where the null hypothesis of homogeneous\ntreatment effects gets rejected in the second case but fails to be rejected in\nthe first application."}, {"title": "Testing Continuity of a Density via g-order statistics in the Regression Discontinuity Design", "abstract": "In the regression discontinuity design (RDD), it is common practice to assess\nthe credibility of the design by testing the continuity of the density of the\nrunning variable at the cut-off, e.g., McCrary (2008). In this paper we propose\nan approximate sign test for continuity of a density at a point based on the\nso-called g-order statistics, and study its properties under two complementary\nasymptotic frameworks. In the first asymptotic framework, the number q of\nobservations local to the cut-off is fixed as the sample size n diverges to\ninfinity, while in the second framework q diverges to infinity slowly as n\ndiverges to infinity. Under both of these frameworks, we show that the test we\npropose is asymptotically valid in the sense that it has limiting rejection\nprobability under the null hypothesis not exceeding the nominal level. More\nimportantly, the test is easy to implement, asymptotically valid under weaker\nconditions than those used by competing methods, and exhibits finite sample\nvalidity under stronger conditions than those needed for its asymptotic\nvalidity. In a simulation study, we find that the approximate sign test\nprovides good control of the rejection probability under the null hypothesis\nwhile remaining competitive under the alternative hypothesis. We finally apply\nour test to the design in Lee (2008), a well-known application of the RDD to\nstudy incumbency advantage."}, {"title": "Causal Inference for Survival Analysis", "abstract": "In this paper, we propose the use of causal inference techniques for survival\nfunction estimation and prediction for subgroups of the data, upto individual\nunits. Tree ensemble methods, specifically random forests were modified for\nthis purpose. A real world healthcare dataset was used with about 1800 patients\nwith breast cancer, which has multiple patient covariates as well as disease\nfree survival days (DFS) and a death event binary indicator (y). We use the\ntype of cancer curative intervention as the treatment variable (T=0 or 1,\nbinary treatment case in our example). The algorithm is a 2 step approach. In\nstep 1, we estimate heterogeneous treatment effects using a causalTree with the\nDFS as the dependent variable. Next, in step 2, for each selected leaf of the\ncausalTree with distinctly different average treatment effect (with respect to\nsurvival), we fit a survival forest to all the patients in that leaf, one\nforest each for treatment T=0 as well as T=1 to get estimated patient level\nsurvival curves for each treatment (more generally, any model can be used at\nthis step). Then, we subtract the patient level survival curves to get the\ndifferential survival curve for a given patient, to compare the survival\nfunction as a result of the 2 treatments. The path to a selected leaf also\ngives us the combination of patient features and their values which are\ncausally important for the treatment effect difference at the leaf."}, {"title": "Two-way fixed effects estimators with heterogeneous treatment effects", "abstract": "Linear regressions with period and group fixed effects are widely used to\nestimate treatment effects. We show that they estimate weighted sums of the\naverage treatment effects (ATE) in each group and period, with weights that may\nbe negative. Due to the negative weights, the linear regression coefficient may\nfor instance be negative while all the ATEs are positive. We propose another\nestimator that solves this issue. In the two applications we revisit, it is\nsignificantly different from the linear regression estimator."}, {"title": "How does monetary policy affect income inequality in Japan? Evidence from grouped data", "abstract": "We examine the effects of monetary policy on income inequality in Japan using\na novel econometric approach that jointly estimates the Gini coefficient based\non micro-level grouped data of households and the dynamics of macroeconomic\nquantities. Our results indicate different effects on income inequality for\ndifferent types of households: A monetary tightening increases inequality when\nincome data is based on households whose head is employed (workers'\nhouseholds), while the effect reverses over the medium term when considering a\nbroader definition of households. Differences in the relative strength of the\ntransmission channels can account for this finding. Finally we demonstrate that\nthe proposed joint estimation strategy leads to more informative inference\nwhile results based on the frequently used two-step estimation approach yields\ninconclusive results."}, {"title": "Schooling Choice, Labour Market Matching, and Wages", "abstract": "We develop inference for a two-sided matching model where the characteristics\nof agents on one side of the market are endogenous due to pre-matching\ninvestments. The model can be used to measure the impact of frictions in labour\nmarkets using a single cross-section of matched employer-employee data. The\nobserved matching of workers to firms is the outcome of a discrete, two-sided\nmatching process where firms with heterogeneous preferences over education\nsequentially choose workers according to an index correlated with worker\npreferences over firms. The distribution of education arises in equilibrium\nfrom a Bayesian game: workers, knowing the distribution of worker and firm\ntypes, invest in education prior to the matching process. Although the observed\nmatching exhibits strong cross-sectional dependence due to the matching\nprocess, we propose an asymptotically valid inference procedure that combines\ndiscrete choice methods with simulation."}, {"title": "Panel Data Analysis with Heterogeneous Dynamics", "abstract": "This paper proposes a model-free approach to analyze panel data with\nheterogeneous dynamic structures across observational units. We first compute\nthe sample mean, autocovariances, and autocorrelations for each unit, and then\nestimate the parameters of interest based on their empirical distributions. We\nthen investigate the asymptotic properties of our estimators using double\nasymptotics and propose split-panel jackknife bias correction and inference\nbased on the cross-sectional bootstrap. We illustrate the usefulness of our\nprocedures by studying the deviation dynamics of the law of one price. Monte\nCarlo simulations confirm that the proposed bias correction is effective and\nyields valid inference in small samples."}, {"title": "A Bayesian panel VAR model to analyze the impact of climate change on high-income economies", "abstract": "In this paper, we assess the impact of climate shocks on futures markets for\nagricultural commodities and a set of macroeconomic quantities for multiple\nhigh-income economies. To capture relations among countries, markets, and\nclimate shocks, this paper proposes parsimonious methods to estimate\nhigh-dimensional panel VARs. We assume that coefficients associated with\ndomestic lagged endogenous variables arise from a Gaussian mixture model while\nfurther parsimony is achieved using suitable global-local shrinkage priors on\nseveral regions of the parameter space. Our results point towards pronounced\nglobal reactions of key macroeconomic quantities to climate shocks. Moreover,\nthe empirical findings highlight substantial linkages between regionally\nlocated climate shifts and global commodity markets."}, {"title": "Varying Random Coefficient Models", "abstract": "This paper provides a new methodology to analyze unobserved heterogeneity\nwhen observed characteristics are modeled nonlinearly. The proposed model\nbuilds on varying random coefficients (VRC) that are determined by nonlinear\nfunctions of observed regressors and additively separable unobservables. This\npaper proposes a novel estimator of the VRC density based on weighted sieve\nminimum distance. The main example of sieve bases are Hermite functions which\nyield a numerically stable estimation procedure. This paper shows inference\nresults that go beyond what has been shown in ordinary RC models. We provide in\neach case rates of convergence and also establish pointwise limit theory of\nlinear functionals, where a prominent example is the density of potential\noutcomes. In addition, a multiplier bootstrap procedure is proposed to\nconstruct uniform confidence bands. A Monte Carlo study examines finite sample\nproperties of the estimator and shows that it performs well even when the\nregressors associated to RC are far from being heavy tailed. Finally, the\nmethodology is applied to analyze heterogeneity in income elasticity of demand\nfor housing."}, {"title": "Inference on Local Average Treatment Effects for Misclassified Treatment", "abstract": "We develop point-identification for the local average treatment effect when\nthe binary treatment contains a measurement error. The standard instrumental\nvariable estimator is inconsistent for the parameter since the measurement\nerror is non-classical by construction. We correct the problem by identifying\nthe distribution of the measurement error based on the use of an exogenous\nvariable that can even be a binary covariate. The moment conditions derived\nfrom the identification lead to generalized method of moments estimation with\nasymptotically valid inferences. Monte Carlo simulations and an empirical\nillustration demonstrate the usefulness of the proposed procedure."}, {"title": "Shapley Value Methods for Attribution Modeling in Online Advertising", "abstract": "This paper re-examines the Shapley value methods for attribution analysis in\nthe area of online advertising. As a credit allocation solution in cooperative\ngame theory, Shapley value method directly quantifies the contribution of\nonline advertising inputs to the advertising key performance indicator (KPI)\nacross multiple channels. We simplify its calculation by developing an\nalternative mathematical formulation. The new formula significantly improves\nthe computational efficiency and therefore extends the scope of applicability.\nBased on the simplified formula, we further develop the ordered Shapley value\nmethod. The proposed method is able to take into account the order of channels\nvisited by users. We claim that it provides a more comprehensive insight by\nevaluating the attribution of channels at different stages of user conversion\njourneys. The proposed approaches are illustrated using a real-world online\nadvertising campaign dataset."}, {"title": "Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects", "abstract": "To estimate the dynamic effects of an absorbing treatment, researchers often\nuse two-way fixed effects regressions that include leads and lags of the\ntreatment. We show that in settings with variation in treatment timing across\nunits, the coefficient on a given lead or lag can be contaminated by effects\nfrom other periods, and apparent pretrends can arise solely from treatment\neffects heterogeneity. We propose an alternative estimator that is free of\ncontamination, and illustrate the relative shortcomings of two-way fixed\neffects regressions with leads and lags through an empirical application."}, {"title": "Revisiting the thermal and superthermal two-class distribution of incomes: A critical perspective", "abstract": "This paper offers a two-pronged critique of the empirical investigation of\nthe income distribution performed by physicists over the past decade. Their\nfinding rely on the graphical analysis of the observed distribution of\nnormalized incomes. Two central observations lead to the conclusion that the\nmajority of incomes are exponentially distributed, but neither each individual\npiece of evidence nor their concurrent observation robustly proves that the\nthermal and superthermal mixture fits the observed distribution of incomes\nbetter than reasonable alternatives. A formal analysis using popular measures\nof fit shows that while an exponential distribution with a power-law tail\nprovides a better fit of the IRS income data than the log-normal distribution\n(often assumed by economists), the thermal and superthermal mixture's fit can\nbe improved upon further by adding a log-normal component. The economic\nimplications of the thermal and superthermal distribution of incomes, and the\nexpanded mixture are explored in the paper."}, {"title": "Estimating Treatment Effects in Mover Designs", "abstract": "Researchers increasingly leverage movement across multiple treatments to\nestimate causal effects. While these \"mover regressions\" are often motivated by\na linear constant-effects model, it is not clear what they capture under weaker\nquasi-experimental assumptions. I show that binary treatment mover regressions\nrecover a convex average of four difference-in-difference comparisons and are\nthus causally interpretable under a standard parallel trends assumption.\nEstimates from multiple-treatment models, however, need not be causal without\nstronger restrictions on the heterogeneity of treatment effects and\ntime-varying shocks. I propose a class of two-step estimators to isolate and\ncombine the large set of difference-in-difference quasi-experiments generated\nby a mover design, identifying mover average treatment effects under\nconditional-on-covariate parallel trends and effect homogeneity restrictions. I\ncharacterize the efficient estimators in this class and derive specification\ntests based on the model's overidentifying restrictions. Future drafts will\napply the theory to the Finkelstein et al. (2016) movers design, analyzing the\ncausal effects of geography on healthcare utilization."}, {"title": "Transaction Costs in Collective Waste Recovery Systems in the EU", "abstract": "The study aims to identify the institutional flaws of the current EU waste\nmanagement model by analysing the economic model of extended producer\nresponsibility and collective waste management systems and to create a model\nfor measuring the transaction costs borne by waste recovery organizations. The\nmodel was approbated by analysing the Bulgarian collective waste management\nsystems that have been complying with the EU legislation for the last 10 years.\nThe analysis focuses on waste oils because of their economic importance and the\nlimited number of studies and analyses in this field as the predominant body of\nresearch to date has mainly addressed packaging waste, mixed household waste or\ndiscarded electrical and electronic equipment. The study aims to support the\nprocess of establishing a circular economy in the EU, which was initiated in\n2015."}, {"title": "Empirical Equilibrium", "abstract": "We study the foundations of empirical equilibrium, a refinement of Nash\nequilibrium that is based on a non-parametric characterization of empirical\ndistributions of behavior in games (Velez and Brown,2020b arXiv:1907.12408).\nThe refinement can be alternatively defined as those Nash equilibria that do\nnot refute the regular QRE theory of Goeree, Holt, and Palfrey (2005). By\ncontrast, some empirical equilibria may refute monotone additive randomly\ndisturbed payoff models. As a by product, we show that empirical equilibrium\ndoes not coincide with refinements based on approximation by monotone additive\nrandomly disturbed payoff models, and further our understanding of the\nempirical content of these models."}, {"title": "Price Competition with Geometric Brownian motion in Exchange Rate Uncertainty", "abstract": "We analyze an operational policy for a multinational manufacturer to hedge\nagainst exchange rate uncertainties and competition. We consider a single\nproduct and single period. Because of long-lead times, the capacity investment\nmust done before the selling season begins when the exchange rate between the\ntwo countries is uncertain. we consider a duopoly competition in the foreign\ncountry. We model the exchange rate as a random variable. We investigate the\nimpact of competition and exchange rate on optimal capacities and optimal\nprices. We show how competition can impact the decision of the home\nmanufacturer to enter the foreign market."}, {"title": "Statistical and Economic Evaluation of Time Series Models for Forecasting Arrivals at Call Centers", "abstract": "Call centers' managers are interested in obtaining accurate point and\ndistributional forecasts of call arrivals in order to achieve an optimal\nbalance between service quality and operating costs. We present a strategy for\nselecting forecast models of call arrivals which is based on three pillars: (i)\nflexibility of the loss function; (ii) statistical evaluation of forecast\naccuracy; (iii) economic evaluation of forecast performance using money\nmetrics. We implement fourteen time series models and seven forecast\ncombination schemes on three series of daily call arrivals. Although we focus\nmainly on point forecasts, we also analyze density forecast evaluation. We show\nthat second moments modeling is important both for point and density\nforecasting and that the simple Seasonal Random Walk model is always\noutperformed by more general specifications. Our results suggest that call\ncenter managers should invest in the use of forecast models which describe both\nfirst and second moments of call arrivals."}, {"title": "Economic inequality and Islamic Charity: An exploratory agent-based modeling approach", "abstract": "Economic inequality is one of the pivotal issues for most of economic and\nsocial policy makers across the world to insure the sustainable economic growth\nand justice. In the mainstream school of economics, namely neoclassical\ntheories, economic issues are dealt with in a mechanistic manner. Such a\nmainstream framework is majorly focused on investigating a socio-economic\nsystem based on an axiomatic scheme where reductionism approach plays a vital\nrole. The major limitations of such theories include unbounded rationality of\neconomic agents, reducing the economic aggregates to a set of predictable\nfactors and lack of attention to adaptability and the evolutionary nature of\neconomic agents. In tackling deficiencies of conventional economic models, in\nthe past two decades, some new approaches have been recruited. One of those\nnovel approaches is the Complex adaptive systems (CAS) framework which has\nshown a very promising performance in action. In contrast to mainstream school,\nunder this framework, the economic phenomena are studied in an organic manner\nwhere the economic agents are supposed to be both boundedly rational and\nadaptive. According to it, the economic aggregates emerge out of the ways\nagents of a system decide and interact. As a powerful way of modeling CASs,\nAgent-based models (ABMs) has found a growing application among academicians\nand practitioners. ABMs show that how simple behavioral rules of agents and\nlocal interactions among them at micro-scale can generate surprisingly complex\npatterns at macro-scale. In this paper, ABMs have been used to show (1) how an\neconomic inequality emerges in a system and to explain (2) how sadaqah as an\nIslamic charity rule can majorly help alleviating the inequality and how\nresource allocation strategies taken by charity entities can accelerate this\nalleviation."}, {"title": "Aide et Croissance dans les pays de l'Union Economique et Mon{é}taire Ouest Africaine (UEMOA) : retour sur une relation controvers{é}e", "abstract": "The main purpose of this paper is to analyze threshold effects of official\ndevelopment assistance (ODA) on economic growth in WAEMU zone countries. To\nachieve this, the study is based on OECD and WDI data covering the period\n1980-2015 and used Hansen's Panel Threshold Regression (PTR) model to\n\"bootstrap\" aid threshold above which its effectiveness is effective. The\nevidence strongly supports the view that the relationship between aid and\neconomic growth is non-linear with a unique threshold which is 12.74% GDP.\nAbove this value, the marginal effect of aid is 0.69 points, \"all things being\nequal to otherwise\". One of the main contribution of this paper is to show that\nWAEMU countries need investments that could be covered by the foreign aid. This\nlater one should be considered just as a complementary resource. Thus, WEAMU\ncountries should continue to strengthen their efforts in internal resource\nmobilization in order to fulfil this need."}, {"title": "Endogenous growth - A dynamic technology augmentation of the Solow model", "abstract": "In this paper, I endeavour to construct a new model, by extending the classic\nexogenous economic growth model by including a measurement which tries to\nexplain and quantify the size of technological innovation ( A ) endogenously. I\ndo not agree technology is a \"constant\" exogenous variable, because it is\nhumans who create all technological innovations, and it depends on how much\nhuman and physical capital is allocated for its research. I inspect several\npossible approaches to do this, and then I test my model both against sample\nand real world evidence data. I call this method \"dynamic\" because it tries to\nmodel the details in resource allocations between research, labor and capital,\nby affecting each other interactively. In the end, I point out which is the new\nresidual and the parts of the economic growth model which can be further\nimproved."}, {"title": "Optimal Linear Instrumental Variables Approximations", "abstract": "This paper studies the identification and estimation of the optimal linear\napproximation of a structural regression function. The parameter in the linear\napproximation is called the Optimal Linear Instrumental Variables Approximation\n(OLIVA). This paper shows that a necessary condition for standard inference on\nthe OLIVA is also sufficient for the existence of an IV estimand in a linear\nmodel. The instrument in the IV estimand is unknown and may not be identified.\nA Two-Step IV (TSIV) estimator based on Tikhonov regularization is proposed,\nwhich can be implemented by standard regression routines. We establish the\nasymptotic normality of the TSIV estimator assuming neither completeness nor\nidentification of the instrument. As an important application of our analysis,\nwe robustify the classical Hausman test for exogeneity against misspecification\nof the linear structural model. We also discuss extensions to weighted least\nsquares criteria. Monte Carlo simulations suggest an excellent finite sample\nperformance for the proposed inferences. Finally, in an empirical application\nestimating the elasticity of intertemporal substitution (EIS) with US data, we\nobtain TSIV estimates that are much larger than their standard IV counterparts,\nwith our robust Hausman test failing to reject the null hypothesis of\nexogeneity of real interest rates."}, {"title": "Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic Logit Models", "abstract": "We study the identification and estimation of structural parameters in\ndynamic panel data logit models where decisions are forward-looking and the\njoint distribution of unobserved heterogeneity and observable state variables\nis nonparametric, i.e., fixed-effects model. We consider models with two\nendogenous state variables: the lagged decision variable, and the time duration\nin the last choice. This class of models includes as particular cases important\neconomic applications such as models of market entry-exit, occupational choice,\nmachine replacement, inventory and investment decisions, or dynamic demand of\ndifferentiated products. The identification of structural parameters requires a\nsufficient statistic that controls for unobserved heterogeneity not only in\ncurrent utility but also in the continuation value of the forward-looking\ndecision problem. We obtain the minimal sufficient statistic and prove\nidentification of some structural parameters using a conditional likelihood\napproach. We apply this estimator to a machine replacement model."}, {"title": "Density Forecasts in Panel Data Models: A Semiparametric Bayesian Perspective", "abstract": "This paper constructs individual-specific density forecasts for a panel of\nfirms or households using a dynamic linear model with common and heterogeneous\ncoefficients as well as cross-sectional heteroskedasticity. The panel\nconsidered in this paper features a large cross-sectional dimension N but short\ntime series T. Due to the short T, traditional methods have difficulty in\ndisentangling the heterogeneous parameters from the shocks, which contaminates\nthe estimates of the heterogeneous parameters. To tackle this problem, I assume\nthat there is an underlying distribution of heterogeneous parameters, model\nthis distribution nonparametrically allowing for correlation between\nheterogeneous parameters and initial conditions as well as individual-specific\nregressors, and then estimate this distribution by combining information from\nthe whole panel. Theoretically, I prove that in cross-sectional homoskedastic\ncases, both the estimated common parameters and the estimated distribution of\nthe heterogeneous parameters achieve posterior consistency, and that the\ndensity forecasts asymptotically converge to the oracle forecast.\nMethodologically, I develop a simulation-based posterior sampling algorithm\nspecifically addressing the nonparametric density estimation of unobserved\nheterogeneous parameters. Monte Carlo simulations and an empirical application\nto young firm dynamics demonstrate improvements in density forecasts relative\nto alternative approaches."}, {"title": "Efficiency in Micro-Behaviors and FL Bias", "abstract": "In this paper, we propose a model which simulates odds distributions of\npari-mutuel betting system under two hypotheses on the behavior of bettors: 1.\nThe amount of bets increases very rapidly as the deadline for betting comes\nnear. 2. Each bettor bets on a horse which gives the largest expectation value\nof the benefit. The results can be interpreted as such efficient behaviors do\nnot serve to extinguish the FL bias but even produce stronger FL bias."}, {"title": "The Finite Sample Performance of Treatment Effects Estimators based on the Lasso", "abstract": "This paper contributes to the literature on treatment effects estimation with\nmachine learning inspired methods by studying the performance of different\nestimators based on the Lasso. Building on recent work in the field of\nhigh-dimensional statistics, we use the semiparametric efficient score\nestimation structure to compare different estimators. Alternative weighting\nschemes are considered and their suitability for the incorporation of machine\nlearning estimators is assessed using theoretical arguments and various Monte\nCarlo experiments. Additionally we propose an own estimator based on doubly\nrobust Kernel matching that is argued to be more robust to nuisance parameter\nmisspecification. In the simulation study we verify theory based intuition and\nfind good finite sample properties of alternative weighting scheme estimators\nlike the one we propose."}, {"title": "Data-Driven Investment Decision-Making: Applying Moore's Law and S-Curves to Business Strategies", "abstract": "This paper introduces a method for linking technological improvement rates\n(i.e. Moore's Law) and technology adoption curves (i.e. S-Curves). There has\nbeen considerable research surrounding Moore's Law and the generalized versions\napplied to the time dependence of performance for other technologies. The prior\nwork has culminated with methodology for quantitative estimation of\ntechnological improvement rates for nearly any technology. This paper examines\nthe implications of such regular time dependence for performance upon the\ntiming of key events in the technological adoption process. We propose a simple\ncrossover point in performance which is based upon the technological\nimprovement rates and current level differences for target and replacement\ntechnologies. The timing for the cross-over is hypothesized as corresponding to\nthe first 'knee'? in the technology adoption \"S-curve\" and signals when the\nmarket for a given technology will start to be rewarding for innovators. This\nis also when potential entrants are likely to intensely experiment with\nproduct-market fit and when the competition to achieve a dominant design\nbegins. This conceptual framework is then back-tested by examining two\ntechnological changes brought about by the internet, namely music and video\ntransmission. The uncertainty analysis around the cases highlight opportunities\nfor organizations to reduce future technological uncertainty. Overall, the\nresults from the case studies support the reliability and utility of the\nconceptual framework in strategic business decision-making with the caveat that\nwhile technical uncertainty is reduced, it is not eliminated."}, {"title": "Happy family of stable marriages", "abstract": "Some aspects of the problem of stable marriage are discussed. There are two\ndistinguished marriage plans: the fully transferable case, where money can be\ntransferred between the participants, and the fully non transferable case where\neach participant has its own rigid preference list regarding the other gender.\nWe continue to discuss intermediate partial transferable cases. Partial\ntransferable plans can be approached as either special cases of cooperative\ngames using the notion of a core, or as a generalization of the cyclical\nmonotonicity property of the fully transferable case (fake promises). We shall\nintroduced these two approaches, and prove the existence of stable marriage for\nthe fully transferable and non-transferable plans."}, {"title": "Bitcoin price and its marginal cost of production: support for a fundamental value", "abstract": "This study back-tests a marginal cost of production model proposed to value\nthe digital currency bitcoin. Results from both conventional regression and\nvector autoregression (VAR) models show that the marginal cost of production\nplays an important role in explaining bitcoin prices, challenging recent\nallegations that bitcoins are essentially worthless. Even with markets pricing\nbitcoin in the thousands of dollars each, the valuation model seems robust. The\ndata show that a price bubble that began in the Fall of 2017 resolved itself in\nearly 2018, converging with the marginal cost model. This suggests that while\nbubbles may appear in the bitcoin market, prices will tend to this bound and\nnot collapse to zero."}, {"title": "Model Selection in Time Series Analysis: Using Information Criteria as an Alternative to Hypothesis Testing", "abstract": "The issue of model selection in applied research is of vital importance.\nSince the true model in such research is not known, which model should be used\nfrom among various potential ones is an empirical question. There might exist\nseveral competitive models. A typical approach to dealing with this is classic\nhypothesis testing using an arbitrarily chosen significance level based on the\nunderlying assumption that a true null hypothesis exists. In this paper we\ninvestigate how successful this approach is in determining the correct model\nfor different data generating processes using time series data. An alternative\napproach based on more formal model selection techniques using an information\ncriterion or cross-validation is suggested and evaluated in the time series\nenvironment via Monte Carlo experiments. This paper also explores the\neffectiveness of deciding what type of general relation exists between two\nvariables (e.g. relation in levels or relation in first differences) using\nvarious strategies based on hypothesis testing and on information criteria with\nthe presence or absence of unit roots."}, {"title": "A Double Machine Learning Approach to Estimate the Effects of Musical Practice on Student's Skills", "abstract": "This study investigates the dose-response effects of making music on youth\ndevelopment. Identification is based on the conditional independence assumption\nand estimation is implemented using a recent double machine learning estimator.\nThe study proposes solutions to two highly practically relevant questions that\narise for these new methods: (i) How to investigate sensitivity of estimates to\ntuning parameter choices in the machine learning part? (ii) How to assess\ncovariate balancing in high-dimensional settings? The results show that\nimprovements in objectively measured cognitive skills require at least medium\nintensity, while improvements in school grades are already observed for low\nintensity of practice."}, {"title": "Flexible shrinkage in high-dimensional Bayesian spatial autoregressive models", "abstract": "This article introduces two absolutely continuous global-local shrinkage\npriors to enable stochastic variable selection in the context of\nhigh-dimensional matrix exponential spatial specifications. Existing approaches\nas a means to dealing with overparameterization problems in spatial\nautoregressive specifications typically rely on computationally demanding\nBayesian model-averaging techniques. The proposed shrinkage priors can be\nimplemented using Markov chain Monte Carlo methods in a flexible and efficient\nway. A simulation study is conducted to evaluate the performance of each of the\nshrinkage priors. Results suggest that they perform particularly well in\nhigh-dimensional environments, especially when the number of parameters to\nestimate exceeds the number of observations. For an empirical illustration we\nuse pan-European regional economic growth data."}, {"title": "Tilting Approximate Models", "abstract": "Model approximations are common practice when estimating structural or\nquasi-structural models. The paper considers the econometric properties of\nestimators that utilize projections to reimpose information about the exact\nmodel in the form of conditional moments. The resulting estimator efficiently\ncombines the information provided by the approximate law of motion and the\nmoment conditions. The paper develops the corresponding asymptotic theory and\nprovides simulation evidence that tilting substantially reduces the mean\nsquared error for parameter estimates. It applies the methodology to pricing\nlong-run risks in aggregate consumption in the US, whereas the model is solved\nusing the Campbell and Shiller (1988) approximation. Tilting improves empirical\nfit and results suggest that approximation error is a source of upward bias in\nestimates of risk aversion and downward bias in the elasticity of intertemporal\nsubstitution."}, {"title": "Modeling the residential electricity consumption within a restructured power market", "abstract": "The United States' power market is featured by the lack of judicial power at\nthe federal level. The market thus provides a unique testing environment for\nthe market organization structure. At the same time, the econometric modeling\nand forecasting of electricity market consumption become more challenging.\nImport and export, which generally follow simple rules in European countries,\ncan be a result of direct market behaviors. This paper seeks to build a general\nmodel for power consumption and using the model to test several hypotheses."}, {"title": "Estimation and Inference for Policy Relevant Treatment Effects", "abstract": "The policy relevant treatment effect (PRTE) measures the average effect of\nswitching from a status-quo policy to a counterfactual policy. Estimation of\nthe PRTE involves estimation of multiple preliminary parameters, including\npropensity scores, conditional expectation functions of the outcome and\ncovariates given the propensity score, and marginal treatment effects. These\npreliminary estimators can affect the asymptotic distribution of the PRTE\nestimator in complicated and intractable manners. In this light, we propose an\northogonal score for double debiased estimation of the PRTE, whereby the\nasymptotic distribution of the PRTE estimator is obtained without any influence\nof preliminary parameter estimators as far as they satisfy mild requirements of\nconvergence rates. To our knowledge, this paper is the first to develop limit\ndistribution theories for inference about the PRTE."}, {"title": "Ill-posed Estimation in High-Dimensional Models with Instrumental Variables", "abstract": "This paper is concerned with inference about low-dimensional components of a\nhigh-dimensional parameter vector $\\beta^0$ which is identified through\ninstrumental variables. We allow for eigenvalues of the expected outer product\nof included and excluded covariates, denoted by $M$, to shrink to zero as the\nsample size increases. We propose a novel estimator based on desparsification\nof an instrumental variable Lasso estimator, which is a regularized version of\n2SLS with an additional correction term. This estimator converges to $\\beta^0$\nat a rate depending on the mapping properties of $M$ captured by a sparse link\ncondition. Linear combinations of our estimator of $\\beta^0$ are shown to be\nasymptotically normally distributed. Based on consistent covariance estimation,\nour method allows for constructing confidence intervals and statistical tests\nfor single or low-dimensional components of $\\beta^0$. In Monte-Carlo\nsimulations we analyze the finite sample behavior of our estimator."}, {"title": "Asymptotic Refinements of a Misspecification-Robust Bootstrap for Generalized Empirical Likelihood Estimators", "abstract": "I propose a nonparametric iid bootstrap procedure for the empirical\nlikelihood, the exponential tilting, and the exponentially tilted empirical\nlikelihood estimators that achieves asymptotic refinements for t tests and\nconfidence intervals, and Wald tests and confidence regions based on such\nestimators. Furthermore, the proposed bootstrap is robust to model\nmisspecification, i.e., it achieves asymptotic refinements regardless of\nwhether the assumed moment condition model is correctly specified or not. This\nresult is new, because asymptotic refinements of the bootstrap based on these\nestimators have not been established in the literature even under correct model\nspecification. Monte Carlo experiments are conducted in dynamic panel data\nsetting to support the theoretical finding. As an application, bootstrap\nconfidence intervals for the returns to schooling of Hellerstein and Imbens\n(1999) are calculated. The result suggests that the returns to schooling may be\nhigher."}, {"title": "Quasi-Experimental Shift-Share Research Designs", "abstract": "Many studies use shift-share (or ``Bartik'') instruments, which average a set\nof shocks with exposure share weights. We provide a new econometric framework\nfor shift-share instrumental variable (SSIV) regressions in which\nidentification follows from the quasi-random assignment of shocks, while\nexposure shares are allowed to be endogenous. The framework is motivated by an\nequivalence result: the orthogonality between a shift-share instrument and an\nunobserved residual can be represented as the orthogonality between the\nunderlying shocks and a shock-level unobservable. SSIV regression coefficients\ncan similarly be obtained from an equivalent shock-level regression, motivating\nshock-level conditions for their consistency. We discuss and illustrate several\npractical insights of this framework in the setting of Autor et al. (2013),\nestimating the effect of Chinese import competition on manufacturing employment\nacross U.S. commuting zones."}, {"title": "The Impact of Supervision and Incentive Process in Explaining Wage Profile and Variance", "abstract": "The implementation of a supervision and incentive process for identical\nworkers may lead to wage variance that stems from employer and employee\noptimization. The harder it is to assess the nature of the labor output, the\nmore important such a process becomes, and the influence of such a process on\nwage development growth. The dynamic model presented in this paper shows that\nan employer will choose to pay a worker a starting wage that is less than what\nhe deserves, resulting in a wage profile that fits the classic profile in the\nhuman-capital literature. The wage profile and wage variance rise at times of\ntechnological advancements, which leads to increased turnover as older workers\nare replaced by younger workers due to a rise in the relative marginal cost of\nthe former."}, {"title": "Asymptotic Refinements of a Misspecification-Robust Bootstrap for Generalized Method of Moments Estimators", "abstract": "I propose a nonparametric iid bootstrap that achieves asymptotic refinements\nfor t tests and confidence intervals based on GMM estimators even when the\nmodel is misspecified. In addition, my bootstrap does not require recentering\nthe moment function, which has been considered as critical for GMM. Regardless\nof model misspecification, the proposed bootstrap achieves the same sharp\nmagnitude of refinements as the conventional bootstrap methods which establish\nasymptotic refinements by recentering in the absence of misspecification. The\nkey idea is to link the misspecified bootstrap moment condition to the large\nsample theory of GMM under misspecification of Hall and Inoue (2003). Two\nexamples are provided: Combining data sets and invalid instrumental variables."}, {"title": "A Consistent Variance Estimator for 2SLS When Instruments Identify Different LATEs", "abstract": "Under treatment effect heterogeneity, an instrument identifies the\ninstrument-specific local average treatment effect (LATE). With multiple\ninstruments, two-stage least squares (2SLS) estimand is a weighted average of\ndifferent LATEs. What is often overlooked in the literature is that the\npostulated moment condition evaluated at the 2SLS estimand does not hold unless\nthose LATEs are the same. If so, the conventional heteroskedasticity-robust\nvariance estimator would be inconsistent, and 2SLS standard errors based on\nsuch estimators would be incorrect. I derive the correct asymptotic\ndistribution, and propose a consistent asymptotic variance estimator by using\nthe result of Hall and Inoue (2003, Journal of Econometrics) on misspecified\nmoment condition models. This can be used to correctly calculate the standard\nerrors regardless of whether there is more than one LATE or not."}, {"title": "Leave-out estimation of variance components", "abstract": "We propose leave-out estimators of quadratic forms designed for the study of\nlinear models with unrestricted heteroscedasticity. Applications include\nanalysis of variance and tests of linear restrictions in models with many\nregressors. An approximation algorithm is provided that enables accurate\ncomputation of the estimator in very large datasets. We study the large sample\nproperties of our estimator allowing the number of regressors to grow in\nproportion to the number of observations. Consistency is established in a\nvariety of settings where plug-in methods and estimators predicated on\nhomoscedasticity exhibit first-order biases. For quadratic forms of increasing\nrank, the limiting distribution can be represented by a linear combination of\nnormal and non-central $\\chi^2$ random variables, with normality ensuing under\nstrong identification. Standard error estimators are proposed that enable tests\nof linear restrictions and the construction of uniformly valid confidence\nintervals for quadratic forms of interest. We find in Italian social security\nrecords that leave-out estimates of a variance decomposition in a two-way fixed\neffects model of wage determination yield substantially different conclusions\nregarding the relative contribution of workers, firms, and worker-firm sorting\nto wage inequality than conventional methods. Monte Carlo exercises corroborate\nthe accuracy of our asymptotic approximations, with clear evidence of\nnon-normality emerging when worker mobility between blocks of firms is limited."}, {"title": "A Quantitative Analysis of Possible Futures of Autonomous Transport", "abstract": "Autonomous ships (AS) used for cargo transport have gained a considerable\namount of attention in recent years. They promise benefits such as reduced crew\ncosts, increased safety and increased flexibility. This paper explores the\neffects of a faster increase in technological performance in maritime shipping\nachieved by leveraging fast-improving technological domains such as computer\nprocessors, and advanced energy storage. Based on historical improvement rates\nof several modes of transport (Cargo Ships, Air, Rail, Trucking) a simplified\nMarkov-chain Monte-Carlo (MCMC) simulation of an intermodal transport model\n(IMTM) is used to explore the effects of differing technological improvement\nrates for AS. The results show that the annual improvement rates of traditional\nshipping (Ocean Cargo Ships = 2.6%, Air Cargo = 5.5%, Trucking = 0.6%, Rail =\n1.9%, Inland Water Transport = 0.4%) improve at lower rates than technologies\nassociated with automation such as Computer Processors (35.6%), Fuel Cells\n(14.7%) and Automotive Autonomous Hardware (27.9%). The IMTM simulations up to\nthe year 2050 show that the introduction of any mode of autonomous transport\nwill increase competition in lower cost shipping options, but is unlikely to\nsignificantly alter the overall distribution of transport mode costs. Secondly,\nif all forms of transport end up converting to autonomous systems, then the\nuncertainty surrounding the improvement rates yields a complex intermodal\ntransport solution involving several options, all at a much lower cost over\ntime. Ultimately, the research shows a need for more accurate measurement of\ncurrent autonomous transport costs and how they are changing over time."}, {"title": "A Growth Model with Unemployment", "abstract": "A standard growth model is modified in a straightforward way to incorporate\nwhat Keynes (1936) suggests in the \"essence\" of his general theory. The\ntheoretical essence is the idea that exogenous changes in investment cause\nchanges in employment and unemployment. We implement this idea by assuming the\npath for capital growth rate is exogenous in the growth model. The result is a\ngrowth model that can explain both long term trends and fluctuations around the\ntrend. The modified growth model was tested using the U.S. economic data from\n1947 to 2014. The hypothesized inverse relationship between the capital growth\nand changes in unemployment was confirmed, and the structurally estimated model\nfits fluctuations in unemployment reasonably well."}, {"title": "The Role of Agricultural Sector Productivity in Economic Growth: The Case of Iran's Economic Development Plan", "abstract": "This study provides the theoretical framework and empirical model for\nproductivity growth evaluations in agricultural sector as one of the most\nimportant sectors in Iran's economic development plan. We use the Solow\nresidual model to measure the productivity growth share in the value-added\ngrowth of the agricultural sector. Our time series data includes value-added\nper worker, employment, and capital in this sector. The results show that the\naverage total factor productivity growth rate in the agricultural sector is\n-0.72% during 1991-2010. Also, during this period, the share of total factor\nproductivity growth in the value-added growth is -19.6%, while it has been\nforecasted to be 33.8% in the fourth development plan. Considering the\neffective role of capital in the agricultural low productivity, we suggest\napplying productivity management plans (especially in regards of capital\nproductivity) to achieve future growth goals."}, {"title": "Estimating Trade-Related Adjustment Costs in the Agricultural Sector in Iran", "abstract": "Tariff liberalization and its impact on tax revenue is an important\nconsideration for developing countries, because they are increasingly facing\nthe difficult task of implementing and harmonizing regional and international\ntrade commitments. The tariff reform and its costs for Iranian government is\none of the issues that are examined in this study. Another goal of this paper\nis, estimating the cost of trade liberalization. On this regard, imports value\nof agricultural sector in Iran in 2010 was analyzed according to two scenarios.\nFor reforming nuisance tariff, a VAT policy is used in both scenarios. In this\nstudy, TRIST method is used. In the first scenario, imports' value decreased to\na level equal to the second scenario and higher tariff revenue will be created.\nThe results show that reducing the average tariff rate does not always result\nin the loss of tariff revenue. This paper is a witness that different forms of\ntariff can generate different amount of income when they have same level of\nliberalization and equal effect on producers. Therefore, using a good tariff\nregime can help a government to generate income when increases social welfare\nby liberalization."}, {"title": "On the relation between Sion's minimax theorem and existence of Nash equilibrium in asymmetric multi-players zero-sum game with only one alien", "abstract": "We consider the relation between Sion's minimax theorem for a continuous\nfunction and a Nash equilibrium in an asymmetric multi-players zero-sum game in\nwhich only one player is different from other players, and the game is\nsymmetric for the other players. Then,\n  1. The existence of a Nash equilibrium, which is symmetric for players other\nthan one player, implies Sion's minimax theorem for pairs of this player and\none of other players with symmetry for the other players.\n  2. Sion's minimax theorem for pairs of one player and one of other players\nwith symmetry for the other players implies the existence of a Nash equilibrium\nwhich is symmetric for the other players.\n  Thus, they are equivalent."}, {"title": "Cluster-Robust Standard Errors for Linear Regression Models with Many Controls", "abstract": "It is common practice in empirical work to employ cluster-robust standard\nerrors when using the linear regression model to estimate some\nstructural/causal effect of interest. Researchers also often include a large\nset of regressors in their model specification in order to control for observed\nand unobserved confounders. In this paper we develop inference methods for\nlinear regression models with many controls and clustering. We show that\ninference based on the usual cluster-robust standard errors by Liang and Zeger\n(1986) is invalid in general when the number of controls is a non-vanishing\nfraction of the sample size. We then propose a new clustered standard errors\nformula that is robust to the inclusion of many controls and allows to carry\nout valid inference in a variety of high-dimensional linear regression models,\nincluding fixed effects panel data models and the semiparametric partially\nlinear model. Monte Carlo evidence supports our theoretical results and shows\nthat our proposed variance estimator performs well in finite samples. The\nproposed method is also illustrated with an empirical application that\nre-visits Donohue III and Levitt's (2001) study of the impact of abortion on\ncrime."}, {"title": "Shift-Share Designs: Theory and Inference", "abstract": "We study inference in shift-share regression designs, such as when a regional\noutcome is regressed on a weighted average of sectoral shocks, using regional\nsector shares as weights. We conduct a placebo exercise in which we estimate\nthe effect of a shift-share regressor constructed with randomly generated\nsectoral shocks on actual labor market outcomes across U.S. Commuting Zones.\nTests based on commonly used standard errors with 5\\% nominal significance\nlevel reject the null of no effect in up to 55\\% of the placebo samples. We use\na stylized economic model to show that this overrejection problem arises\nbecause regression residuals are correlated across regions with similar\nsectoral shares, independently of their geographic location. We derive novel\ninference methods that are valid under arbitrary cross-regional correlation in\nthe regression residuals. We show using popular applications of shift-share\ndesigns that our methods may lead to substantially wider confidence intervals\nin practice."}, {"title": "The transmission of uncertainty shocks on income inequality: State-level evidence from the United States", "abstract": "In this paper, we explore the relationship between state-level household\nincome inequality and macroeconomic uncertainty in the United States. Using a\nnovel large-scale macroeconometric model, we shed light on regional disparities\nof inequality responses to a national uncertainty shock. The results suggest\nthat income inequality decreases in most states, with a pronounced degree of\nheterogeneity in terms of shapes and magnitudes of the dynamic responses. By\ncontrast, some few states, mostly located in the West and South census region,\ndisplay increasing levels of income inequality over time. We find that this\ndirectional pattern in responses is mainly driven by the income composition and\nlabor market fundamentals. In addition, forecast error variance decompositions\nallow for a quantitative assessment of the importance of uncertainty shocks in\nexplaining income inequality. The findings highlight that volatility shocks\naccount for a considerable fraction of forecast error variance for most states\nconsidered. Finally, a regression-based analysis sheds light on the driving\nforces behind differences in state-specific inequality responses."}, {"title": "Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots", "abstract": "We propose a new class of unit root tests that exploits invariance properties\nin the Locally Asymptotically Brownian Functional limit experiment associated\nto the unit root model. The invariance structures naturally suggest tests that\nare based on the ranks of the increments of the observations, their average,\nand an assumed reference density for the innovations. The tests are\nsemiparametric in the sense that they are valid, i.e., have the correct\n(asymptotic) size, irrespective of the true innovation density. For a correctly\nspecified reference density, our test is point-optimal and nearly efficient.\nFor arbitrary reference densities, we establish a Chernoff-Savage type result,\ni.e., our test performs as well as commonly used tests under Gaussian\ninnovations but has improved power under other, e.g., fat-tailed or skewed,\ninnovation distributions. To avoid nonparametric estimation, we propose a\nsimplified version of our test that exhibits the same asymptotic properties,\nexcept for the Chernoff-Savage result that we are only able to demonstrate by\nmeans of simulations."}, {"title": "Point-identification in multivariate nonseparable triangular models", "abstract": "In this article we introduce a general nonparametric point-identification\nresult for nonseparable triangular models with a multivariate first- and second\nstage. Based on this we prove point-identification of Hedonic models with\nmultivariate heterogeneity and endogenous observable characteristics, extending\nand complementing identification results from the literature which all require\nexogeneity. As an additional application of our theoretical result, we show\nthat the BLP model (Berry et al. 1995) can also be identified without index\nrestrictions."}, {"title": "Machine Learning for Dynamic Discrete Choice", "abstract": "Dynamic discrete choice models often discretize the state vector and restrict\nits dimension in order to achieve valid inference. I propose a novel two-stage\nestimator for the set-identified structural parameter that incorporates a\nhigh-dimensional state space into the dynamic model of imperfect competition.\nIn the first stage, I estimate the state variable's law of motion and the\nequilibrium policy function using machine learning tools. In the second stage,\nI plug the first-stage estimates into a moment inequality and solve for the\nstructural parameter. The moment function is presented as the sum of two\ncomponents, where the first one expresses the equilibrium assumption and the\nsecond one is a bias correction term that makes the sum insensitive (i.e.,\northogonal) to first-stage bias. The proposed estimator uniformly converges at\nthe root-N rate and I use it to construct confidence regions. The results\ndeveloped here can be used to incorporate high-dimensional state space into\nclassic dynamic discrete choice models, for example, those considered in Rust\n(1987), Bajari et al. (2007), and Scott (2013)."}, {"title": "A Unified Framework for Efficient Estimation of General Treatment Models", "abstract": "This paper presents a weighted optimization framework that unifies the\nbinary,multi-valued, continuous, as well as mixture of discrete and continuous\ntreatment, under the unconfounded treatment assignment. With a general loss\nfunction, the framework includes the average, quantile and asymmetric least\nsquares causal effect of treatment as special cases. For this general\nframework, we first derive the semiparametric efficiency bound for the causal\neffect of treatment, extending the existing bound results to a wider class of\nmodels. We then propose a generalized optimization estimation for the causal\neffect with weights estimated by solving an expanding set of equations. Under\nsome sufficient conditions, we establish consistency and asymptotic normality\nof the proposed estimator of the causal effect and show that the estimator\nattains our semiparametric efficiency bound, thereby extending the existing\nliterature on efficient estimation of causal effect to a wider class of\napplications. Finally, we discuss etimation of some causal effect functionals\nsuch as the treatment effect curve and the average outcome. To evaluate the\nfinite sample performance of the proposed procedure, we conduct a small scale\nsimulation study and find that the proposed estimation has practical value. To\nillustrate the applicability of the procedure, we revisit the literature on\ncampaign advertise and campaign contributions. Unlike the existing procedures\nwhich produce mixed results, we find no evidence of campaign advertise on\ncampaign contribution."}, {"title": "Can GDP measurement be further improved? Data revision and reconciliation", "abstract": "Recent years have seen many attempts to combine expenditure-side estimates of\nU.S. real output (GDE) growth with income-side estimates (GDI) to improve\nestimates of real GDP growth. We show how to incorporate information from\nmultiple releases of noisy data to provide more precise estimates while\navoiding some of the identifying assumptions required in earlier work. This\nrelies on a new insight: using multiple data releases allows us to distinguish\nnews and noise measurement errors in situations where a single vintage does\nnot.\n  Our new measure, GDP++, fits the data better than GDP+, the GDP growth\nmeasure of Aruoba et al. (2016) published by the Federal Reserve Bank of\nPhiladephia. Historical decompositions show that GDE releases are more\ninformative than GDI, while the use of multiple data releases is particularly\nimportant in the quarters leading up to the Great Recession."}, {"title": "When Do Households Invest in Solar Photovoltaics? An Application of Prospect Theory", "abstract": "While investments in renewable energy sources (RES) are incentivized around\nthe world, the policy tools that do so are still poorly understood, leading to\ncostly misadjustments in many cases. As a case study, the deployment dynamics\nof residential solar photovoltaics (PV) invoked by the German feed-in tariff\nlegislation are investigated. Here we report a model showing that the question\nof when people invest in residential PV systems is found to be not only\ndetermined by profitability, but also by profitability's change compared to the\nstatus quo. This finding is interpreted in the light of loss aversion, a\nconcept developed in Kahneman and Tversky's Prospect Theory. The model is able\nto reproduce most of the dynamics of the uptake with only a few financial and\nbehavioral assumptions"}, {"title": "Estimation in a Generalization of Bivariate Probit Models with Dummy Endogenous Regressors", "abstract": "The purpose of this paper is to provide guidelines for empirical researchers\nwho use a class of bivariate threshold crossing models with dummy endogenous\nvariables. A common practice employed by the researchers is the specification\nof the joint distribution of the unobservables as a bivariate normal\ndistribution, which results in a bivariate probit model. To address the problem\nof misspecification in this practice, we propose an easy-to-implement\nsemiparametric estimation framework with parametric copula and nonparametric\nmarginal distributions. We establish asymptotic theory, including root-n\nnormality, for the sieve maximum likelihood estimators that can be used to\nconduct inference on the individual structural parameters and the average\ntreatment effect (ATE). In order to show the practical relevance of the\nproposed framework, we conduct a sensitivity analysis via extensive Monte Carlo\nsimulation exercises. The results suggest that the estimates of the parameters,\nespecially the ATE, are sensitive to parametric specification, while\nsemiparametric estimation exhibits robustness to underlying data generating\nprocesses. We then provide an empirical illustration where we estimate the\neffect of health insurance on doctor visits. In this paper, we also show that\nthe absence of excluded instruments may result in identification failure, in\ncontrast to what some practitioners believe."}, {"title": "Quantifying the Computational Advantage of Forward Orthogonal Deviations", "abstract": "Under suitable conditions, one-step generalized method of moments (GMM) based\non the first-difference (FD) transformation is numerically equal to one-step\nGMM based on the forward orthogonal deviations (FOD) transformation. However,\nwhen the number of time periods ($T$) is not small, the FOD transformation\nrequires less computational work. This paper shows that the computational\ncomplexity of the FD and FOD transformations increases with the number of\nindividuals ($N$) linearly, but the computational complexity of the FOD\ntransformation increases with $T$ at the rate $T^{4}$ increases, while the\ncomputational complexity of the FD transformation increases at the rate $T^{6}$\nincreases. Simulations illustrate that calculations exploiting the FOD\ntransformation are performed orders of magnitude faster than those using the FD\ntransformation. The results in the paper indicate that, when one-step GMM based\non the FD and FOD transformations are the same, Monte Carlo experiments can be\nconducted much faster if the FOD version of the estimator is used."}, {"title": "Tests for price indices in a dynamic item universe", "abstract": "There is generally a need to deal with quality change and new goods in the\nconsumer price index due to the underlying dynamic item universe. Traditionally\naxiomatic tests are defined for a fixed universe. We propose five tests\nexplicitly formulated for a dynamic item universe, and motivate them both from\nthe perspectives of a cost-of-goods index and a cost-of-living index. None of\nthe indices satisfies all the tests at the same time, which are currently\navailable for making use of scanner data that comprises the whole item\nuniverse. The set of tests provides a rigorous diagnostic for whether an index\nis completely appropriate in a dynamic item universe, as well as pointing\ntowards the directions of possible remedies. We thus outline a large index\nfamily that potentially can satisfy all the tests."}, {"title": "A Residual Bootstrap for Conditional Value-at-Risk", "abstract": "A fixed-design residual bootstrap method is proposed for the two-step\nestimator of Francq and Zako\\\"ian (2015) associated with the conditional\nValue-at-Risk. The bootstrap's consistency is proven for a general class of\nvolatility models and intervals are constructed for the conditional\nValue-at-Risk. A simulation study reveals that the equal-tailed percentile\nbootstrap interval tends to fall short of its nominal value. In contrast, the\nreversed-tails bootstrap interval yields accurate coverage. We also compare the\ntheoretically analyzed fixed-design bootstrap with the recursive-design\nbootstrap. It turns out that the fixed-design bootstrap performs equally well\nin terms of average coverage, yet leads on average to shorter intervals in\nsmaller samples. An empirical application illustrates the interval estimation."}, {"title": "Inference based on Kotlarski's Identity", "abstract": "Kotlarski's identity has been widely used in applied economic research.\nHowever, how to conduct inference based on this popular identification approach\nhas been an open question for two decades. This paper addresses this open\nproblem by constructing a novel confidence band for the density function of a\nlatent variable in repeated measurement error model. The confidence band builds\non our finding that we can rewrite Kotlarski's identity as a system of linear\nmoment restrictions. The confidence band controls the asymptotic size uniformly\nover a class of data generating processes, and it is consistent against all\nfixed alternatives. Simulation studies support our theoretical results."}, {"title": "Efficient Difference-in-Differences Estimation with High-Dimensional Common Trend Confounding", "abstract": "This study considers various semiparametric difference-in-differences models\nunder different assumptions on the relation between the treatment group\nidentifier, time and covariates for cross-sectional and panel data. The\nvariance lower bound is shown to be sensitive to the model assumptions imposed\nimplying a robustness-efficiency trade-off. The obtained efficient influence\nfunctions lead to estimators that are rate double robust and have desirable\nasymptotic properties under weak first stage convergence conditions. This\nenables to use sophisticated machine-learning algorithms that can cope with\nsettings where common trend confounding is high-dimensional. The usefulness of\nthe proposed estimators is assessed in an empirical example. It is shown that\nthe efficiency-robustness trade-offs and the choice of first stage predictors\ncan lead to divergent empirical results in practice."}, {"title": "Bootstrap Methods in Econometrics", "abstract": "The bootstrap is a method for estimating the distribution of an estimator or\ntest statistic by re-sampling the data or a model estimated from the data.\nUnder conditions that hold in a wide variety of econometric applications, the\nbootstrap provides approximations to distributions of statistics, coverage\nprobabilities of confidence intervals, and rejection probabilities of\nhypothesis tests that are more accurate than the approximations of first-order\nasymptotic distribution theory. The reductions in the differences between true\nand nominal coverage or rejection probabilities can be very large. In addition,\nthe bootstrap provides a way to carry out inference in certain settings where\nobtaining analytic distributional approximations is difficult or impossible.\nThis article explains the usefulness and limitations of the bootstrap in\ncontexts of interest in econometrics. The presentation is informal and\nexpository. It provides an intuitive understanding of how the bootstrap works.\nMathematical details are available in references that are cited."}, {"title": "Bayesian shrinkage in mixture of experts models: Identifying robust determinants of class membership", "abstract": "A method for implicit variable selection in mixture of experts frameworks is\nproposed. We introduce a prior structure where information is taken from a set\nof independent covariates. Robust class membership predictors are identified\nusing a normal gamma prior. The resulting model setup is used in a finite\nmixture of Bernoulli distributions to find homogenous clusters of women in\nMozambique based on their information sources on HIV. Fully Bayesian inference\nis carried out via the implementation of a Gibbs sampler."}, {"title": "On the Choice of Instruments in Mixed Frequency Specification Tests", "abstract": "Time averaging has been the traditional approach to handle mixed sampling\nfrequencies. However, it ignores information possibly embedded in high\nfrequency. Mixed data sampling (MIDAS) regression models provide a concise way\nto utilize the additional information in high-frequency variables. In this\npaper, we propose a specification test to choose between time averaging and\nMIDAS models, based on a Durbin-Wu-Hausman test. In particular, a set of\ninstrumental variables is proposed and theoretically validated when the\nfrequency ratio is large. As a result, our method tends to be more powerful\nthan existing methods, as reconfirmed through the simulations."}, {"title": "Estimating grouped data models with a binary dependent variable and fixed effects: What are the issues", "abstract": "This article deals with asimple issue: if we have grouped data with a binary\ndependent variable and want to include fixed effects (group specific\nintercepts) in the specification, is Ordinary Least Squares (OLS) in any way\nsuperior to a (conditional) logit form? In particular, what are the\nconsequences of using OLS instead of a fixed effects logit model with respect\nto the latter dropping all units which show no variability in the dependent\nvariable while the former allows for estimation using all units. First, we show\nthat the discussion of fthe incidental parameters problem is based on an\nassumption about the kinds of data being studied; for what appears to be the\ncommon use of fixed effect models in political science the incidental\nparameters issue is illusory. Turning to linear models, we see that OLS yields\na linear combination of the estimates for the units with and without variation\nin the dependent variable, and so the coefficient estimates must be carefully\ninterpreted. The article then compares two methods of estimating logit models\nwith fixed effects, and shows that the Chamberlain conditional logit is as good\nas or better than a logit analysis which simply includes group specific\nintercepts (even though the conditional logit technique was designed to deal\nwith the incidental parameters problem!). Related to this, the article\ndiscusses the estimation of marginal effects using both OLS and logit. While it\nappears that a form of logit with fixed effects can be used to estimate\nmarginal effects, this method can be improved by starting with conditional\nlogit and then using the those parameter estimates to constrain the logit with\nfixed effects model. This method produces estimates of sample average marginal\neffects that are at least as good as OLS, and much better when group size is\nsmall or the number of groups is large. ."}, {"title": "Proxy Controls and Panel Data", "abstract": "We provide new results for nonparametric identification, estimation, and\ninference of causal effects using `proxy controls': observables that are noisy\nbut informative proxies for unobserved confounding factors. Our analysis\napplies to cross-sectional settings but is particularly well-suited to panel\nmodels. Our identification results motivate a simple and `well-posed'\nnonparametric estimator. We derive convergence rates for the estimator and\nconstruct uniform confidence bands with asymptotically correct size. In panel\nsettings, our methods provide a novel approach to the difficult problem of\nidentification with non-separable, general heterogeneity and fixed $T$. In\npanels, observations from different periods serve as proxies for unobserved\nheterogeneity and our key identifying assumptions follow from restrictions on\nthe serial dependence structure. We apply our methods to two empirical\nsettings. We estimate consumer demand counterfactuals using panel data and we\nestimate causal effects of grade retention on cognitive performance."}, {"title": "Nonparametric Regression with Selectively Missing Covariates", "abstract": "We consider the problem of regression with selectively observed covariates in\na nonparametric framework. Our approach relies on instrumental variables that\nexplain variation in the latent covariates but have no direct effect on\nselection. The regression function of interest is shown to be a weighted\nversion of observed conditional expectation where the weighting function is a\nfraction of selection probabilities. Nonparametric identification of the\nfractional probability weight (FPW) function is achieved via a partial\ncompleteness assumption. We provide primitive functional form assumptions for\npartial completeness to hold. The identification result is constructive for the\nFPW series estimator. We derive the rate of convergence and also the pointwise\nasymptotic distribution. In both cases, the asymptotic performance of the FPW\nseries estimator does not suffer from the inverse problem which derives from\nthe nonparametric instrumental variable approach. In a Monte Carlo study, we\nanalyze the finite sample properties of our estimator and we compare our\napproach to inverse probability weighting, which can be used alternatively for\nunconditional moment estimation. In the empirical application, we focus on two\ndifferent applications. We estimate the association between income and health\nusing linked data from the SHARE survey and administrative pension information\nand use pension entitlements as an instrument. In the second application we\nrevisit the question how income affects the demand for housing based on data\nfrom the German Socio-Economic Panel Study (SOEP). In this application we use\nregional income information on the residential block level as an instrument. In\nboth applications we show that income is selectively missing and we demonstrate\nthat standard methods that do not account for the nonrandom selection process\nlead to significantly biased estimates for individuals with low income."}, {"title": "Granger causality on horizontal sum of Boolean algebras", "abstract": "The intention of this paper is to discuss the mathematical model of causality\nintroduced by C.W.J. Granger in 1969. The Granger's model of causality has\nbecome well-known and often used in various econometric models describing\ncausal systems, e.g., between commodity prices and exchange rates.\n  Our paper presents a new mathematical model of causality between two measured\nobjects. We have slightly modified the well-known Kolmogorovian probability\nmodel. In particular, we use the horizontal sum of set $\\sigma$-algebras\ninstead of their direct product."}, {"title": "On LASSO for Predictive Regression", "abstract": "Explanatory variables in a predictive regression typically exhibit low signal\nstrength and various degrees of persistence. Variable selection in such a\ncontext is of great importance. In this paper, we explore the pitfalls and\npossibilities of the LASSO methods in this predictive regression framework. In\nthe presence of stationary, local unit root, and cointegrated predictors, we\nshow that the adaptive LASSO cannot asymptotically eliminate all cointegrating\nvariables with zero regression coefficients. This new finding motivates a novel\npost-selection adaptive LASSO, which we call the twin adaptive LASSO (TAlasso),\nto restore variable selection consistency. Accommodating the system of\nheterogeneous regressors, TAlasso achieves the well-known oracle property. In\ncontrast, conventional LASSO fails to attain coefficient estimation consistency\nand variable screening in all components simultaneously. We apply these LASSO\nmethods to evaluate the short- and long-horizon predictability of S\\&P 500\nexcess returns."}, {"title": "Simple Inference on Functionals of Set-Identified Parameters Defined by Linear Moments", "abstract": "This paper proposes a new approach to obtain uniformly valid inference for\nlinear functionals or scalar subvectors of a partially identified parameter\ndefined by linear moment inequalities. The procedure amounts to bootstrapping\nthe value functions of randomly perturbed linear programming problems, and does\nnot require the researcher to grid over the parameter space. The low-level\nconditions for uniform validity rely on genericity results for linear programs.\nThe unconventional perturbation approach produces a confidence set with a\ncoverage probability of 1 over the identified set, but obtains exact coverage\non an outer set, is valid under weak assumptions, and is computationally simple\nto implement."}, {"title": "The Incidental Parameters Problem in Testing for Remaining Cross-section Correlation", "abstract": "In this paper we consider the properties of the Pesaran (2004, 2015a) CD test\nfor cross-section correlation when applied to residuals obtained from panel\ndata models with many estimated parameters. We show that the presence of\nperiod-specific parameters leads the CD test statistic to diverge as length of\nthe time dimension of the sample grows. This result holds even if cross-section\ndependence is correctly accounted for and hence constitutes an example of the\nIncidental Parameters Problem. The relevance of this problem is investigated\nboth for the classical Time Fixed Effects estimator as well as the Common\nCorrelated Effects estimator of Pesaran (2006). We suggest a weighted CD test\nstatistic which re-establishes standard normal inference under the null\nhypothesis. Given the widespread use of the CD test statistic to test for\nremaining cross-section correlation, our results have far reaching implications\nfor empirical researchers."}, {"title": "Prices, Profits, Proxies, and Production", "abstract": "This paper studies nonparametric identification and counterfactual bounds for\nheterogeneous firms that can be ranked in terms of productivity. Our approach\nworks when quantities and prices are latent, rendering standard approaches\ninapplicable. Instead, we require observation of profits or other\noptimizing-values such as costs or revenues, and either prices or price proxies\nof flexibly chosen variables. We extend classical duality results for\nprice-taking firms to a setup with discrete heterogeneity, endogeneity, and\nlimited variation in possibly latent prices. Finally, we show that convergence\nresults for nonparametric estimators may be directly converted to convergence\nresults for production sets."}, {"title": "Universal and idiosyncratic characteristic lengths in bacterial genomes", "abstract": "In condensed matter physics, simplified descriptions are obtained by\ncoarse-graining the features of a system at a certain characteristic length,\ndefined as the typical length beyond which some properties are no longer\ncorrelated. From a physics standpoint, in vitro DNA has thus a characteristic\nlength of 300 base pairs (bp), the Kuhn length of the molecule beyond which\ncorrelations in its orientations are typically lost. From a biology standpoint,\nin vivo DNA has a characteristic length of 1000 bp, the typical length of\ngenes. Since bacteria live in very different physico-chemical conditions and\nsince their genomes lack translational invariance, whether larger, universal\ncharacteristic lengths exist is a non-trivial question. Here, we examine this\nproblem by leveraging the large number of fully sequenced genomes available in\npublic databases. By analyzing GC content correlations and the evolutionary\nconservation of gene contexts (synteny) in hundreds of bacterial chromosomes,\nwe conclude that a fundamental characteristic length around 10-20 kb can be\ndefined. This characteristic length reflects elementary structures involved in\nthe coordination of gene expression, which are present all along the genome of\nnearly all bacteria. Technically, reaching this conclusion required us to\nimplement methods that are insensitive to the presence of large idiosyncratic\ngenomic features, which may co-exist along these fundamental universal\nstructures."}, {"title": "Differential proteomics highlights macrophage-specific responses to amorphous silica nanoparticles", "abstract": "The technological and economic benefits of engineered nanomaterials may be\noffset by their adverse effects on living organisms. One of the highly produced\nnanomaterials under such scrutiny is amorphous silica nanoparticles, which are\nknown to have an appreciable, although reversible, inflammatory potential. This\nis due to their selective toxicity toward macrophages, and it is thus important\nto study the cellular responses of this cell type to silica nanoparticles to\nbetter understand the direct or indirect adverse effects of nanosilica. We have\nhere studied the responses of the RAW264.7 murine macrophage cells and of the\ncontrol MPC11 plasma cells to subtoxic concentrations of nanosilica, using a\ncombination of pro-teomic and targeted approaches. This allowed us to document\nalterations in the cellular cytoskeleton, in the phagocytic capacity of the\ncells as well as their ability to respond to bacterial stimuli. More\nsurprisingly, silica nanoparticles also induce a greater sensitivity of\nmacrophages to DNA alkylating agents, such as styrene oxide, even at doses\nwhich do not induce any appreciable cell death."}, {"title": "Zinc oxide induces the stringent response and major reorientations in the central metabolism of Bacillus subtilis", "abstract": "Microorganisms, such as bacteria, are one of the first targets of\nnanoparticles in the environment. In this study, we tested the effect of two\nnanoparticles, ZnO and TiO2, with the salt ZnSO4 as the control, on the\nGram-positive bacterium Bacillus subtilis by 2D gel electrophoresis-based\nproteomics. Despite a significant effect on viability (LD50), TiO2 NPs had no\ndetectable effect on the proteomic pattern, while ZnO NPs and ZnSO4\nsignificantly modified B. subtilis metabolism. These results allowed us to\nconclude that the effects of ZnO observed in this work were mainly attributable\nto Zn dissolution in the culture media. Proteomic analysis highlighted twelve\nmodulated proteins related to central metabolism: MetE and MccB (cysteine\nmetabolism), OdhA, AspB, IolD, AnsB, PdhB and YtsJ (Krebs cycle) and XylA,\nYqjI, Drm and Tal (pentose phosphate pathway). Biochemical assays, such as free\nsulfhydryl, CoA-SH and malate dehydrogenase assays corroborated the observed\ncentral metabolism reorientation and showed that Zn stress induced oxidative\nstress, probably as a consequence of thiol chelation stress by Zn ions. The\nother patterns affected by ZnO and ZnSO4 were the stringent response and the\ngeneral stress response. Nine proteins involved in or controlled by the\nstringent response showed a modified expression profile in the presence of ZnO\nNPs or ZnSO4: YwaC, SigH, YtxH, YtzB, TufA, RplJ, RpsB, PdhB and Mbl. An\nincrease in the ppGpp concentration confirmed the involvement of the stringent\nresponse during a Zn stress. All these metabolic reorientations in response to\nZn stress were probably the result of complex regulatory mechanisms including\nat least the stringent response via YwaC."}, {"title": "NGS Based Haplotype Assembly Using Matrix Completion", "abstract": "We apply matrix completion methods for haplotype assembly from NGS reads to\ndevelop the new HapSVT, HapNuc, and HapOPT algorithms. This is performed by\napplying a mathematical model to convert the reads to an incomplete matrix and\nestimating unknown components. This process is followed by quantizing and\ndecoding the completed matrix in order to estimate haplotypes. These algorithms\nare compared to the state-of-the-art algorithms using simulated data as well as\nthe real fosmid data. It is shown that the SNP missing rate and the haplotype\nblock length of the proposed HapOPT are better than those of HapCUT2 with\ncomparable accuracy in terms of reconstruction rate and switch error rate. A\nprogram implementing the proposed algorithms in MATLAB is freely available at\nhttps://github.com/smajidian/HapMC."}, {"title": "Eight-cluster structure of chloroplast genomes differs from similar one observed for bacteria", "abstract": "Previously, a seven-cluster pattern claiming to be a universal one in\nbacterial genomes has been reported. Keeping in mind the most popular theory of\nchloroplast origin, we checked whether a similar pattern is observed in\nchloroplast genomes. Surprisingly, eight cluster structure has been found, for\nchloroplasts. The pattern observed for chloroplasts differs rather\nsignificantly, from bacterial one, and from that latter observed for\ncyanobacteria. The structure is provided by clustering of the fragments of\nequal length isolated within a genome so that each fragment is converted in\ntriplet frequency dictionary with non-overlapping triplets with no gaps in\nframe tiling. The points in 63-dimensional space were clustered due to elastic\nmap technique. The eight cluster found in chloroplasts comprises the fragments\nof a genome bearing tRNA genes and exhibiting excessively high\n$\\mathsf{GC}$-content, in comparison to the entire genome."}, {"title": "Predicting the spectrum of TCR repertoire sharing with a data-driven model of recombination", "abstract": "Despite the extreme diversity of T cell repertoires, many identical T-cell\nreceptor (TCR) sequences are found in a large number of individual mice and\nhumans. These widely-shared sequences, often referred to as `public', have been\nsuggested to be over-represented due to their potential immune functionality or\ntheir ease of generation by V(D)J recombination. Here we show that even for\nlarge cohorts the observed degree of sharing of TCR sequences between\nindividuals is well predicted by a model accounting for by the known\nquantitative statistical biases in the generation process, together with a\nsimple model of thymic selection. Whether a sequence is shared by many\nindividuals is predicted to depend on the number of queried individuals and the\nsampling depth, as well as on the sequence itself, in agreement with the data.\nWe introduce the degree of publicness conditional on the queried cohort size\nand the size of the sampled repertoires. Based on these observations we propose\na public/private sequence classifier, `PUBLIC' (Public Universal Binary\nLikelihood Inference Classifier), based on the generation probability, which\nperforms very well even for small cohort sizes."}, {"title": "Single nucleotide polymorphisms that modulate microRNA regulation of gene expression in tumors", "abstract": "Genome-wide association studies (GWAS) have identified single nucleotide\npolymorphisms (SNPs) associated with trait diversity and disease\nsusceptibility, yet the functional properties of many genetic variants and\ntheir molecular interactions remains unclear. It has been hypothesized that\nSNPs in microRNA binding sites may disrupt gene regulation by microRNAs\n(miRNAs), short non-coding RNAs that bind to mRNA and downregulate the target\ngene. While a number of studies have been conducted to predict the location of\nSNPs in miRNA binding sites, to date there has been no comprehensive analysis\nof how SNP variants may impact miRNA regulation of genes. Here we investigate\nthe functional properties of genetic variants and their effects on miRNA\nregulation of gene expression in cancer. Our analysis is motivated by the\nhypothesis that distinct alleles may cause differential binding (from miRNAs to\nmRNAs or from transcription factors to DNA) and change the expression of genes.\nWe previously identified pathways--systems of genes conferring specific cell\nfunctions--that are dysregulated by miRNAs in cancer, by comparing\nmiRNA-pathway associations between healthy and tumor tissue. We draw on these\nresults as a starting point to assess whether SNPs in genes on dysregulated\npathways are responsible for miRNA dysregulation of individual genes in tumors.\nUsing an integrative analysis that incorporates miRNA expression, mRNA\nexpression, and SNP genotype data, we identify SNPs that appear to influence\nthe association between miRNAs and genes, which we term \"regulatory QTLs\n(regQTLs)\": loci whose alleles impact the regulation of genes by miRNAs. We\ndescribe the method, apply it to analyze four cancer types (breast, liver,\nlung, prostate) using data from The Cancer Genome Atlas (TCGA), and provide a\ntool to explore the findings."}, {"title": "Minimum error correction-based haplotype assembly: considerations for long read data", "abstract": "The single nucleotide polymorphism (SNP) is the most widely studied type of\ngenetic variation. A haplotype is defined as the sequence of alleles at SNP\nsites on each haploid chromosome. Haplotype information is essential in\nunravelling the genome-phenotype association. Haplotype assembly is a\nwell-known approach for reconstructing haplotypes, exploiting reads generated\nby DNA sequencing devices. The Minimum Error Correction (MEC) metric is often\nused for reconstruction of haplotypes from reads. However, problems with the\nMEC metric have been reported. Here, we investigate the MEC approach to\ndemonstrate that it may result in incorrectly reconstructed haplotypes for\ndevices that produce error-prone long reads. Specifically, we evaluate this\napproach for devices developed by Illumina, Pacific BioSciences and Oxford\nNanopore Technologies. We show that imprecise haplotypes may be reconstructed\nwith a lower MEC than that of the exact haplotype. The performance of MEC is\nexplored for different coverage levels and error rates of data. Our simulation\nresults reveal that in order to avoid incorrect MEC-based haplotypes, a\ncoverage of 25 is needed for reads generated by Pacific BioSciences RS systems."}, {"title": "The bromodomain-containing protein Ibd1 links multiple chromatin related protein complexes to highly expressed genes in Tetrahymena thermophila", "abstract": "Background: The chromatin remodelers of the SWI/SNF family are critical\ntranscriptional regulators. Recognition of lysine acetylation through a\nbromodomain (BRD) component is key to SWI/SNF function; in most eukaryotes,\nthis function is attributed to SNF2/Brg1.\n  Results: Using affinity purification coupled to mass spectrometry (AP-MS) we\nidentified members of a SWI/SNF complex (SWI/SNFTt) in Tetrahymena thermophila.\nSWI/SNFTt is composed of 11 proteins, Snf5Tt, Swi1Tt, Swi3Tt, Snf12Tt, Brg1Tt,\ntwo proteins with potential chromatin interacting domains and four proteins\nwithout orthologs to SWI/SNF proteins in yeast or mammals. SWI/SNFTt subunits\nlocalize exclusively to the transcriptionally active macronucleus (MAC) during\ngrowth and development, consistent with a role in transcription. While\nTetrahymena Brg1 does not contain a BRD, our AP-MS results identified a\nBRD-containing SWI/SNFTt component, Ibd1 that associates with SWI/SNFTt during\ngrowth but not development. AP-MS analysis of epitope-tagged Ibd1 revealed it\nto be a subunit of several additional protein complexes, including putative\nSWRTt, and SAGATt complexes as well as a putative H3K4-specific histone methyl\ntransferase complex. Recombinant Ibd1 recognizes acetyl-lysine marks on\nhistones correlated with active transcription. Consistent with our AP-MS and\nhistone array data suggesting a role in regulation of gene expression, ChIP-Seq\nanalysis of Ibd1 indicated that it primarily binds near promoters and within\ngene bodies of highly expressed genes during growth.\n  Conclusions: Our results suggest that through recognizing specific histones\nmarks, Ibd1 targets active chromatin regions of highly expressed genes in\nTetrahymena where it subsequently might coordinate the recruitment of several\nchromatin remodeling complexes to regulate the transcriptional landscape of\nvegetatively growing Tetrahymena cells."}, {"title": "Modeling and analysis of RNA-seq data: a review from a statistical perspective", "abstract": "Background: Since the invention of next-generation RNA sequencing (RNA-seq)\ntechnologies, they have become a powerful tool to study the presence and\nquantity of RNA molecules in biological samples and have revolutionized\ntranscriptomic studies. The analysis of RNA-seq data at four different levels\n(samples, genes, transcripts, and exons) involve multiple statistical and\ncomputational questions, some of which remain challenging up to date.\n  Results: We review RNA-seq analysis tools at the sample, gene, transcript,\nand exon levels from a statistical perspective. We also highlight the\nbiological and statistical questions of most practical considerations.\n  Conclusion: The development of statistical and computational methods for\nanalyzing RNA- seq data has made significant advances in the past decade.\nHowever, methods developed to answer the same biological question often rely on\ndiverse statical models and exhibit different performance under different\nscenarios. This review discusses and compares multiple commonly used\nstatistical models regarding their assumptions, in the hope of helping users\nselect appropriate methods as needed, as well as assisting developers for\nfuture method development."}, {"title": "Identification of a complete YPT1 Rab GTPase sequence from the fungal pathogen Colletotrichum incanum", "abstract": "Colletotrichum represent a genus of fungal species primarily known as plant\npathogens with severe economic impacts in temperate, subtropical and tropical\nclimates Consensus taxonomy and classification systems for Colletotrichum\nspecies have been undergoing revision as high resolution genomic data becomes\navailable. Here we propose an alternative annotation that provides a complete\nsequence for a Colletotrichum YPT1 gene homolog using the whole genome shotgun\nsequence of Colletotrichum incanum isolated from soybean crops in Illinois,\nUSA."}, {"title": "The Qatar Genome: A Population-Specific Tool for Precision Medicine in the Middle East", "abstract": "Reaching the full potential of precision medicine depends on the quality of\npersonalized genome interpretation. In order to facilitate precision medicine\nin regions of the Middle East and North Africa (MENA), a population-specific\nreference genome for the indigenous Arab popula-tion of Qatar (QTRG) was\nconstructed by incorporating allele frequency data from sequencing of 1,161\nQataris, representing 0.4% of the population. A total of 20.9 million SNP and\n3.1 million indels were observed in Qatar, including an average of 1.79% novel\nvariants per individual ge-nome. Replacement of the GRCh37 standard reference\nwith QTRG in a best practices genome analysis workflow resulted in an average\nof 7* deeper coverage depth (an improvement of 23%), and 756,671 fewer variants\non average, a reduction of 16% that is attributed to common Qatari alleles\nbeing present in the QTRG reference. The benefit for using QTRG varies across\nances-tries, a factor that should be taken into consideration when selecting an\nappropriate reference for analysis."}, {"title": "A quick guide for student-driven community genome annotation", "abstract": "High quality gene models are necessary to expand the molecular and genetic\ntools available for a target organism, but these are available for only a\nhandful of model organisms that have undergone extensive curation and\nexperimental validation over the course of many years. The majority of gene\nmodels present in biological databases today have been identified in draft\ngenome assemblies using automated annotation pipelines that are frequently\nbased on orthologs from distantly related model organisms. Manual curation is\ntime consuming and often requires substantial expertise, but is instrumental in\nimproving gene model structure and identification. Manual annotation may seem\nto be a daunting and cost-prohibitive task for small research communities but\ninvolving undergraduates in community genome annotation consortiums can be\nmutually beneficial for both education and improved genomic resources. We\noutline a workflow for efficient manual annotation driven by a team of\nprimarily undergraduate annotators. This model can be scaled to large teams and\nincludes quality control processes through incremental evaluation. Moreover, it\ngives students an opportunity to increase their understanding of genome biology\nand to participate in scientific research in collaboration with peers and\nsenior researchers at multiple institutions."}, {"title": "Transcription Factor-DNA Binding Via Machine Learning Ensembles", "abstract": "We present ensemble methods in a machine learning (ML) framework combining\npredictions from five known motif/binding site exploration algorithms. For a\ngiven TF the ensemble starts with position weight matrices (PWM's) for the\nmotif, collected from the component algorithms. Using dimension reduction, we\nidentify significant PWM-based subspaces for analysis. Within each subspace a\nmachine classifier is built for identifying the TF's gene (promoter) targets\n(Problem 1). These PWM-based subspaces form an ML-based sequence analysis tool.\nProblem 2 (finding binding motifs) is solved by agglomerating k-mer (string)\nfeature PWM-based subspaces that stand out in identifying gene targets. We\napproach Problem 3 (binding sites) with a novel machine learning approach that\nuses promoter string features and ML importance scores in a classification\nalgorithm locating binding sites across the genome. For target gene\nidentification this method improves performance (measured by the F1 score) by\nabout 10 percentage points over the (a) motif scanning method and (b) the\ncoexpression-based association method. Top motif outperformed 5 component\nalgorithms as well as two other common algorithms (BEST and DEME). For\nidentifying individual binding sites on a benchmark cross species database\n(Tompa et al., 2005) we match the best performer without much human\nintervention. It also improved the performance on mammalian TFs.\n  The ensemble can integrate orthogonal information from different weak\nlearners (potentially using entirely different types of features) into a\nmachine learner that can perform consistently better for more TFs. The TF gene\ntarget identification component (problem 1 above) is useful in constructing a\ntranscriptional regulatory network from known TF-target associations. The\nensemble is easily extendable to include more tools as well as future PWM-based\ninformation."}, {"title": "The sequencing and interpretation of the genome obtained from a Serbian individual", "abstract": "Recent genetic studies and whole-genome sequencing projects have greatly\nimproved our understanding of human variation and clinically actionable genetic\ninformation. Smaller ethnic populations, however, remain underrepresented in\nboth individual and large-scale sequencing efforts and hence present an\nopportunity to discover new variants of biomedical and demographic\nsignificance. This report describes the sequencing and analysis of a genome\nobtained from an individual of Serbian origin, introducing tens of thousands of\npreviously unknown variants to the currently available pool. Ancestry analysis\nplaces this individual in close proximity of the Central and Eastern European\npopulations; i.e., closest to Croatian, Bulgarian and Hungarian individuals\nand, in terms of other Europeans, furthest from Ashkenazi Jewish, Spanish,\nSicilian, and Baltic individuals. Our analysis confirmed gene flow between\nNeanderthal and ancestral pan-European populations, with similar contributions\nto the Serbian genome as those observed in other European groups. Finally, to\nassess the burden of potentially disease-causing/clinically relevant variation\nin the sequenced genome, we utilized manually curated genotype-phenotype\nassociation databases and variant-effect predictors. We identified several\nvariants that have previously been associated with severe early-onset disease\nthat is not evident in the proband, as well as variants that could yet prove to\nbe clinically relevant to the proband over the next decades. The presence of\nnumerous private and low-frequency variants along with the observed and\npredicted disease-causing mutations in this genome exemplify some of the global\nchallenges of genome interpretation, especially in the context of understudied\nethnic groups."}, {"title": "Quantifying Local Randomness in Human DNA and RNA Sequences Using Erdos Motifs", "abstract": "In 1932, Paul Erdos asked whether a random walk constructed from a binary\nsequence can achieve the lowest possible deviation (lowest discrepancy), for\nthe sequence itself and for all its subsequences formed by homogeneous\narithmetic progressions. Although avoiding low discrepancy is impossible for\ninfinite sequences, as recently proven by Terence Tao, attempts were made to\nconstruct such sequences with finite lengths. We recognize that such\nconstructed sequences (we call these \"Erdos sequences\") exhibit certain\nhallmarks of randomness at the local level: they show roughly equal frequencies\nof subsequences, and at the same time exclude the trivial periodic patterns.\nFor the human DNA we examine the frequency of a set of Erdos motifs of\nlength-10 using three nucleotides-to-binary mappings. The particular length-10\nErdos sequence is derived by the length-11 Mathias sequence and is identical\nwith the first 10 digits of the Thue-Morse sequence, underscoring the fact that\nboth are deficient in periodicities. Our calculations indicate that: (1) the\npurine (A and G)/pyridimine (C and T) based Erdos motifs are greatly\nunderrepresented in the human genome, (2) the strong(G and C)/weak(A and T)\nbased Erdos motifs are slightly overrepresented, (3) the densities of the two\nare negatively correlated, (4) the Erdos motifs based on all three mappings\nbeing combined are slightly underrepresented, and (5) the strong/weak based\nErdos motifs are greatly overrepresented in the human messenger RNA sequences."}, {"title": "Identifying viruses from metagenomic data by deep learning", "abstract": "The recent development of metagenomic sequencing makes it possible to\nsequence microbial genomes including viruses in an environmental sample.\nIdentifying viral sequences from metagenomic data is critical for downstream\nvirus analyses. The existing reference-based and gene homology-based methods\nare not efficient in identifying unknown viruses or short viral sequences. Here\nwe have developed a reference-free and alignment-free machine learning method,\nDeepVirFinder, for predicting viral sequences in metagenomic data using deep\nlearning techniques. DeepVirFinder was trained based on a large number of viral\nsequences discovered before May 2015. Evaluated on the sequences after that\ndate, DeepVirFinder outperformed the state-of-the-art method VirFinder at all\ncontig lengths. Enlarging the training data by adding millions of purified\nviral sequences from environmental metavirome samples significantly improves\nthe accuracy for predicting under-represented viruses. Applying DeepVirFinder\nto real human gut metagenomic samples from patients with colorectal carcinoma\n(CRC) identified 51,138 viral sequences belonging to 175 bins. Ten bins were\nassociated with the cancer status, indicating their potential use for\nnon-invasive diagnosis of CRC. In summary, DeepVirFinder greatly improved the\nprecision and recall rates of viral identification, and it will significantly\naccelerate the discovery rate of viruses."}, {"title": "Innovative method for reducing uninformative calls in non-invasive prenatal testing", "abstract": "Non-invasive prenatal testing or NIPT is currently among the top researched\ntopic in obstetric care. While the performance of the current state-of-the-art\nNIPT solutions achieve high sensitivity and specificity, they still struggle\nwith a considerable number of samples that cannot be concluded with certainty.\nSuch uninformative results are often subject to repeated blood sampling and\nre-analysis, usually after two weeks, and this period may cause a stress to the\nfuture mothers as well as increase the overall cost of the test. We propose a\nsupplementary method to traditional z-scores to reduce the number of such\nuninformative calls. The method is based on a novel analysis of the length\nprofile of circulating cell free DNA which compares the change in such profiles\nwhen random-based and length-based elimination of some fragments is performed.\nThe proposed method is not as accurate as the standard z-score; however, our\nresults suggest that combination of these two independent methods correctly\nresolves a substantial portion of healthy samples with an uninformative result.\nAdditionally, we discuss how the proposed method can be used to identify\nmaternal aberrations, thus reducing the risk of false positive and false\nnegative calls.\n  Keywords: Next-generation sequencing, Cell-free DNA, Uninformative result,\nMethod, Trisomy, Prenatal testing"}, {"title": "Genesis of the alpha beta T-cell receptor", "abstract": "The T-cell (TCR) repertoire relies on the diversity of receptors composed of\ntwo chains, called $\\alpha$ and $\\beta$, to recognize pathogens. Using results\nof high throughput sequencing and computational chain-pairing experiments of\nhuman TCR repertoires, we quantitively characterize the $\\alpha\\beta$\ngeneration process. We estimate the probabilities of a rescue recombination of\nthe $\\beta$ chain on the second chromosome upon failure or success on the first\nchromosome. Unlike $\\beta$ chains, $\\alpha$ chains recombine simultaneously on\nboth chromosomes, resulting in correlated statistics of the two genes which we\npredict using a mechanistic model. We find that $\\sim 28 \\%$ of cells express\nboth $\\alpha$ chains. We report that clones sharing the same $\\beta$ chain but\ndifferent $\\alpha$ chains are overrepresented, suggesting that they respond to\ncommon immune challenges. Altogether, our statistical analysis gives a complete\nquantitative mechanistic picture that results in the observed correlations in\nthe generative process. We learn that the probability to generate any\nTCR$\\alpha\\beta$ is lower than $10^{-12}$ and estimate the generation diversity\nand sharing properties of the $\\alpha\\beta$ TCR repertoire."}, {"title": "Evolution of biosequence search algorithms: a brief survey", "abstract": "The paper surveys the evolution of main algorithmic techniques to compare and\nsearch biological sequences. We highlight key algorithmic ideas emerged in\nresponse to several interconnected factors: shifts of biological analytical\nparadigm, advent of new sequencing technologies, and a substantial increase in\nsize of the available data. We discuss the expansion of alignment-free\ntechniques coming to replace alignment-based algorithms in large-scale\nanalyses. We further emphasize recently emerged and growing applications of\nsketching methods which support comparison of massive datasets, such as\nmetagenomics samples. Finally, we focus on the transition to population\ngenomics and outline associated algorithmic challenges."}, {"title": "Quantitative and functional post-translational modification proteomics reveals that TREPH1 plays a role in plant thigmomorphogenesis", "abstract": "Plants can sense both intracellular and extracellular mechanical forces and\ncan respond through morphological changes. The signaling components responsible\nfor mechanotransduction of the touch response are largely unknown. Here, we\nperformed a high-throughput SILIA (stable isotope labeling in\nArabidopsis)-based quantitative phosphoproteomics analysis to profile changes\nin protein phosphorylation resulting from 40 seconds of force stimulation in\nArabidopsis thaliana. Of the 24 touch-responsive phosphopeptides identified,\nmany were derived from kinases, phosphatases, cytoskeleton proteins, membrane\nproteins and ion transporters. TOUCH-REGULATED PHOSPHOPROTEIN1 (TREPH1) and MAP\nKINASE KINASE 2 (MKK2) and/or MKK1 became rapidly phosphorylated in\ntouch-stimulated plants. Both TREPH1 and MKK2 are required for touch-induced\ndelayed flowering, a major component of thigmomorphogenesis. The treph1-1 and\nmkk2 mutants also exhibited defects in touch-inducible gene expression. A\nnon-phosphorylatable site-specific isoform of TREPH1 (S625A) failed to restore\ntouch-induced flowering delay of treph1-1, indicating the necessity of S625 for\nTREPH1 function and providing evidence consistent with the possible functional\nrelevance of the touch-regulated TREPH1 phosphorylation. Bioinformatic analysis\nand biochemical subcellular fractionation of TREPH1 protein indicate that it is\na soluble protein. Altogether, these findings identify new protein players in\nArabidopsis thigmomorphogenesis regulation, suggesting that protein\nphosphorylation may play a critical role in plant force responses."}, {"title": "SSCU: an R/Bioconductor package for analyzing selective profile in synonymous codon usage", "abstract": "Background Synonymous codon choice is mainly affected by mutation and\nselection. For the majority of genes within a genome, mutational pressure is\nthe major driving force, but selective strength can be strong and dominant for\nspecific set of genes or codons. More specifically, the selective strength on\ntranslational efficiency and accuracy increases with the gene's expression\nlevel. Many statistical approaches have been developed to evaluate and quantify\nthe selective profile in codon usage, including S index and Akashi's test, but\nno program or pipeline has been developed that includes these tests and\nautomates the calculation.\n  Results In this study, we release an R package SSCU (selective strength for\ncodon usage, v2.4.0), which includes tools for codon usage analyses. The\npackage identifies optimal codons using two approaches (comparative and\ncorrelative methods), implements well-established statistics for detecting\ncodon selection, such as S index, Akashi's test, and estimates standard genomic\nstatistics, such as genomic GC3, RSCU and Nc.\n  Conclusions The package is useful for researchers working on the codon usage\nanalysis, and thus has general interest to the biological research community.\nThe package is deposited and curated at the Bioconductor site, and has\ncurrently been downloaded for more than 2000 times and ranked as top 50%\npackages."}, {"title": "Pharmacogenomics in the Age of GWAS, Omics Atlases, and PheWAS", "abstract": "The search for causative pharmacogenomic loci is being transformed by\nintegrative omics pipelines, but their outputs have only begun being applied to\ntest design. We assess the direction of the field in light of Biobanks/PheWAS,\nomics atlases, and AI. We first assess the potential of recent epigenome and\nspatial genome concepts, datasets, and methods to improve the functionality of\nPIP-style pipelines. We then discuss new potential methods of genetic test\ndesign on the basis of the outputs of such pipelines. We conclude with a vision\nfor a pharmacophenomic atlas, in which omics atlas data, PheWAS associations,\nand biobank data would be used with AI to design thousands of genetic tests for\nclinical deployment in an automated parallel process."}, {"title": "Whole genome resequencing reveals diagnostic markers for investigating global migration and hybridization between minke whale species", "abstract": "Background: In the marine environment, where there are few absolute physical\nbarriers, contemporary contact between previously isolated species can occur\nacross great distances, and in some cases, may be inter-oceanic. [..] in the\nminke whale species complex [...] migrations [..] have been documented and\nfertile hybrids and back-crossed individuals between both species have also\nbeen identified. However, it is not known whether this represents a\ncontemporary event, potentially driven by ecosystem changes in the Antarctic,\nor a sporadic occurrence happening over an evolutionary time-scale. We\nsuccessfully used whole genome resequencing to identify a panel of diagnostic\nSNPs which now enable us address this evolutionary question.\n  Results: A large number of SNPs displaying fixed or nearly fixed allele\nfrequency differences among the minke whale species were identified from the\nsequence data. Five panels of putatively diagnostic markers were established on\na genotyping platform for validation of allele frequencies; two panels (26 and\n24 SNPs) separating the two species of minke whale, and three panels (22, 23,\nand 24 SNPs) differentiating the three subspecies of common minke whale. The\npanels were validated against a set of reference samples, demonstrating the\nability to accurately identify back-crossed whales up to three generations.\n  Conclusions: This work has resulted in the development of a panel of novel\ndiagnostic genetic markers to address inter-oceanic and global contact among\nthe genetically isolated minke whale species and sub-species. These markers,\nincluding a globally relevant genetic reference data set for this species\ncomplex, are now openly available for researchers [..]. The approach used here,\ncombining whole genome resequencing and high-throughput genotyping, represents\na universal approach to develop similar tools for other species and population\ncomplexes."}, {"title": "A reproducible effect size is more useful than an irreproducible hypothesis test to analyze high throughput sequencing datasets", "abstract": "Motivation: P values derived from the null hypothesis significance testing\nframework are strongly affected by sample size, and are known to be\nirreproducible in underpowered studies, yet no suitable replacement has been\nproposed. Results: Here we present implementations of non-parametric\nstandardized median effect size estimates, dNEF, for high-throughput sequencing\ndatasets. Case studies are shown for transcriptome and tag-sequencing datasets.\nThe dNEF measure is shown to be more reproducible and robust than P values and\nrequires sample sizes as small as 3 to reproducibly identify differentially\nabundant features. Availability: Source code and binaries freely available at:\nhttps://bioconductor.org/packages/ALDEx2.html , omicplotR, and\nhttps://github.com/ggloor/CoDaSeq ."}, {"title": "Virus genome sequence classification using features based on nucleotides, words and compression", "abstract": "The ICTV develops, refines and maintains a universal virus taxonomy; Order is\nthe highest taxon in the branching hierarchy of recognised viral taxa.\nHistorically, ICTV (sub)committees have classified viruses on the basis of\nmorphological characteristics and various other attributes. Today, virtually\nall new viral genomes are assembled from metagenomic datasets and are not\nlinked directly to biological agents. Thus, placing a virus into a taxonomic\nscheme solely from primary genome structure is an increasingly important\nproblem. Various simple descriptive statistics of a viral genome sequence have\nbeen used successfully for virus classification. Here, we use the NCBI's viral\nand viroid reference sequence collection (RefSeq) and a common experimental\nframework to compare the performance of different genome sequence-derived\nfeatures and classifiers in the task of assigning a virus to one of seven ICTV\nOrders. The nucleotide-, word-, and compression-based features we consider\ninclude genome length, the k-mer Natural Vector (k = 1, ..., 6) and its\nderivatives, return time distribution, and general-purpose and DNA-specific\ncompression ratios; the classifiers used are the k-NN and SVM. The combination\nof genome length and k-NN has the worst, yet still respectable, performance\n(mean error rate of 0.137); the best performance is achieved using 4-mer counts\nand SVM (mean error rate of 0.006). We investigate the main causes of\nmisclassification, explore which viruses are more difficult to classify, and\nuse the best performing combination to predict the Orders of 1,834 unclassified\nviruses. A subsequent version of RefSeq assigned Orders to 17 of these\npreviously unlabelled viruses. Since 16 of our predictions match these\nassignments, our approach could aid virologists dealing with viruses that are\nknown only from sequence data."}, {"title": "Integrated systems approach identifies pathways from the genome to triglycerides through a metabolomic causal network", "abstract": "Introduction: To leverage functionality and clinical relevance into\nunderstanding systems biology, one needs to understand the pathway of the\ngenetic effects on risk factors/disease through intermediate molecular levels,\nsuch as metabolomics. Systems approaches integrate multi-omic information to\nfind pathways to disease endpoints and make optimal inference decisions.\nMethod: Here, we introduce a multi-stage approach to integrate causal networks\nin observational studies and GWAS to facilitate mechanistic understanding\nthrough identification of pathways from the genome to risk factors/disease via\nmetabolomics. The pathways in causal networks reveal the underlying\nrelationships behind observations, which do not play a significant role in more\ntraditional correlative analyses, where one variable at a time is considered.\nResults: We identified a causal network over the metabolomic level using the\ngenome directed acyclic graph (G-DAG), to systematically assess whether\nvariations in the genome lead to variations in triglyceride levels as a risk\nfactor of cardiovascular disease. We found LRRC46 and LRRC69 harboring\nloss-of-function mutations have significant effect on two metabolites with\ndirect effects on triglyceride levels. We also found pathways of FAM198B and\nC6orf25 to triglycerides through indirect paths from metabolites. Conclusion:\nIntegrating causal networks with GWAS facilitates mechanistic understanding in\ncomparison to one-variable-at-a-time approaches due to accounting for\nrelationships among components at intermediate molecular levels. This approach\nis complementary to experimental studies to identify efficacious targets in the\nage of big data sets."}, {"title": "Effect of Blast Exposure on Gene-Gene Interactions", "abstract": "Repeated exposure to low-level blast may initiate a range of adverse health\nproblem such as traumatic brain injury (TBI). Although many studies\nsuccessfully identified genes associated with TBI, yet the cellular mechanisms\nunderpinning TBI are not fully elucidated. In this study, we investigated\nunderlying relationship among genes through constructing transcript Bayesian\nnetworks using RNA-seq data. The data for pre- and post-blast transcripts,\nwhich were collected on 33 individuals in Army training program, combined with\nour system approach provide unique opportunity to investigate the effect of\nblast-wave exposure on gene-gene interactions. Digging into the networks, we\nidentified four subnetworks related to immune system and inflammatory process\nthat are disrupted due to the exposure. Among genes with relatively high fold\nchange in their transcript expression level, ATP6V1G1, B2M, BCL2A1, PELI,\nS100A8, TRIM58 and ZNF654 showed major impact on the dysregulation of the\ngene-gene interactions. This study reveals how repeated exposures to traumatic\nconditions increase the level of fold change of transcript expression and\nhypothesizes new targets for further experimental studies."}, {"title": "Transient crosslinking kinetics optimize gene cluster interactions", "abstract": "Our understanding of how chromosomes structurally organize and dynamically\ninteract has been revolutionized through the lens of long-chain polymer\nphysics. Major protein contributors to chromosome structure and dynamics are\ncondensin and cohesin that stochastically generate loops within and between\nchains, and entrap proximal strands of sister chromatids. In this paper, we\nexplore the ability of transient, protein-mediated, gene-gene crosslinks to\ninduce clusters of genes, thereby dynamic architecture, within the highly\nrepeated ribosomal DNA that comprises the nucleolus of budding yeast. We\nimplement three approaches: live cell microscopy; computational modeling of the\nfull genome during G1 in budding yeast, exploring four decades of timescales\nfor transient crosslinks between 5k bp domains in the nucleolus on Chromosome\nXII; and, temporal network models with automated community detection algorithms\napplied to the full range of 4D modeling datasets. The data analysis tools\ndetect and track gene clusters, their size, number, persistence time, and their\nplasticity. Of biological significance, our analysis reveals an optimal mean\ncrosslink lifetime that promotes pairwise and cluster gene interactions through\n\"flexible\" clustering. In this state, large gene clusters self-assemble yet\nfrequently interact, marked by gene exchanges between clusters, which in turn\nmaximizes global gene interactions in the nucleolus. This regime stands between\ntwo limiting cases each with far less global gene interactions: with shorter\ncrosslink lifetimes, \"rigid\" clustering emerges with clusters that interact\ninfrequently; with longer crosslink lifetimes, there is a dissolution of\nclusters. These observations are compared with imaging experiments on a normal\nyeast strain and two condensin-modified mutant cell strains, applying the same\nimage analysis pipeline to the experimental and simulated datasets."}, {"title": "Integrating splice-isoform expression into genome-scale models characterizes breast cancer metabolism", "abstract": "Motivation: Despite being often perceived as the main contributors to cell\nfate and physiology, genes alone cannot predict cellular phenotype. During the\nprocess of gene expression, 95% of human genes can code for multiple proteins\ndue to alternative splicing. While most splice variants of a gene carry the\nsame function, variants within some key genes can have remarkably different\nroles. To bridge the gap between genotype and phenotype, condition- and\ntissue-specific models of metabolism have been constructed. However, current\nmetabolic models only include information at the gene level. Consequently, as\nrecently acknowledged by the scientific community, common situations where\nchanges in splice-isoform expression levels alter the metabolic outcome cannot\nbe modeled. Results: We here propose GEMsplice, the first method for the\nincorporation of splice-isoform expression data into genome-scale metabolic\nmodels. Using GEMsplice, we make full use of RNA-Seq quantitative expression\nprofiles to predict, for the first time, the effects of splice isoform-level\nchanges in the metabolism of 1455 patients with 31 different breast cancer\ntypes. We validate GEMsplice by generating cancer-versus-normal predictions on\nmetabolic pathways, and by comparing with gene-level approaches and available\nliterature on pathways affected by breast cancer. GEMsplice is freely available\nfor academic use at https://github.com/GEMsplice/GEMsplice_code. Compared to\nstate-of-the-art methods, we anticipate that GEMsplice will enable for the\nfirst time computational analyses at transcript level with splice-isoform\nresolution."}, {"title": "Computational and molecular dissection of an X-box cis-Regulatory module", "abstract": "Ciliopathies are a class of human diseases marked by dysfunction of the\ncellular organelle, cilia. While many of the molecular components that make up\ncilia have been identified and studied, comparatively little is understood\nabout the transcriptional regulation of genes encoding these components. The\nconserved transcription factor Regulatory Factor X (RFX)/DAF-19, which acts\nthrough binding to the cis-regulatory motif known as X-box, has been shown to\nregulate ciliary genes in many animals from Caenorhabditis elegans to humans.\nHowever, accumulating evidence suggests that RFX is unable to initiate\ntranscription on its own. Therefore, other factors and cis-regulatory elements\nare likely required. One such element, a DNA motif called the C-box, has\nrecently been identified in C. elegans. It is still unclear if the X-box and\nC-boxes are the only regulatory elements involved and how they interact. To\nthis end, I analyzed the transcriptional regulation of dyf-5, the C. elegans\northolog of the human ciliopathy gene Male-Associated Kinase (MAK). Using\ncomputational methods, I was able to confirm the presence of the previously\nreported X-box and C-boxes as well as identifying an additional C-box. By\nsequentially mutating each of the identified motifs, I identified the role each\npotential motif plays in transcriptional regulation of dyf-5. My results showed\nthat only the X-box and the three C-boxes are necessary and are sufficient to\ndrive transcription, with the X-box and the centre C-box being the major\ncontributors and the other two C-boxes enhancing expression. This study\nadvances the knowledge of gene regulation in general and will further our\nunderstanding of ciliopathies and the mutations that cause them."}, {"title": "Analysis of evolutionary origins of genomic loci harboring 59,732 candidate human-specific regulatory sequences identifies genetic divergence patterns during evolution of Great Apes", "abstract": "Our view of the universe of genomic regions harboring various types of\ncandidate human-specific regulatory sequences (HSRS) has been markedly expanded\nin recent years. To infer the evolutionary origins of loci harboring HSRS,\nanalyses of conservations patterns of 59,732 loci in Modern Humans, Chimpanzee,\nBonobo, Gorilla, Orangutan, Gibbon, and Rhesus genomes have been performed. Two\nmajor evolutionary pathways have been identified comprising thousands of\nsequences that were either inherited from extinct common ancestors (ECAs) or\ncreated de novo in humans after human/chimpanzee split. Thousands of HSRS\nappear inherited from ECAs yet bypassed genomes of our closest evolutionary\nrelatives, presumably due to the incomplete lineage sorting and/or\nspecies-specific loss or regulatory DNA. The bypassing pattern is prominent for\nHSRS associated with development and functions of human brain. Common genomic\nloci that may contributed to speciation during evolution of Great Apes comprise\n248 insertions sites of African Great Ape-specific retrovirus PtERV1 (45.9%; p\n= 1.03E-44) intersecting regions harboring 442 HSRS, which are enriched for\nHSRS associated with human-specific (HS) changes of gene expression in cerebral\norganoids. Among non-human primates (NHP), most significant fractions of\ncandidate HSRS associated with HS expression changes in both excitatory neurons\n(347 loci; 67%) and radial glia (683 loci; 72%) are highly conserved in Gorilla\ngenome. Modern Humans acquired unique combinations of regulatory sequences\nhighly conserved in distinct species of six NHP separated by 30 million years\nof evolution. Concurrently, this unique mosaic of regulatory sequences\ninherited from ECAs was supplemented with 12,486 created de novo HSRS. These\nobservations support the model of complex continuous speciation process during\nevolution of Great Apes that is not likely to occur as an instantaneous event."}, {"title": "CTCF Degradation Causes Increased Usage of Upstream Exons in Mouse Embryonic Stem Cells", "abstract": "Transcriptional repressor CTCF is an important regulator of chromatin 3D\nstructure, facilitating the formation of topologically associating domains\n(TADs). However, its direct effects on gene regulation is less well understood.\nHere, we utilize previously published ChIP-seq and RNA-seq data to investigate\nthe effects of CTCF on alternative splicing of genes with CTCF sites. We\ncompared the amount of RNA-seq signals in exons upstream and downstream of\nbinding sites following auxin-induced degradation of CTCF in mouse embryonic\nstem cells. We found that changes in gene expression following CTCF depletion\nwere significant, with a general increase in the presence of upstream exons. We\ninfer that a possible mechanism by which CTCF binding contributes to\nalternative splicing is by causing pauses in the transcription mechanism during\nwhich splicing elements are able to concurrently act on upstream exons already\ntranscribed into RNA."}, {"title": "A Comparison of Microbial Genome Web Portals", "abstract": "Microbial genome web portals have a broad range of capabilities that address\na number of information-finding and analysis needs for scientists. This article\ncompares the capabilities of the major microbial genome web portals to aid\nresearchers in determining which portal(s) are best suited to solving their\ninformation-finding and analytical needs. We assessed both the bioinformatics\ntools and the data content of BioCyc, KEGG, Ensembl Bacteria, KBase, IMG, and\nPATRIC. For each portal, our assessment compared and tallied the available\ncapabilities. The strengths of BioCyc include its genomic and metabolic tools,\nmulti-search capabilities, table-based analysis tools, regulatory network tools\nand data, omics data analysis tools, breadth of data content, and large amount\nof curated data. The strengths of KEGG include its genomic and metabolic tools.\nThe strengths of Ensembl Bacteria include its genomic tools and large number of\ngenomes. The strengths of KBase include its genomic tools and metabolic models.\nThe strengths of IMG include its genomic tools, multi-search capabilities,\nlarge number of genomes, table-based analysis tools, and breadth of data\ncontent. The strengths of PATRIC include its large number of genomes,\ntable-based analysis tools, metabolic models, and breadth of data content."}, {"title": "Whole genome single nucleotide polymorphism genotyping of Staphylococcus aureus", "abstract": "Next-generation sequencing technology enables routine detection of bacterial\npathogens for clinical diagnostics and genetic research. Whole genome\nsequencing has been of importance in the epidemiologic analysis of bacterial\npathogens. However, few whole genome sequencing-based genotyping pipelines are\navailable for practical applications. Here, we present the whole genome\nsequencing-based single nucleotide polymorphism (SNP) genotyping method and\napply to the evolutionary analysis of methicillin-resistant Staphylococcus\naureus. The SNP genotyping method calls genome variants using next-generation\nsequencing reads of whole genomes and calculates the pair-wise Jaccard\ndistances of the genome variants. The method may reveal the high-resolution\nwhole genome SNP profiles and the structural variants of different isolates of\nmethicillin-resistant S. aureus (MRSA) and methicillin-susceptible S. aureus\n(MSSA) strains. The phylogenetic analysis of whole genomes and particular\nregions may monitor and track the evolution and the transmission dynamic of\nbacterial pathogens. The computer programs of the whole genome sequencing-based\nSNP genotyping method are available to the public at\nhttps://github.com/cyinbox/NGS."}, {"title": "Searching by index for similar sequences: the SEQR algorithm", "abstract": "This paper describes a method to efficiently retrieve protein database\nsequences similar to a query sequence, while allowing for significant numbers\nof mutations. We call this method SEQR for SEQuence Retrieval. This approach\nincreases the speed of sequence similarity searches by an order of magnitude\ncompared to conventional algorithms at the expense of sensitivity. Furthermore,\nretrieval time increases less than linearly with the number of sequences, a\ndesirable property during an era when next generation sequencing technologies\nhave yielded greater than exponential increases in sequence records. The lower\nsensitivity of the algorithm for distantly related sequences compared to\nbenchmarks is not intrinsic to the method itself, but rather due to the\nprocedure used to construct the indexing terms, and may be improved. The\nindexing terms themselves can be added to standard information retrieval\nengines, enabling complex queries that include sequence similarity and other\ndescriptors such as taxonomy and text descriptions."}, {"title": "Linking de novo assembly results with long DNA reads by dnaasm-link application", "abstract": "Currently, third-generation sequencing techniques, which allow to obtain much\nlonger DNA reads compared to the next-generation sequencing technologies, are\nbecoming more and more popular. There are many possibilities to combine data\nfrom next-generation and third-generation sequencing.\n  Herein, we present a new application called dnaasm-link for linking contigs,\na result of \\textit{de novo} assembly of second-generation sequencing data,\nwith long DNA reads. Our tool includes an integrated module to fill gaps with a\nsuitable fragment of appropriate long DNA read, which improves the consistency\nof the resulting DNA sequences. This feature is very important, in particular\nfor complex DNA regions, as presented in the paper. Finally, our implementation\noutperforms other state-of-the-art tools in terms of speed and memory\nrequirements, which may enable the usage of the presented application for\norganisms with a large genome, which is not possible in~existing applications.\n  The presented application has many advantages as (i) significant memory\noptimization and reduction of computation time (ii) filling the gaps through\nthe appropriate fragment of a specified long DNA read (iii) reducing number of\nspanned and unspanned gaps in the existing genome drafts.\n  The application is freely available to all users under GNU Library or Lesser\nGeneral Public License version 3.0 (LGPLv3). The demo application, docker image\nand source code are available at http://dnaasm.sourceforge.net."}, {"title": "DNA methylation markers to assess biological age", "abstract": "Among the different biomarkers of aging based on omics and clinical data, DNA\nmethylation clocks stand apart providing unmatched accuracy in assessing the\nbiological age of both humans and animal models of aging. Here, we discuss\nrobustness of DNA methylation clocks and bounds on their out-of-sample\nperformance and review computational strategies for development of the clocks."}, {"title": "JS-MA: A Jensen-Shannon Divergence Based Method for Mapping Genome-wide Associations on Multiple Diseases", "abstract": "Taking advantages of high-throughput genotyping technology of single\nnucleotide polymorphism (SNP), large genome-wide association studies (GWASs)\nhave been considered as the promise to unravel the complex relationships\nbetween genotypes and phenotypes, in particularly common diseases. However,\ncurrent multi-locus-based methods are insufficient, in terms of computational\ncost and discrimination power, to detect statistically significant interactions\nand they are lacking in the ability of finding diverse genetic effects on\nmultifarious diseases. Especially, multiple statistic tests for high-order\nepistasis ($ \\geq $ 2 SNPs) will raise huge analytical challenges because the\ncomputational cost increases exponentially as the growth of the cardinality of\nSNPs in an epistatic module. In this paper, we develop a simple, fast and\npowerful method, named JS-MA, using the Jensen-Shannon divergence and a\nhigh-dimensional $ k $-mean clustering algorithm for mapping the genome-wide\nmulti-locus epistatic interactions on multiple diseases. Compared with some\nstate-of-the-art association mapping tools, our method is demonstrated to be\nmore powerful and efficient from the experimental results on the systematical\nsimulations. We also applied JS-MA to the GWAS datasets from WTCCC for two\ncommon diseases, i.e. Rheumatoid Arthritis and Type 1 Diabetes. JS-MA not only\nconfirms some recently reported biologically meaningful associations but also\nidentifies some novel findings. Therefore, we believe that our method is\nsuitable and efficient for the full-scale analysis of multi-disease-related\ninteractions in the large GWASs."}, {"title": "A Multi-Trait Approach Identified Genetic Variants Including a Rare Mutation in RGS3 with Impact on Abnormalities of Cardiac Structure/Function", "abstract": "Heart failure is a major cause for premature death. Given heterogeneity of\nthe heart failure syndrome, identifying genetic determinants of cardiac\nfunction and structure may provide greater insights into heart failure. Despite\nprogress in understanding the genetic basis of heart failure through genome\nwide association studies, heritability of heart failure is not well understood.\nGaining further insights into mechanisms that contribute to heart failure\nrequires systematic approaches that go beyond single trait analysis. We\nintegrated Bayesian multi-trait approach and Bayesian networks for the analysis\nof 10 correlated traits of cardiac structure and function measured for 3387\nindividuals with whole exome sequence data. While using single-trait based\napproaches did not find any significant genetic variant, applying the\nintegrative Bayesian multi-trait approach, we identified 3 novel variants\nlocated in genes, RGS3, CHD3, and MRPL38 with significant impact on the cardiac\ntraits such as left ventricular volume index, parasternal long axis\ninterventricular septum thickness, and mean left ventricular wall thickness.\nAmong these, the rare variant NC_000009.11:g.116346115C>A (rs144636307) in RGS3\nshowed pleiotropic effect on left ventricular mass index, left ventricular\nvolume index and Maximum left atrial anterior-posterior diameter while RGS3 can\ninhibit TGF-beta signaling associated with left ventricle dilation and systolic\ndysfunction."}, {"title": "Private Shotgun DNA Sequencing", "abstract": "Current techniques in sequencing a genome allow a service provider (e.g. a\nsequencing company) to have full access to the genome information, and thus the\nprivacy of individuals regarding their lifetime secret is violated. In this\npaper, we introduce the problem of private DNA sequencing, where the goal is to\nkeep the DNA sequence private to the sequencer. We propose an architecture,\nwhere the task of reading fragments of DNA and the task of DNA assembly are\nseparated, the former is done at the sequencer(s), and the later is completed\nat a local trusted data collector. To satisfy the privacy constraint at the\nsequencer and reconstruction condition at the data collector, we create an\ninformation gap between these two relying on two techniques: (i) we use more\nthan one non-colluding sequencer, all reporting the read fragments to the\nsingle data collector, (ii) adding the fragments of some known DNA molecules,\nwhich are still unknown to the sequencers, to the pool. We prove that these two\ntechniques provide enough freedom to satisfy both conditions at the same time."}, {"title": "GenHap: A Novel Computational Method Based on Genetic Algorithms for Haplotype Assembly", "abstract": "The computational problem of inferring the full haplotype of a cell starting\nfrom read sequencing data is known as haplotype assembly, and consists in\nassigning all heterozygous Single Nucleotide Polymorphisms (SNPs) to exactly\none of the two chromosomes. Indeed, the knowledge of complete haplotypes is\ngenerally more informative than analyzing single SNPs and plays a fundamental\nrole in many medical applications. To reconstruct the two haplotypes, we\naddressed the weighted Minimum Error Correction (wMEC) problem, which is a\nsuccessful approach for haplotype assembly. This NP-hard problem consists in\ncomputing the two haplotypes that partition the sequencing reads into two\ndisjoint sub-sets, with the least number of corrections to the SNP values. To\nthis aim, we propose here GenHap, a novel computational method for haplotype\nassembly based on Genetic Algorithms, yielding optimal solutions by means of a\nglobal search process. In order to evaluate the effectiveness of our approach,\nwe run GenHap on two synthetic (yet realistic) datasets, based on the Roche/454\nand PacBio RS II sequencing technologies. We compared the performance of GenHap\nagainst HapCol, an efficient state-of-the-art algorithm for haplotype phasing.\nOur results show that GenHap always obtains high accuracy solutions (in terms\nof haplotype error rate), and is up to 4x faster than HapCol in the case of\nRoche/454 instances and up to 20x faster when compared on the PacBio RS II\ndataset. Finally, we assessed the performance of GenHap on two different real\ndatasets. Future-generation sequencing technologies, producing longer reads\nwith higher coverage, can highly benefit from GenHap, thanks to its capability\nof efficiently solving large instances of the haplotype assembly problem."}, {"title": "De novo inference of diversity genes and analysis of non-canonical V(DD)J recombination in immunoglobulins", "abstract": "The V(D)J recombination forms the immunoglobulin genes by joining the\nvariable (V), diversity (D), and joining (J) germline genes. Since variations\nin germline genes have been linked to various diseases, personalized\nimmunogenomics aims at finding alleles of germline genes across various\npatients. Although recent studies described algorithms for de novo inference of\nV and J genes from immunosequencing data, they stopped short of solving a more\ndifficult problem of reconstructing D genes that form the highly divergent CDR3\nregions and provide the most important contribution to the antigen binding. We\npresent the IgScout algorithm for de novo D gene reconstruction and apply it to\nreveal new alleles of human D genes and previously unknown D genes in camel, an\nimportant model organism in immunology. We further analyze non-canonical V(DD)J\nrecombination that results in unusually long tandem CDR3s and thus expands the\ndiversity of the antibody repertoires. We demonstrate that tandem CDR3s\nrepresent a consistent and functional feature of all analyzed immunosequencing\ndatasets, reveal ultra-long tandem CDR3s, and shed light on the mechanism\nresponsible for their formation."}, {"title": "A Hybrid HMM Approach for the Dynamics of DNA Methylation", "abstract": "The understanding of mechanisms that control epigenetic changes is an\nimportant research area in modern functional biology. Epigenetic modifications\nsuch as DNA methylation are in general very stable over many cell divisions.\nDNA methylation can however be subject to specific and fast changes over a\nshort time scale even in non-dividing (i.e. not-replicating) cells. Such\ndynamic DNA methylation changes are caused by a combination of active\ndemethylation and de novo methylation processes which have not been\ninvestigated in integrated models. Here we present a hybrid (hidden) Markov\nmodel to describe the cycle of methylation and demethylation over (short) time\nscales. Our hybrid model decribes several molecular events either happening at\ndeterministic points (i.e. describing mechanisms that occur only during cell\ndivision) and other events occurring at random time points. We test our model\non mouse embryonic stem cells using time-resolved data. We predict methylation\nchanges and estimate the efficiencies of the different modification steps\nrelated to DNA methylation and demethylation."}, {"title": "Identifying centromeric satellites with dna-brnn", "abstract": "Summary: Human alpha satellite and satellite 2/3 contribute to several\npercent of the human genome. However, identifying these sequences with\ntraditional algorithms is computationally intensive. Here we develop dna-brnn,\na recurrent neural network to learn the sequences of the two classes of\ncentromeric repeats. It achieves high similarity to RepeatMasker and is times\nfaster. Dna-brnn explores a novel application of deep learning and may\naccelerate the study of the evolution of the two repeat classes.\n  Availability and implementation: https://github.com/lh3/dna-nn\n  Contact: hli@jimmy.harvard.edu"}, {"title": "Proteomic and metagenomic insights into prehistoric Spanish Levantine Rock Art", "abstract": "The Iberian Mediterranean Basin is home to one of the largest groups of\nprehistoric rock art sites in Europe. Despite the cultural relevance of\nprehistoric Spanish Levantine rock art, pigment composition remains partially\nunknown, and the nature of the binders used for painting has yet to be\ndisclosed. In this work, we present the first omic analysis applied to one of\nthe flagship Levantine rock art sites: the Valltorta ravine (Castell{\\'o}n,\nSpain). We used high-throughput sequencing to provide the first description of\nthe bacterial communities colonizing the rock art patina, which proved to be\ndominated by Firmicutes species and might have a protective effect on the\npaintings. Proteomic analysis was also performed on rock art microsamples in\norder to determine the organic binders present in Levantine prehistoric rock\nart pigments. This information could shed light on the controversial dating of\nthis UNESCO Cultural Heritage, and contribute to defining the chrono-cultural\nframework of the societies responsible for these paintings."}, {"title": "Predicting Toxicity from Gene Expression with Neural Networks", "abstract": "We train a neural network to predict chemical toxicity based on gene\nexpression data. The input to the network is a full expression profile\ncollected either in vitro from cultured cells or in vivo from live animals. The\noutput is a set of fine grained predictions for the presence of a variety of\npathological effects in treated animals. When trained on the Open TG-GATEs\ndatabase it produces good results, outperforming classical models trained on\nthe same data. This is a promising approach for efficiently screening chemicals\nfor toxic effects, and for more accurately evaluating drug candidates based on\npreclinical data."}, {"title": "BOAssembler: a Bayesian Optimization Framework to Improve RNA-Seq Assembly Performance", "abstract": "High throughput sequencing of RNA (RNA-Seq) can provide us with millions of\nshort fragments of RNA transcripts from a sample. How to better recover the\noriginal RNA transcripts from those fragments (RNA-Seq assembly) is still a\ndifficult task. For example, RNA-Seq assembly tools typically require\nhyper-parameter tuning to achieve good performance for particular datasets.\nThis kind of tuning is usually unintuitive and time-consuming. Consequently,\nusers often resort to default parameters, which do not guarantee consistent\ngood performance for various datasets.\n  Here we propose BOAssembler (https://github.com/olivomao/boassembler), a\nframework that enables end-to-end automatic tuning of RNA-Seq assemblers, based\non Bayesian Optimization principles. Experiments show this data-driven approach\nis effective to improve the overall assembly performance. The approach would be\nhelpful for downstream (e.g. gene, protein, cell) analysis, and more broadly,\nfor future bioinformatics benchmark studies."}, {"title": "Using sequencing coverage statistics to identify sex chromosomes in minke whales", "abstract": "The ever-increasing number of genome sequencing and resequencing projects is\na central source of insights into the ecology and evolution of non-model\norganisms. An important aspect of genomics is the elucidation of sex\ndetermination systems and identifying genes on sex chromosomes. This not only\nhelps reveal mechanisms behind sex determination in the species under study,\nbut their characteristics make sex chromosomes a unique tool for studying the\nmechanisms and effects of recombination and genomic rearrangements and how they\naffect adaption and selection. Despite this, many sequencing projects omit such\ninvestigations. Here, we apply a simple method using sequencing coverage\nstatistics to identify scaffolds belonging to the sex chromosomes of minke\nwhale, and show how the sex chromosome system can be determined using coverage\nstatistics alone. Using publicly available data, we identify the previously\nunknown sex of an Antarctic minke whale as female. We further investigate\npublic sequence data from the different species and sub-species of minke whale,\nand classify genomic scaffolds from a published minke whale assembly as X or Y\nchromosomal sequences. Our findings are consistent with previous results that\nidentified a handful of scaffolds as sex chromosomal, but we are able to\nidentify a much larger set of scaffolds, likely to represent close to the\ncomplete sex chromosomal sequences for the minke whale. Sequence coverage\nstatistics provides a readily available tool for investigating the sex\ndetermination system and locate genes on sex chromosomes. This analysis is\nstraightforward and can often be performed with existing resources."}, {"title": "HIV-1 virus cycle replication: a review of RNA polymerase II transcription, alternative splicing and protein synthesis", "abstract": "HIV virus replication is a time-related process that includes several stages.\nFocusing on the core steps, RNA polymerase II transcripts in an early stage\npre-mRNA containing regulator proteins (i.e nef,tat,rev,vif,vpr,vpu), which are\ncompletely spliced by the spliceosome complex (0.9kb and 1.8kb) and exported to\nthe ribosome for protein synthesis. These splicing and export processes are\nregulated by tat protein, which binds on Trans-activation response (TAR)\nelement, and by rev protein, which binds to the Rev-responsive Element (RRE).\nAs long as these regulators are synthesized, splicing is progressively\ninhibited (from 4.0kb to 9.0kb) and mRNAs are translated into structural and\nenzymatic proteins (env, gag-pol). During this RNAPII scanning and splicing,\naround 40 different multi-cystronic mRNA have been produced. Long-read\nsequencing has been applied to the HIV-1 virus genome (type HXB2CG) with the\nHIV.pro software, a fortran 90 code for simulating the virus replication cycle,\nspecially RNAPII transcription, exon/intron splicing and ribosome protein\nsynthesis, including the frameshift at gag/pol gene and the ribosome pause at\nenv gene. All HIV-1 virus proteins have been identified as far as other ORFs.\nAs observed, tat/rev protein regulators have different length depending on the\nsplicing cleavage site: tat protein varies from 224aa to a final state of 72aa,\nwhereas rev protein from 25aa to 27aa, with a maximum of 119aa. Furthermore,\nseveral ORFs coding for small polypeptides sPEP (less than 10 amino acids) and\nfor other unidentified proteins have been localised with unknown functionality.\nThe detailed analysis of the HIV virus replication and the virus proteomics are\nimportant for identifying which antigens are presented by macrophages to CD4\ncells, for localizing reactive epitopes or for creating transfer vectors to\ndevelop new HIV vaccines and effective therapies."}, {"title": "pdbmine: A Node.js API for the RCSB Protein Data Bank (PDB)", "abstract": "Summary: The advent of Web-based tools that assist in the analysis and\nvisualization of macromolecules require application programming interfaces\n(APIs) designed for modern web frameworks. To this end, we have developed a\nNode.js module pdbmine that allows any user to generate faster data-request\nqueries to the RCSB Protein Data Bank (PDB). This JavaScript API acts as a\nlayer over the XML-based RCSB PDB RESTful API. The relatively simple nature of\nthe function calls within this module allows the user to easily implement and\nintegrate pdbmine into larger Node.js web applications.\n  Availability: This module can be installed via the Node Package Manager (NPM)\nat https://www.npmjs.com/package/pdbmine/, and is hosted on GitHub under the\nopen-source MIT license at https://github.com/nnj1/pdbmine/. Relevant\ndocumentation is detailed at https://nnj1.github.io/pdbmine/"}, {"title": "Statistical methods for the quantitative genetic analysis of high-throughput phenotyping data", "abstract": "The advent of plant phenomics, coupled with the wealth of genotypic data\ngenerated by next-generation sequencing technologies, provides exciting new\nresources for investigations into and improvement of complex traits. However,\nthese new technologies also bring new challenges in quantitative genetics,\nnamely, a need for the development of robust frameworks that can accommodate\nthese high-dimensional data. In this chapter, we describe methods for the\nstatistical analysis of high-throughput phenotyping (HTP) data with the goal of\nenhancing the prediction accuracy of genomic selection (GS). Following the\nIntroduction in Section 1, Section 2 discusses field-based HTP, including the\nuse of unmanned aerial vehicles and light detection and ranging, as well as how\nwe can achieve increased genetic gain by utilizing image data derived from HTP.\nSection 3 considers extending commonly used GS models to integrate HTP data as\ncovariates associated with the principal trait response, such as yield.\nParticular focus is placed on single-trait, multi-trait, and genotype by\nenvironment interaction models. One unique aspect of HTP data is that phenomics\nplatforms often produce large-scale data with high spatial and temporal\nresolution for capturing dynamic growth, development, and stress responses.\nSection 4 discusses the utility of a random regression model for performing\nlongitudinal GS. The chapter concludes with a discussion of some standing\nissues."}, {"title": "A bioinformatics pipeline for the identification of CHO cell differential gene expression from RNA-Seq data", "abstract": "In recent years the publication of genome sequences for the Chinese hamster\nand Chinese hamster ovary (CHO) cell lines have facilitated study of these\nbiopharmaceutical cell factories with unprecedented resolution. Our\nunderstanding of the CHO cell transcriptome, in particular, has rapidly\nadvanced through the application of next-generation sequencing (NGS) technology\nto characterise RNA expression (RNA-Seq). In this chapter we present a\ncomputational pipeline for the analysis of CHO cell RNA-Seq data from the\nIllumina platform to identify differentially expressed genes. The example data\nand bioinformatics workflow required to run this analysis are freely available\nat www.cgcdb.org/rnaseq_analysis_protocol.html."}, {"title": "RACS: Rapid Analysis of ChIP-Seq data for contig based genomes", "abstract": "Background: Chromatin immunoprecipitation coupled to next generation\nsequencing (ChIP-Seq) is a widely used technique to investigate the function of\nchromatin-related proteins in a genome-wide manner. ChIP-Seq generates large\nquantities of data which can be difficult to process and analyse, particularly\nfor organisms with contig based genomes. Contig-based genomes often have poor\nannotations for cis-elements, for example enhancers, that are important for\ngene expression. Poorly annotated genomes make a comprehensive analysis of\nChIP-Seq data difficult and as such standardized analysis pipelines are\nlacking. Methods: We report a computational pipeline that utilizes traditional\nHigh-Performance Computing techniques and open source tools for processing and\nanalysing data obtained from ChIP-Seq. We applied our computational pipeline\n\"Rapid Analysis of ChIP-Seq data\" (RACS) to ChIP-Seq data that was generated in\nthe model organism Tetrahymena thermophila, an example of an organism with a\ngenome that is available in contigs. Results: To test the performance and\nefficiency of RACs, we performed control ChIP-Seq experiments allowing us to\nrapidly eliminate false positives when analyzing our previously published data\nset. Our pipeline segregates the found read accumulations between genic and\nintergenic regions and is highly efficient for rapid downstream analyses.\nConclusions: Altogether, the computational pipeline presented in this report is\nan efficient and highly reliable tool to analyze genome-wide ChIP-Seq data\ngenerated in model organisms with contig-based genomes.\n  RACS is an open source computational pipeline available to download from:\nhttps://bitbucket.org/mjponce/racs --or--\nhttps://gitrepos.scinet.utoronto.ca/public/?a=summary&p=RACS"}, {"title": "RNASeqR: an R package for automated two-group RNA-Seq analysis workflow", "abstract": "RNA-Seq analysis has revolutionized researchers' understanding of the\ntranscriptome in biological research. Assessing the differences in\ntranscriptomic profiles between tissue samples or patient groups enables\nresearchers to explore the underlying biological impact of transcription.\nRNA-Seq analysis requires multiple processing steps and huge computational\ncapabilities. There are many well-developed R packages for individual steps;\nhowever, there are few R/Bioconductor packages that integrate existing software\ntools into a comprehensive RNA-Seq analysis and provide fundamental end-to-end\nresults in pure R environment so that researchers can quickly and easily get\nfundamental information in big sequencing data. To address this need, we have\ndeveloped the open source R/Bioconductor package, RNASeqR. It allows users to\nrun an automated RNA-Seq analysis with only six steps, producing essential\ntabular and graphical results for further biological interpretation. The\nfeatures of RNASeqR include: six-step analysis, comprehensive visualization,\nbackground execution version, and the integration of both R and command-line\nsoftware. RNASeqR provides fast, light-weight, and easy-to-run RNA-Seq analysis\npipeline in pure R environment. It allows users to efficiently utilize popular\nsoftware tools, including both R/Bioconductor and command-line tools, without\npredefining the resources or environments. RNASeqR is freely available for\nLinux and macOS operating systems from Bioconductor\n(https://bioconductor.org/packages/release/bioc/html/RNASeqR.html)."}, {"title": "Tumor Microenvironment-based Gene Signatures Divides Novel Immune and Stromal Subgroup Classification of Lung Adenocarcinoma", "abstract": "Tumor microenvironment has complex effects on tumorigenesis and metastasis.\nHowever, there is still a lack of comprehensive understanding of the\nrelationship among molecular and cellular characteristics in tumor\nmicroenvironment, clinical prognosis and immunotherpy response. In this study,\nthe immune and stromal (non-immune) signatures of tumor microenvironment were\nintegrated to identify novel subgroups of lung adenocarcinoma by\neigendecomposition and extraction algorithms of bioinformatics and machine\nlearning, such as non-negative matrix factorization and multitask learning.\nTumors were classified into 4 groups according to the activation of immunity\nand stroma by novel signatures. The 4 groups had different mutation landscape,\nmolecular, cellular characteristics and prognosis, which have been validation\nin 6 independent data sets containing 1551 patients. High-immune and\nlow-stromal activation group links to high immunocyte infiltration, high\nimmunocompetence, low fibroblasts, endothelial cells, collagen, laminin, tumor\nmutation burden, and better overall survival. We developed a novel model based\non tumor microenvironment by integrating immune and stromal activation, namely\nPMBT (prognostic model based on tumor microenvironment). The PMBT showed the\nvalue to predict overall survival and immunotherapy responses."}, {"title": "ReadsMap: a new tool for high precision mapping of DNAseq and RNAseq read sequences", "abstract": "There are currently plenty of programs available for mapping short sequences\n(reads) to a genome. Most of them, however, including such popular and actively\ndeveloped programs as Bowtie, BWA, TopHat and many others, are based on\nBurrows-Wheeler Transform (BWT) algorithm. This approach is very effective for\nmapping high-homology reads, but runs into problems when mapping reads with\nhigh level of errors or SNP. Also it has problems with mapping RNASeq spliced\nreads (such as reads that aligning with gaps corresponding intron sequences),\nthe kind that is essential for finding introns and alternative splicing gene\nisoforms. Meanwhile, finding intron positions is the most important task for\ndetermining the gene structure, and especially alternatively spliced variants\nof genes. In this paper, we propose a new algorithm that involves hashing\nreference genome. ReadsMap program, implementing such algorithm, demonstrate\nvery high-accuracy mapping of large number of short reads to one or more\ngenomic contigs. It is achieved mostly by better alignment of very short parts\nof reads separated by long introns with accounting information from mapping\nother reads containing the same intron inserted between bigger blocks.\nAvailability and implementation: ReadsMap is implemented in C. It is\nincorporated in Fgenesh++ gene identification pipeline and is freely available\nto academic users at Softberry web server www.softberry.com."}, {"title": "Transcriptomic Causal Networks identified patterns of differential gene regulation in human brain from Schizophrenia cases versus controls", "abstract": "Common and complex traits are the consequence of the interaction and\nregulation of multiple genes simultaneously, which work in a coordinated way.\nHowever, the vast majority of studies focus on the differential expression of\none individual gene at a time. Here, we aim to provide insight into the\nunderlying relationships of the genes expressed in the human brain in cases\nwith schizophrenia (SCZ) and controls. We introduced a novel approach to\nidentify differential gene regulatory patterns and identify a set of essential\ngenes in the brain tissue. Our method integrates genetic, transcriptomic, and\nHi-C data and generates a transcriptomic-causal network. Employing this\napproach for analysis of RNA-seq data from CommonMind Consortium, we identified\ndifferential regulatory patterns for SCZ cases and control groups to unveil the\nmechanisms that control the transcription of the genes in the human brain. Our\nanalysis identified modules with a high number of SCZ-associated genes as well\nas assessing the relationship of the hubs with their down-stream genes in both,\ncases and controls. In addition, the results identified essential genes for\nbrain function and suggested new genes putatively related to SCZ."}, {"title": "A multi-modal neural network for learning cis and trans regulation of stress response in yeast", "abstract": "Deciphering gene regulatory networks is a central problem in computational\nbiology. Here, we explore the use of multi-modal neural networks to learn\npredictive models of gene expression that include cis and trans regulatory\ncomponents. We learn models of stress response in the budding yeast\nSaccharomyces cerevisiae. Our models achieve high performance and substantially\noutperform other state-of-the-art methods such as boosting algorithms that use\npre-defined cis-regulatory features. Our model learns several cis and trans\nregulators including well-known master stress response regulators. We use our\nmodels to perform in-silico TF knock-out experiments and demonstrate that\nin-silico predictions of target gene changes correlate with the results of the\ncorresponding TF knockout microarray experiment."}, {"title": "Reading tea leaves? Polygenic scores and differences in traits among groups", "abstract": "In the past decade, Genome-Wide Association Studies (GWAS) have delivered an\nincreasingly broad view of the genetic basis of human phenotypic variation. One\nof the major developments from GWAS is polygenic scores, a genetic predictor of\nan individual's genetic predisposition towards a trait constructed from GWAS.\nThe success of GWAS and polygenic scores seems to suggest that we will soon be\nable to settle debates about whether phenotypic differences among groups are\ndriven in part by genetics. However, answering these questions is more\ncomplicated than it seems at first glance and touches on many old issues about\nthe interpretation of human genetic variation. In this perspective piece, I\noutline the ways in which issues of causality, stratification,\ngene-by-environment interactions, and divergence among groups all complicate\nthe interpretation of among-population polygenic score differences."}, {"title": "Organizing genome engineering for the gigabase scale", "abstract": "Engineering the entire genome of an organism enables large-scale changes in\norganization, function, and external interactions, with significant\nimplications for industry, medicine, and the environment. Improvements to DNA\nsynthesis and organism engineering are already enabling substantial changes to\norganisms with megabase genomes, such as Escherichia coli and Saccharomyces\ncerevisiae. Simultaneously, recent advances in genome-scale modeling are\nincreasingly informing the design of metabolic networks. However, major\nchallenges remain for integrating these and other relevant technologies into\nworkflows that can scale to the engineering of gigabase genomes.\n  In particular, we find that a major under-recognized challenge is\ncoordinating the flow of models, designs, constructs, and measurements across\nthe large teams and complex technological systems that will likely be required\nfor gigabase genome engineering. We recommend that the community address these\nchallenges by 1) adopting and extending existing standards and technologies for\nrepresenting and exchanging information at the gigabase genomic scale, 2)\ndeveloping new technologies to address major open questions around data\ncuration and quality control, 3) conducting fundamental research on the\nintegration of modeling and design at the genomic scale, and 4) developing new\nlegal and contractual infrastructure to better enable collaboration across\nmultiple institutions."}, {"title": "Autism spectrum disorder: a neuro-immunometabolic hypothesis of the developmental origins", "abstract": "Fetal neuroinflammation and prenatal stress (PS) may contribute to lifelong\nneurological disabilities. Astrocytes and microglia, among the brain's\nnon-neuronal glia cell populations, play a pivotal role in neurodevelopment,\npredisposition to and initiation of disease throughout lifespan. One of the\nmost common neurodevelopmental disorders manifesting between 1-4 years of age\nis autism spectrum disorder (ASD). A pathological glial-neuronal interplay is\nthought to increase the risk for clinical manifestation of ASD in at-risk\nchildren, but the mechanisms remain poorly understood and integrative,\nmulti-scale models are needed. We propose a model that integrates the data\nacross the scales of physiological organization, from genome to phenotype, and\nprovides a foundation to explain the disparate findings on the genomic level.\nWe hypothesize that via gene-environment interactions, fetal neuroinflammation\nand PS may reprogram glial immunometabolic phenotypes that impact\nneurodevelopment and neurobehavior. Drawing on genomic data from the recently\npublished series of ovine and rodent glial transcriptome analyses with fetuses\nexposed to neuroinflammation or PS, we conduct an analysis on the Simons\nFoundation Autism Research Initiative (SFARI) Gene database. We confirm 21 gene\nhits. Using unsupervised statistical network analysis, we then identify six\nclusters of probable protein-protein interactions mapping onto the\nimmunometabolic and stress response networks and epigenetic memory. These\nfindings support our hypothesis. We discuss the implications for ASD etiology,\nearly detection, and novel therapeutic approaches. We conclude with delineation\nof the next steps to verify our model on the individual gene level in an\nassumption-free manner."}, {"title": "Whole genome sequencing identifies putative associations between genomic polymorphisms and clinical response to the antiepileptic drug levetiracetam", "abstract": "In the context of pharmacogenomics, whole genome sequencing provides a\npowerful approach for identifying correlations between response variability to\nspecific drugs and genomic polymorphisms in a population, in an unbiased\nmanner. In this study, we employed whole genome sequencing of DNA samples from\npatients showing extreme response (n=72) and non-response (n=27) to the\nantiepileptic drug levetiracetam, in order to identify genomic variants that\nunderlie response to the drug. Although no common SNP (MAF>5%) crossed the\nconventional genome-wide significance threshold of 5e-8, we found common\npolymorphisms in genes SPNS3, HDC, MDGA2, NSG1 and RASGEF1C, which collectively\npredict clinical response to levetiracetam in our cohort with ~91% predictive\naccuracy. Among these genes, HDC, NSG1, MDGA2 and RASGEF1C are potentially\nimplicated in synaptic neurotransmission, while SPNS3 is an atypical solute\ncarrier transporter homologous to SV2A, the known molecular target of\nlevetiracetam. Furthermore, we performed gene- and pathway-based statistical\nanalysis on sets of rare and low-frequency variants (MAF<5%) and we identified\nassociations between the following genes or pathways and response to\nlevetiracetam: a) genes PRKCB and DLG2, which are involved in glutamatergic\nneurotransmission, a known target of anticonvulsants, including levetiracetam;\nb) genes FILIP1 and SEMA6D, which are involved in axon guidance and modelling\nof neural connections; and c) pathways with a role in synaptic\nneurotransmission, such as WNT5A-dependent internalization of FZD4 and\ndisinhibition of SNARE formation. In summary, our approach to utilise whole\ngenome sequencing on subjects with extreme response phenotypes is a feasible\nroute to generate plausible hypotheses for investigating the genetic factors\nunderlying drug response variability in cases of pharmaco-resistant epilepsy."}, {"title": "AFDP: An Automated Function Description Prediction Approach to Improve Accuracy of Protein Function Predictions", "abstract": "With the rapid growth in high-throughput biological sequencing technologies\nand subsequently the amount of produced omics data, it is essential to develop\nautomated methods to annotate the functionality of unknown genes and proteins.\nThere are developed tools such as AHRD applying known proteins characterization\nto annotate unknown ones. Some other algorithms such as eggNOG apply\northologous groups of proteins to detect the most probable function. However,\nwhile the available tools focus on the detection of the most similar\ncharacterization, they are not able to generalize and integrate information\nfrom multiple homologs while maintaining accuracy. Here, we devise AFDP, an\nintegrated approach for protein function prediction which benefits from the\ncombination of two available tools, AHRD and eggNOG, to predict the\nfunctionality of novel proteins and produce more precise human readable\ndescriptions by applying our stCFExt algorithm. StCFExt creates function\ndescriptions applying available manually curated descriptions in swiss-prot.\nUsing a benchmark dataset we show that the annotations predicted by our\napproach are more accurate than eggNOG and AHRD annotations."}, {"title": "A Stochastic Automata Network Description for Spatial DNA-Methylation Models", "abstract": "DNA methylation is an important biological mechanism to regulate gene\nexpression and control cell development. Mechanistic modeling has become a\npopular approach to enhance our understanding of the dynamics of methylation\npattern formation in living cells. Recent findings suggest that the methylation\nstate of a cytosine base can be influenced by its DNA neighborhood. Therefore,\nit is necessary to generalize existing mathematical models that consider only\none cytosine and its partner on the opposite DNA-strand (CpG), in order to\ninclude such neighborhood dependencies. One approach is to describe the system\nas a stochastic automata network (SAN) with functional transitions. We show\nthat single-CpG models can successfully be generalized to multiple CpGs using\nthe SAN description and verify the results by comparing them to results from\nextensive Monte-Carlo simulations."}, {"title": "CD44 alternative splicing is a sensor of intragenic DNA methylation in tumors", "abstract": "DNA methylation (meDNA) is a suspected modulator of alternative splicing,\nwhile splicing in turn is involved in tumour formations nearly as frequently as\nDNA mutations. Yet, the impact of meDNA on tumorigenesis via its effect on\nsplicing has not been thoroughly explored. Here, we find that HCT116 colon\ncarcinoma cells inactivated for the DNA methylases DNMT1 and DNMT3b undergo a\npartial epithelial to mesenchymal transition (EMT) associated with alternative\nsplicing of the CD44 transmembrane receptor. The skipping of CD44 variant exons\nis in part explained by altered expression or splicing of splicing and\nchromatin factors. A direct effect of meDNA on alternative splicing was\nsustained by transient depletion of DNMT1 and the methyl-binding genes MBD1,\nMBD2, and MBD3. Yet, local changes in intragenic meDNA also altered recruitment\nof MBD1 protein and of the chromatin factor HP1$\\gamma$ known to alter\ntranscriptional pausing and alternative splicing decisions. We further tested\nif meDNA level has sufficiently strong direct impact on the outcome of\nalternative splicing to have a predictive value in the MCF10A model for breast\ncancer progression and in patients with acute lymphoblastic leukemia (B ALL).\nWe found that a small number of differentially spliced genes mostly involved in\nsplicing and signal transduction is systematically correlated with local meDNA.\nAltogether, our observations suggest that, although DNA methylation has\nmultiple avenues to alternative splicing, its indirect effect may be also\nmediated through alternative splicing isoforms of these sensors of meDNA."}, {"title": "Primary and secondary anti-viral response captured by the dynamics and phenotype of individual T cell clones", "abstract": "The diverse repertoire of T-cell receptors (TCR) plays a key role in the\nadaptive immune response to infections. Previous studies show that secondary\nresponses to the yellow fever vaccine - the model for acute infection in humans\n- are weaker than primary ones, but only quantitative measurements can describe\nthe concentration changes and lineage fates for distinct T-cell clones in vivo\nover time. Using TCR alpha and beta repertoire sequencing for T-cell subsets,\nas well as single-cell RNAseq and TCRseq, we track the concentrations and\nphenotypes of individual T-cell clones in response to primary and secondary\nyellow fever immunization showing their large diversity. We confirm the\nsecondary response is an order of magnitude weaker, albeit $\\sim10$ days faster\nthan the primary one. Estimating the fraction of the T-cell response directed\nagainst the single immunodominant epitope, we identify the sequence features of\nTCRs that define the high precursor frequency of the two major TCR motifs\nspecific for this particular epitope. We also show the consistency of clonal\nexpansion dynamics between bulk alpha and beta repertoires, using a new\nmethodology to reconstruct alpha-beta pairings from clonal trajectories."}, {"title": "Guidelines for reporting single-cell RNA-Seq experiments", "abstract": "Single-cell RNA-Sequencing (scRNA-Seq) has undergone major technological\nadvances in recent years, enabling the conception of various organism-level\ncell atlassing projects. With increasing numbers of datasets being deposited in\npublic archives, there is a need to address the challenges of enabling the\nreproducibility of such data sets. Here, we describe guidelines for a minimum\nset of metadata to sufficiently describe scRNA-Seq experiments, ensuring\nreproducibility of data analyses."}, {"title": "Identification of Biomarkers Driving Blood Cell Development", "abstract": "A blood cell lineage consists of several consecutive developmental stages\nfrom the pluripotent or multipotent stem cell to a particular stage of\nterminally differentiated cells. There is considerable interest in identifying\nthe key regulatory genes that govern blood cell development from the gene\nexpression data without considering the underlying network between\ntranscription factors (TFs) and their target genes. In this study, we introduce\na novel expression pattern that key regulators expose along the differentiation\npath. We deploy this pattern to identify the cell-specific key regulators\nresponsible for the development. As proof of concept, we consider this approach\nto data on six developmental stages from mouse embryonic stem cells to\nterminally differentiated macrophages."}, {"title": "A Common Gene Expression Signature Analysis Method for Multiple Types of Cancer", "abstract": "Mining gene expression profiles has proven valuable for identifying\nsignatures serving as surrogates of cancer phenotypes. However, the\nsimilarities of such signatures across different cancer types have not been\nstrong enough to conclude that they represent a universal biological mechanism\nshared among multiple cancer types. Here we describe a network-based approach\nthat explores gene-to-gene connections in multiple cancer datasets while\nmaximizing the overall association of the subnetwork with clinical outcomes.\nWith the dataset of The Cancer Genome Atlas (TCGA), we studied the\ncharacteristics of common gene expression of three types of cancers: Rectum\nadenocarcinoma (READ), Breast invasive carcinoma (BRCA) and Colon\nadenocarcinoma (COAD). By analyzing several pairs of highly correlated genes\nafter filtering and clustering work, we found that the co-expressed genes\nacross multiple types of cancers point to particular biological mechanisms\nrelated to cancer cell progression , suggesting that they represent important\nattributes of cancer in need of being elucidated for potential applications in\ndiagnostic, prognostic and therapeutic products applicable to multiple cancer\ntypes."}, {"title": "De Novo Assembly of Uca minax Transcriptome from Next Generation Sequencing", "abstract": "High-throughput cDNA sequencing (RNA-seq) is a very powerful technique to\nquantify gene expression in an unbiased way. The Crustacean family is among the\ngroups of organisms sparsely represented in current genomic databases. Here we\npresent transcriptome data from Uca minax (red-jointed fiddler crab) as an\nopportunity to extend our knowledge. Next generation sequencing was performed\non six tissue samples from Uca minax using the Illumina HiSeq system. Six\nTranscriptome libraries were created using Trinity; a free, open-source\nsoftware tool for de novo transcriptome assembly of high-throughput mRNA\nsequencing (RNA-seq) data with the absence of a reference genome. In addition,\nseveral tools that aid in management of data were used, such as RSEM, Bowtie,\nBlast, and IGV; a tool for visualizing RNA-seq analysis results. Fast quality\ncontrol (FastQC) analysis of the raw sequenced files revealed that both adapter\nand PCR primer sequences were prevalently present, which may require a\npreprocessing step."}, {"title": "Deciphering the regulatory genome of $\\textit{Escherichia coli}$, one hundred promoters at a time", "abstract": "Advances in DNA sequencing have revolutionized our ability to read genomes.\nHowever, even in the most well-studied of organisms, the bacterium ${\\it\nEscherichia coli}$, for $\\approx$ 65$\\%$ of the promoters we remain completely\nignorant of their regulation. Until we have cracked this regulatory Rosetta\nStone, efforts to read and write genomes will remain haphazard. We introduce a\nnew method (Reg-Seq) linking a massively-parallel reporter assay and mass\nspectrometry to produce a base pair resolution dissection of more than 100\npromoters in ${\\it E. coli}$ in 12 different growth conditions. First, we show\nthat our method recapitulates regulatory information from known sequences.\nThen, we examine the regulatory architectures for more than 80 promoters in the\n${\\it E. coli}$ genome which previously had no known regulation. In many cases,\nwe also identify which transcription factors mediate their regulation. The\nmethod introduced here clears a path for fully characterizing the regulatory\ngenome of model organisms, with the potential of moving on to an array of other\nmicrobes of ecological and medical relevance."}, {"title": "DNA methylation heterogeneity induced by collaborations between enhancers", "abstract": "During mammalian embryo development, reprogramming of DNA methylation plays\nimportant roles in the erasure of parental epigenetic memory and the\nestablishment of na\\\"{i}ve pluripogent cells. Multiple enzymes that regulate\nthe processes of methylation and demethylation work together to shape the\npattern of genome-scale DNA methylation and guid the process of cell\ndifferentiation. Recent availability of methylome information from single-cell\nwhole genome bisulfite sequencing (scBS-seq) provides an opportunity to study\nDNA methylation dynamics in the whole genome in individual cells, which reveal\nthe heterogeneous methylation distributions of enhancers in embryo stem cells\n(ESCs). In this study, we developed a computational model of enhancer\nmethylation inheritance to study the dynamics of genome-scale DNA methylation\nreprogramming during exit from pluripotency. The model enables us to track\ngenome-scale DNA methylation reprogramming at single-cell level during the\nembryo development process, and reproduce the DNA methylation heterogeneity\nreported by scBS-seq. Model simulations show that DNA methylation heterogeneity\nis an intrinsic property driven by cell division along the development process,\nand the collaboration between neighboring enhancers is required for\nheterogeneous methylation. Our study suggest that the mechanism of genome-scale\noscillation proposed by Rulands et al. (2018) might not necessary to the DNA\nmethylation during exit from pluripotency."}, {"title": "Phylogenetic analyses of the severe acute respiratory syndrome coronavirus 2 reflected the several routes of introduction to Taiwan, the United States, and Japan", "abstract": "Worldwide Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)\ninfection is disrupting in the economy and anxiety of people. The public\nanxiety has increased the psychological burden on government and healthcare\nprofessionals, resulting in a government worker suicide in Japan. The terrified\npeople are asking the government for border measures. However, are border\nmeasures possible for this virus? By analyzing 48 almost complete virus genome\nsequences, we found out that the viruses that invaded Taiwan, the United\nStates, and Japan were introduced independently. We identified thirteen\nparsimony-informative sites and three groups (CTC, TCC, and TCT). Viruses found\noutside China did not form a monophyletic clade, opposite to previous study.\nThese results suggest the difficulty of implementing effective border measures\nagainst this virus."}, {"title": "Phylogenetic Study of 2019-nCoV by Using Alignment Free Method (Evolutionary Bifurcation of Novel Coronavirus Mutants)", "abstract": "The phylogenetic tree of SARS-CoV-2 (nCov-19) viruses is reconstructed\naccording to the similarity of genome sequences. The tree topology of\nBetacoronavirus is remarkably consistent with biologist's systematics. Because\nthe tree construction contains enough information about virus mutants, it is\nsuitable to study the evolutionary relationship between novel coronavirus\nmutants transmitted among humans. The emergences of 14 kinds of main mutants\nare studied and these strains can be classified as eight bifurcations of the\nphylogenetic tree. It is found that there exist three types of virus mutations,\nnamely, the mutation among sub-branches of the same branch, the off-root\nmutation and the root-oriented mutation between large branches of the tree.\nFrom the point of the relation between viral mutation and host selection we\nfound that individuals with low immunity provide a special environment for the\npositive natural selection of virus evolution. It gives a mechanism to explain\nwhy large mutations between two distant branches generally occur in the nCov-19\nphylogenetic tree. The finding is helpful to formulate strategies to control\nthe spread of COVID-19."}, {"title": "The design and construction of reference pangenome graphs", "abstract": "The recent advances in sequencing technologies enables the assembly of\nindividual genomes to the reference quality. How to integrate multiple genomes\nfrom the same species and to make the integrated representation accessible to\nbiologists remain an open challenge. Here we propose a graph-based data model\nand associated formats to represent multiple genomes while preserving the\ncoordinate of the linear reference genome. We implemented our ideas in the\nminigraph toolkit and demonstrate that we can efficiently construct a pangenome\ngraph and compactly encode tens of thousands of structural variants missing\nfrom the current reference genome."}, {"title": "Analysis of genetic differences between psychiatric disorders: Exploring pathways and cell-types/tissues involved and ability to differentiate the disorders by polygenic scores", "abstract": "Although displaying genetic correlations, psychiatric disorders are\nclinically defined as categorical entities as they each have distinguishing\nclinical features and may involve different treatments. Identifying\ndifferential genetic variations between these disorders may reveal how the\ndisorders differ biologically and help to guide more personalized treatment.\n  Here we presented a comprehensive analysis to identify genetic markers\ndifferentially associated with various psychiatric disorders/traits based on\nGWAS summary statistics, covering 18 psychiatric traits/disorders and 26\ncomparisons. We also conducted comprehensive analysis to unravel the genes,\npathways and SNP functional categories involved, and the cell types and tissues\nimplicated. We also assessed how well one could distinguish between psychiatric\ndisorders by polygenic risk scores (PRS).\n  SNP-based heritabilities (h2SNP) were significantly larger than zero for most\ncomparisons. Based on current GWAS data, PRS have mostly modest power to\ndistinguish between psychiatric disorders. For example, we estimated that AUC\nfor distinguishing schizophrenia from major depressive disorder (MDD), bipolar\ndisorder (BPD) from MDD and schizophrenia from BPD were 0.694, 0.602 and 0.618\nrespectively, while the maximum AUC (based on h2SNP) were 0.763, 0.749 and\n0.726 respectively. We also uncovered differences in each pair of studied\ntraits in terms of their differences in genetic correlation with comorbid\ntraits. For example, clinically-defined MDD appeared to more strongly\ngenetically correlated with other psychiatric disorders and heart disease, when\ncompared to non-clinically-defined depression in UK Biobank.\n  Our findings highlight genetic differences between psychiatric disorders and\nthe mechanisms involved. PRS may aid differential diagnosis of selected\npsychiatric disorders in the future with larger GWAS samples."}, {"title": "Genome Variant Calling with a Deep Averaging Network", "abstract": "Variant calling, the problem of estimating whether a position in a DNA\nsequence differs from a reference sequence, given noisy, redundant, overlapping\nshort sequences that cover that position, is fundamental to genomics. We\npropose a deep averaging network designed specifically for variant calling. Our\nmodel takes into account the independence of each short input read sequence by\ntransforming individual reads through a series of convolutional layers,\nlimiting the communication between individual reads to averaging and\nconcatenating operations. Training and testing on the precisionFDA Truth\nChallenge (pFDA), we match state of the art overall 99.89 F1 score. Genome\ndatasets exhibit extreme skew between easy examples and those on the decision\nboundary. We take advantage of this property to converge models at 5x the speed\nof standard epoch-based training by skipping easy examples during training. To\nfacilitate future work, we release our code, trained models and pre-processed\npublic domain datasets."}, {"title": "A framework to decipher the genetic architecture of combinations of complex diseases: applications in cardiovascular medicine", "abstract": "Genome-wide association studies(GWAS) have proven to be highly useful in\nrevealing the genetic basis of complex diseases. At present, most GWAS are\nstudies of a particular single disease diagnosis against controls. However, in\npractice, an individual is often affected by more than one condition/disorder.\nFor example, patients with coronary artery disease(CAD) are often comorbid with\ndiabetes mellitus(DM). Along a similar line, it is often clinically meaningful\nto study patients with one disease but without a comorbidity. For example,\nobese DM may have different pathophysiology from non-obese DM.\n  Here we developed a statistical framework to uncover susceptibility variants\nfor comorbid disorders (or a disorder without comorbidity), using GWAS summary\nstatistics only. In essence, we mimicked a case-control GWAS in which the cases\nare affected with comorbidities or a disease without a relevant comorbid\ncondition (in either case, we may consider the cases as those affected by a\nspecific subtype of disease, as characterized by the presence or absence of\ncomorbid conditions). We extended our methodology to deal with continuous\ntraits with clinically meaningful categories (e.g. lipids). In addition, we\nillustrated how the analytic framework may be extended to more than two traits.\nWe verified the feasibility and validity of our method by applying it to\nsimulated scenarios and four cardiometabolic (CM) traits. We also analyzed the\ngenes, pathways, cell-types/tissues involved in CM disease subtypes. LD-score\nregression analysis revealed some subtypes may indeed be biologically distinct\nwith low genetic correlations. Further Mendelian randomization analysis found\ndifferential causal effects of different subtypes to relevant complications. We\nbelieve the findings are of both scientific and clinical value, and the\nproposed method may open a new avenue to analyzing GWAS data."}, {"title": "Estimation of genome size using k-mer frequencies from corrected long reads", "abstract": "The third-generation long reads sequencing technologies, such as PacBio and\nNanopore, have great advantages over second-generation Illumina sequencing in\nde novo assembly studies. However, due to the inherent low base accuracy,\nthird-generation sequencing data cannot be used for k-mer counting and\nestimating genomic profile based on k-mer frequencies. Thus, in current genome\nprojects, second-generation data is also necessary for accurately determining\ngenome size and other genomic characteristics. We show that corrected\nthird-generation data can be used to count k-mer frequencies and estimate\ngenome size reliably, in replacement of using second-generation data.\nTherefore, future genome projects can depend on only one sequencing technology\nto finish both assembly and k-mer analysis, which will largely decrease\nsequencing cost in both time and money. Moreover, we present a fast\nlight-weight tool kmerfreq and use it to perform all the k-mer counting tasks\nin this work. We have demonstrated that corrected third-generation sequencing\ndata can be used to estimate genome size and developed a new open-source C/C++\nk-mer counting tool, kmerfreq, which is freely available at\nhttps://github.com/fanagislab/kmerfreq."}, {"title": "SOS: Online probability estimation and generation of T and B cell receptors", "abstract": "Recent advances in modelling VDJ recombination and subsequent selection of T\nand B cell receptors provide useful tools to analyze and compare immune\nrepertoires across time, individuals, and tissues. A suite of tools--IGoR [1],\nOLGA [2] and SONIA [3]--have been publicly released to the community that allow\nfor the inference of generative and selection models from high-throughput\nsequencing data. However using these tools requires some scripting or\ncommand-line skills and familiarity with complex datasets. As a result the\napplication of the above models has not been available to a broad audience. In\nthis application note we fill this gap by presenting Simple OLGA & SONIA (SOS),\na web-based interface where users with no coding skills can compute the\ngeneration and post-selection probabilities of their sequences, as well as\ngenerate batches of synthetic sequences. The application also functions on\nmobile phones."}, {"title": "Coronavirus SARS-CoV-2: Analysis of subgenomic mRNA transcription, 3CLpro and PL2pro protease cleavage sites and protein synthesis", "abstract": "Coronaviruses have recently caused world-wide severe outbreaks: SARS (Severe\nAcute Respiratory Syndrome) in 2002 and MERS (Middle-East Respiratory Syndrome)\nin 2012. At the end of 2019, a new coronavirus outbreak appeared in Wuhan\n(China) seafood market as first focus of infection, becoming a pandemics in\n2020, spreading mainly into Europe and Asia. Although the virus family is\nwell-known, this specific virus presents considerable differences, as higher\ntransmission rates, being a challenge for diagnostic methods, treatments and\nvaccines. Coronavirus(C++).pro is a C++ application which simulates Coronavirus\nreplication cycle. This software has identified virus type in short times and\nprovided FASTA files of virus proteins, a list of mRNA sequences and secondary\nstructures. Furthermore, the software has identified a list of structural,\nnon-structural and accessory proteins in 2019-nCoV virus genome more similar to\nSARS than to MERS, as several fusion proteins characteristics of this virus\ntype. These results are useful as a first step in order to develop diagnostic\nmethods, new vaccines or antiviral drugs, which could avoid virus replication\nin any stage: fusion inhibitors, RdRp inhibitors and PL2pro/3CLpro protease\ninhibitors."}, {"title": "HLA predictions from the bronchoalveolar lavage fluid samples of five patients at the early stage of the Wuhan seafood market COVID-19 outbreak", "abstract": "We are in the midst of a global viral pandemic, one with no cure and a high\nmortality rate. The Human Leukocyte Antigen (HLA) gene complex plays a critical\nrole in host immunity. We predicted HLA class I and II alleles from the\ntranscriptome sequencing data prepared from the bronchoalveolar lavage fluid\nsamples of five patients at the early stage of the COVID-19 outbreak. We\nidentified the HLA-I allele A*24:02 in four out of five patients, which is\nhigher than the expected frequency (17.2%) in the South Han Chinese population.\nThe difference is statistically significant with a p-value less than $10^{-4}$.\nOur analysis results may help provide future insights on disease\nsusceptibility."}, {"title": "LAI-Net: Local-Ancestry Inference with Neural Networks", "abstract": "Local-ancestry inference (LAI), also referred to as ancestry deconvolution,\nprovides high-resolution ancestry estimation along the human genome. In both\nresearch and industry, LAI is emerging as a critical step in DNA sequence\nanalysis with applications extending from polygenic risk scores (used to\npredict traits in embryos and disease risk in adults) to genome-wide\nassociation studies, and from pharmacogenomics to inference of human population\nhistory. While many LAI methods have been developed, advances in computing\nhardware (GPUs) combined with machine learning techniques, such as neural\nnetworks, are enabling the development of new methods that are fast, robust and\neasily shared and stored. In this paper we develop the first neural network\nbased LAI method, named LAI-Net, providing competitive accuracy with\nstate-of-the-art methods and robustness to missing or noisy data, while having\na small number of layers."}, {"title": "High fidelity epigenetic inheritance: Information theoretic model predicts $k$-threshold filling of histone modifications post replication", "abstract": "Beyond the genetic code, there is another layer of information encoded as\nchemical modifications on histone proteins positioned along the DNA.\nMaintaining these modifications is crucial for survival and identity of cells.\nHow the information encoded in the histone marks gets inherited, given that\nonly half the parental nucleosomes are transferred to each daughter chromatin,\nis a puzzle. We address this problem using ideas from Information theory and\nunderstanding from recent biological experiments. Mapping the replication and\nreconstruction of modifications to equivalent problems in communication, we ask\nhow well an enzyme-machinery can recover information, if they were ideal\ncomputing machines. Studying a parameter regime where realistic enzymes can\nfunction, our analysis predicts that, pragmatically, enzymes may implement a\nthreshold$-k$ filling algorithm which derives from maximum \\`a posteriori\nprobability decoding. Simulations using our method produce modification\npatterns similar to what is observed in recent experiments."}, {"title": "Identification of Repurposable Drugs and Adverse Drug Reactions for Various Courses of COVID-19 Based on Single-Cell RNA Sequencing Data", "abstract": "Coronavirus disease 2019 (COVID-19) has impacted almost every part of human\nlife worldwide, posing a massive threat to human health. There is no specific\ndrug for COVID-19, highlighting the urgent need for the development of\neffective therapeutics. To identify potentially repurposable drugs, we employed\na systematic approach to mine candidates from U.S. FDA-approved drugs and\npreclinical small-molecule compounds by integrating the gene expression\nperturbation data for chemicals from the Library of Integrated Network-Based\nCellular Signatures project with a publicly available single-cell RNA\nsequencing dataset from mild and severe COVID-19 patients. We identified 281\nFDA-approved drugs that have the potential to be effective against SARS-CoV-2\ninfection, 16 of which are currently undergoing clinical trials to evaluate\ntheir efficacy against COVID-19. We experimentally tested the inhibitory\neffects of tyrphostin-AG-1478 and brefeldin-a on the replication of the\nsingle-stranded ribonucleic acid (ssRNA) virus influenza A virus. In\nconclusion, we have identified a list of repurposable anti-SARS-CoV-2 drugs\nusing a systems biology approach."}, {"title": "Longitudinal high-throughput TCR repertoire profiling reveals the dynamics of T cell memory formation after mild COVID-19 infection", "abstract": "COVID-19 is a global pandemic caused by the SARS-CoV-2 coronavirus. T cells\nplay a key role in the adaptive antiviral immune response by killing infected\ncells and facilitating the selection of virus-specific antibodies. However\nneither the dynamics and cross-reactivity of the SARS-CoV-2-specific T cell\nresponse nor the diversity of resulting immune memory are well understood. In\nthis study we use longitudinal high-throughput T cell receptor (TCR) sequencing\nto track changes in the T cell repertoire following two mild cases of COVID-19.\nIn both donors we identified CD4+ and CD8+ T cell clones with transient clonal\nexpansion after infection. The antigen specificity of CD8+ TCR sequences to\nSARS-CoV-2 epitopes was confirmed by both MHC tetramer binding and presence in\nlarge database of SARS-CoV-2 epitope-specific TCRs. We describe characteristic\nmotifs in TCR sequences of COVID-19-reactive clones and show preferential\noccurence of these motifs in publicly available large dataset of repertoires\nfrom COVID-19 patients. We show that in both donors the majority of\ninfection-reactive clonotypes acquire memory phenotypes. Certain T cell clones\nwere detected in the memory fraction at the pre-infection timepoint, suggesting\nparticipation of pre-existing cross-reactive memory T cells in the immune\nresponse to SARS-CoV-2."}, {"title": "Dinucleotide repeats in coronavirus SARS-CoV-2 genome: evolutionary implications", "abstract": "The ongoing global pandemic of infection disease COVID-19 caused by the 2019\nnovel coronavirus (SARS-COV-2, formerly 2019-nCoV) presents critical threats to\npublic health and the economy since it was identified in China, December 2019.\nThe genome of SARS-CoV-2 had been sequenced and structurally annotated, yet\nlittle is known of the intrinsic organization and evolution of the genome. To\nthis end, we present a mathematical method for the genomic spectrum, a kind of\nbarcode, of SARS-CoV-2 and common human coronaviruses. The genomic spectrum is\nconstructed according to the periodic distributions of nucleotides, and\ntherefore reflects the unique characteristics of the genome. The results\ndemonstrate that coronavirus SARS-CoV-2 exhibits dinucleotide TT islands in the\nnon-structural proteins 3, 4, 5, and 6. Further analysis of the dinucleotide\nregions suggests that the dinucleotide repeats are increased during evolution\nand may confer the evolutionary fitness of the virus. The special dinucleotide\nregions in the SARS-CoV-2 genome identified in this study may become diagnostic\nand pharmaceutical targets in monitoring and curing the COVID-19 disease."}, {"title": "EDGE COVID-19: A Web Platform to generate submission-ready genomes for SARS-CoV-2 sequencing efforts", "abstract": "Genomics has become an essential technology for surveilling emerging\ninfectious disease outbreaks. A wide range of technologies and strategies for\npathogen genome enrichment and sequencing are being used by laboratories\nworldwide, together with different, and sometimes ad hoc, analytical procedures\nfor generating genome sequences. As a result, public repositories now contain\nnon-standard entries of varying quality. A standardized analytical process for\nconsensus genome sequence determination, particularly for outbreaks such as the\nongoing COVID-19 pandemic, is critical to provide a solid genomic basis for\nepidemiological analyses and well-informed decision making. To address this\nneed, we have developed a bioinformatic workflow to standardize the analysis of\nSARS-CoV-2 sequencing data generated with either the Illumina or Oxford\nNanopore platforms. Using an intuitive web-based interface, this workflow\nautomates SARS-CoV-2 reference-based genome assembly, variant calling, lineage\ndetermination, and provides the ability to submit the consensus sequence and\nnecessary metadata to GenBank or GISAID. Given a raw Illumina or Oxford\nNanopore FASTQ read file, this web-based platform enables non-bioinformatics\nexperts to automatically produce a SARS-CoV-2 genome that is ready for\nsubmission to GISAID or GenBank.\n  Availability:https://edge-covid19.edgebioinformatics.org;https://github.com/LANL-Bioinformatics/EDGE/tree/SARS-CoV2"}, {"title": "An Immune-related lncRNAs Model for Prognostic of SKCM Patients Base on Cox Regression and Coexpression Analysis", "abstract": "SKCM is the most dangerous one of skin cancer, its high degree of malignant,\nis the leading cause of skin cancer. And the level of radiation treatment and\nchemical treatment is minimal, so the mortality is high. Because of its complex\nmolecular and cellular heterogeneity, the existing prediction model of skin\ncancer risk is not ideal. In this study, we developed an immune-related lncRNAs\nmodel to predict the prognosis of patients with SKCM. Screening for\nSKCM-related differential expression of lncRNA from TCGA. Identified\nimmune-related lncRNAs and lncRNA-related mRNA based on the co-expression\nmethod. Through univariate and multivariate analysis, an immune-related lncRNA\nmodel is established to analyze the prognosis of SKCM patients. A 4-lncRNA skin\ncancer prediction model was constructed, including MIR155HG, AL137003.2,\nAC011374.2, and AC009495.2. According to the model, SKCM samples were divided\ninto a high-risk group and low-risk group, and predict the survival of the two\ngroups in 30 years. The area under the ROC curve is 0.749, which shows that the\nmodel has excellent performance. We constructed a 4-lncRNA model to predict the\nprognosis of patients with SKCM, indicating that these lncRNAs may play a\nunique role in the carcinogenesis of SKCM."}, {"title": "Immune Fingerprinting through Repertoire Similarity", "abstract": "Immune repertoires provide a unique fingerprint reflecting the immune history\nof individuals, with potential applications in precision medicine. However, the\nquestion of how personal that information is and how it can be used to identify\nindividuals has not been explored. Here, we show that individuals can be\nuniquely identified from repertoires of just a few thousands lymphocytes. We\npresent \"Immprint,\" a classifier using an information-theoretic measure of\nrepertoire similarity to distinguish pairs of repertoire samples coming from\nthe same versus different individuals. Using published T-cell receptor\nrepertoires and statistical modeling, we tested its ability to identify\nindividuals with great accuracy, including identical twins, by computing false\npositive and false negative rates $< 10^{-6}$ from samples composed of 10,000\nT-cells. We verified through longitudinal datasets and simulations that the\nmethod is robust to acute infections and the passage of time. These results\nemphasize the private and personal nature of repertoire data."}, {"title": "Using micro- and macro-level network metrics unveils top communicative gene modules in psoriasis", "abstract": "Background: Psoriasis is a multifactorial chronic inflammatory disorder of\nthe skin with significant morbidity, characterized by hyper proliferation of\nthe epidermis. Even though psoriasis etiology is not fully understood, it is\nbelieved to be multifactorial with numerous key components. Methods: In order\nto cast light on the complex molecular interactions in psoriasis vulgaris at\nboth protein-protein interactions and transcriptomics levels, we analyzed a set\nof microarray gene expression analysis consisting of 170 paired lesional and\nnon-lesional samples. Afterwards, a network analysis was conducted on\nprotein-protein interaction network of differentially expressed genes based on\nmicro- and macro-level network metrics at a systemic level standpoint. Results:\nWe found 17 top communicative genes, all of which experimentally proven to be\npivotal in psoriasis were identified in two modules, namely, cell cycle and\nimmune system. Intra- and inter-gene interaction subnetworks from the top\ncommunicative genes might provide further insight into the corresponding\ncharacteristic mechanisms. Conclusions: Potential gene combinations for\ntherapeutic/diagnostics purposes were identified. Moreover, our proposed\npipeline could be of interest to a broader range of biological network analysis\nstudies."}, {"title": "Comprehensive assessment of error correction methods for high-throughput sequencing data", "abstract": "The advent of DNA and RNA sequencing has revolutionized the study of genomics\nand molecular biology. Next generation sequencing (NGS) technologies like\nIllumina, Ion Torrent, SOLiD sequencing etc. have brought about a quick and\ncheap way to sequence genomes. Recently, third generation sequencing (TGS)\ntechnologies like PacBio and Oxford Nanopore Technology (ONT) have also been\ndeveloped. Different technologies use different underlying methods for\nsequencing and are prone to different error rates. Though many tools exist for\nerror correction of sequencing data from NGS and TGS methods, no standard\nmethod is available yet to evaluate the accuracy and effectiveness of these\nerror-correction tools. In this study, we present a Software Package for Error\nCorrection Tool Assessment on nuCLEic acid sequences (SPECTACLE) providing\ncomprehensive algorithms to evaluate error-correction methods for DNA and RNA\nsequencing, for NGS and TGS platforms. We also present a compilation of\nsequencing datasets for Illumina, PacBio and ONT platforms that present\nchallenging scenarios for error-correction tools. Using these datasets and\nSPECTACLE, we evaluate the performance of 23 different error-correction tools\nand present unique and helpful insights into their strengths and weaknesses. We\nhope that our methodology will standardize the evaluation of DNA and RNA\nerror-correction tools in the future."}, {"title": "Dynamics of B-cell repertoires and emergence of cross-reactive responses in COVID-19 patients with different disease severity", "abstract": "COVID-19 patients show varying severity of the disease ranging from\nasymptomatic to requiring intensive care. Although a number of SARS-CoV-2\nspecific monoclonal antibodies have been identified, we still lack an\nunderstanding of the overall landscape of B-cell receptor (BCR) repertoires in\nCOVID-19 patients. Here, we used high-throughput sequencing of bulk and plasma\nB-cells collected over multiple time points during infection to characterize\nsignatures of B-cell response to SARS-CoV-2 in 19 patients. Using principled\nstatistical approaches, we determined differential features of BCRs associated\nwith different disease severity. We identified 38 significantly expanded clonal\nlineages shared among patients as candidates for specific responses to\nSARS-CoV-2. Using single-cell sequencing, we verified reactivity of BCRs shared\namong individuals to SARS-CoV-2 epitopes. Moreover, we identified natural\nemergence of a BCR with cross-reactivity to SARS-CoV-1 and SARS-CoV-2 in a\nnumber of patients. Our results provide important insights for development of\nrational therapies and vaccines against COVID-19."}, {"title": "Learning the heterogeneous hypermutation landscape of immunoglobulins from high-throughput repertoire data", "abstract": "Somatic hypermutations of immunoglobulin (Ig) genes occuring during affinity\nmaturation drive B-cell receptors' ability to evolve strong binding to their\nantigenic targets. The landscape of these mutations is highly heterogeneous,\nwith certain regions of the Ig gene being preferentially targeted. However, a\nrigorous quantification of this bias has been difficult because of phylogenetic\ncorrelations between sequences and the interference of selective forces. Here,\nwe present an approach that corrects for these issues, and use it to learn a\nmodel of hypermutation preferences from a recently published large IgH\nrepertoire dataset. The obtained model predicts mutation profiles accurately\nand in a reproducible way, including in the previously uncharacterized\nComplementarity Determining Region 3, revealing that both the sequence context\nof the mutation and its absolute position along the gene are important. In\naddition, we show that hypermutations occurring concomittantly along B-cell\nlineages tend to co-localize, suggesting a possible mechanism for accelerating\naffinity maturation."}, {"title": "On the Transcriptomic Signature and General Stress State Associated with Aneuploidy", "abstract": "Whether aneuploid cells with diverse karyotypes have any properties in common\nhas a been a subject of intense interest. A recent study by Terhorst et al. (1)\nreinvestigated the common aneuploidy gene expression (CAGE), disputing the\nconclusion of our recent work (2). In this short article, which has been\nsubmitted to PNAS as a Letter to the Editor, we explain our major concerns\nabout Terhorst et al. and why we believe that our previous conclusion stands\nvalid."}, {"title": "Structural representations of DNA regulatory substrates can enhance sequence-based algorithms by associating functional sequence variants", "abstract": "The nucleotide sequence representation of DNA can be inadequate for resolving\nprotein-DNA binding sites and regulatory substrates, such as those involved in\ngene expression and horizontal gene transfer. Considering that sequence-like\nrepresentations are algorithmically very useful, here we fused over 60\ncurrently available DNA physicochemical and conformational variables into\ncompact structural representations that can encode single DNA binding sites to\nwhole regulatory regions. We find that the main structural components reflect\nkey properties of protein-DNA interactions and can be condensed to the amount\nof information found in a single nucleotide position. The most accurate\nstructural representations compress functional DNA sequence variants by 30% to\n50%, as each instance encodes from tens to thousands of sequences. We show that\na structural distance function discriminates among groups of DNA substrates\nmore accurately than nucleotide sequence-based metrics. As this opens up a\nvariety of implementation possibilities, we develop and test a distance-based\nalignment algorithm, demonstrating the potential of using the structural\nrepresentations to enhance sequence-based algorithms. Due to the bias of most\ncurrent bioinformatic methods to nucleotide sequence representations, it is\npossible that considerable performance increases might still be achievable with\nsuch solutions."}, {"title": "MOSGA: Modular Open-Source Genome Annotator", "abstract": "The generation of high-quality assemblies, even for large eukaryotic genomes,\nhas become a routine task for many biologists thanks to recent advances in\nsequencing technologies. However, the annotation of these assemblies - a\ncrucial step towards unlocking the biology of the organism of interest - has\nremained a complex challenge that often requires advanced bioinformatics\nexpertise. Here we present MOSGA, a genome annotation framework for eukaryotic\ngenomes with a user-friendly web-interface that generates and integrates\nannotations from various tools. The aggregated results can be analyzed with a\nfully integrated genome browser and are provided in a format ready for\nsubmission to NCBI. MOSGA is built on a portable, customizable, and easily\nextendible Snakemake backend, and thus, can be tailored to a wide range of\nusers and projects. We provide MOSGA as a publicly free available web service\nat https://mosga.mathematik.uni-marburg.de and as a docker container at\nregistry.gitlab.com/mosga/mosga:latest. Source code can be found at\nhttps://gitlab.com/mosga/mosga"}, {"title": "Genome-wide association and transcriptome analysis reveals serum ghrelin to be linked with GFRAL", "abstract": "Objective: Ghrelin is an orexigenic peptide hormone involved in the\nregulation of energy homeostasis, food intake and glucose metabolism. Serum\nlevels increase anticipating a meal and fall afterwards. Underlying genetic\nmechanisms of the ghrelin secretion are unknown. Methods: Total serum ghrelin\nwas measured in 1501 subjects selected from the population-based\nLIFE-ADULT-sample after an overnight fast. A genome-wide association study\n(GWAS) was performed. Gene-based expression association analyses\n(transcriptome-wide association study (TWAS)) were done using MetaXcan.\nResults: In the GWAS, three loci reached genome-wide significance: the\nWW-domain containing the oxidoreductase-gene (WWOX; p=1.80E-10) on chromosome\n16q23.3-24.1 (SNP: rs76823993); the Contactin-Associated Protein-Like 2 gene\n(CNTNAP2; p=9.0E-9) on chromosome 7q35-q36 (SNP: rs192092592) and the Ghrelin\nAnd Obestatin Prepropeptide gene (GHRL; p=2.72E-8) on chromosome 3p25.3 (SNP:\nrs143729751). In the TWAS, serum ghrelin was negatively associated with RNA\nexpression of the GDNF Family Receptor Alpha Like (GFRAL), receptor of the\nanorexigenic Growth Differentiation Factor-15 (GDF15), (z-score=-4.288,\np=1.81E-05). Furthermore, ghrelin was positively associated with Ribosomal\nProtein L36 (RPL36; z-score=4.848, p=1.25E-06). Conclusions: Our findings\nprovide evidence of a functional link between two major players of weight\nregulation, the ghrelin system and the GDF15/GFRAL-pathway."}, {"title": "Whole-Genome Sequence of the Trypoxylus dichotomus Japanese rhinoceros beetle", "abstract": "The draft whole-genome sequence of the Japanese rhinoceros beetle, Trypoxylus\ndichotomus was obtained using long-read PacBio sequence technology. The final\nassembled genome consisted of 739 Mbp in 2,347 contigs, with 24.5x mean\ncoverage and a G+C content of 35.99%."}, {"title": "Parameter estimation with a class of outer probability measures", "abstract": "We explore the interplay between random and deterministic phenomena using a\nrepresentation of uncertainty based on the measure-theoretic concept of outer\nmeasure. The meaning of the analogues of different probabilistic concepts is\ninvestigated and examples of application are given. The novelty of this article\nlies mainly in the suitability of the tools introduced for jointly representing\nrandom and deterministic uncertainty. These tools are shown to yield intuitive\nresults in simple situations and to generalise easily to more complex cases.\nConnections with Dempster-Shafer theory, the empirical Bayes methods and\ngeneralised Bayesian inference are also highlighted."}, {"title": "A linear algorithm for multi-target tracking in the context of possibility theory", "abstract": "We present a modelling framework for multi-target tracking based on\npossibility theory and illustrate its ability to account for the general lack\nof knowledge that the target-tracking practitioner must deal with when working\nwith real data. We also introduce and study variants of the notions of point\nprocess and intensity function, which lead to the derivation of an analogue of\nthe probability hypothesis density (PHD) filter. The gains provided by the\nconsidered modelling framework in terms of flexibility lead to the loss of some\nof the abilities that the PHD filter possesses; in particular the estimation of\nthe number of targets by integration of the intensity function. Yet, the\nproposed recursion displays a number of advantages such as facilitating the\nintroduction of observation-driven birth schemes and the modelling the absence\nof information on the initial number of targets in the scene. The performance\nof the proposed approach is demonstrated on simulated data."}, {"title": "Combining multiple observational data sources to estimate causal effects", "abstract": "The era of big data has witnessed an increasing availability of multiple data\nsources for statistical analyses. We consider estimation of causal effects\ncombining big main data with unmeasured confounders and smaller validation data\nwith supplementary information on these confounders. Under the unconfoundedness\nassumption with completely observed confounders, the smaller validation data\nallow for constructing consistent estimators for causal effects, but the big\nmain data can only give error-prone estimators in general. However, by\nleveraging the information in the big main data in a principled way, we can\nimprove the estimation efficiencies yet preserve the consistencies of the\ninitial estimators based solely on the validation data. Our framework applies\nto asymptotically normal estimators, including the commonly-used regression\nimputation, weighting, and matching estimators, and does not require a correct\nspecification of the model relating the unmeasured confounders to the observed\nvariables. We also propose appropriate bootstrap procedures, which makes our\nmethod straightforward to implement using software routines for existing\nestimators."}, {"title": "Relaxed covariate overlap and margin-based causal effect estimation", "abstract": "In most nonrandomized observational studies, differences between treatment\ngroups may arise not only due to the treatment but also because of the effect\nof confounders. Therefore, causal inference regarding the treatment effect is\nnot as straightforward as in a randomized trial. To adjust for confounding due\nto measured covariates, a variety of methods based on the potential outcomes\nframework are used to estimate average treatment effects. One of the key\nassumptions is treatment positivity, which states that the probability of\ntreatment is bounded away from zero and one for any possible combination of the\nconfounders. Methods for performing causal inference when this assumption is\nviolated are relatively limited. In this article, we discuss a new\nbalance-related condition involving the convex hulls of treatment groups, which\nI term relaxed covariate overlap. An advantage of this concept is that it can\nbe linked to a concept from machine learning, termed the margin. Introduction\nof relaxed covariate overlap leads to an approach in which one can perform\ncausal inference in a three-step manner. The methodology is illustrated with\ntwo examples."}, {"title": "Accounting for unobserved covariates with varying degrees of estimability in high dimensional biological data", "abstract": "An important phenomenon in high dimensional biological data is the presence\nof unobserved covariates that can have a significant impact on the measured\nresponse. When these factors are also correlated with the covariate(s) of\ninterest (i.e. disease status), ignoring them can lead to increased type I\nerror and spurious false discovery rate estimates. We show that depending on\nthe strength of this correlation and the informativeness of the observed data\nfor the latent factors, previously proposed estimators for the effect of the\ncovariate of interest that attempt to account for unobserved covariates are\nasymptotically biased, which corroborates previous practitioners' observations\nthat these estimators tend to produce inflated test statistics. We then provide\nan estimator that corrects the bias and prove it has the same asymptotic\ndistribution as the ordinary least squares estimator when every covariate is\nobserved. Lastly, we use previously published DNA methylation data to show our\nmethod can more accurately estimate the direct effect of asthma on methylation\nthan previously published methods, which underestimate the correlation between\nasthma and latent cell type heterogeneity. Our re-analysis shows that the\nmajority of the variability in methylation due to asthma in those data is\nactually mediated through cell composition."}, {"title": "An Information Analysis on Modeling Interaction Effects in Logistic Regression", "abstract": "The Akaike information criterion (AIC) is commonly used to select a logistic\nregression model for optimal prediction of a binary response by a specified\nfamily of models. It however lacks a convincing method of prescribing a proper\nfamily of models using the desired predictors and their interaction effects.\nFor an alternative approach to model selection, we propose a direct selection\nscheme which first identifies the indispensable regressors as main-effect\npredictors, then examines significant interaction effects between the selected\npredictors such that a logistic model is constructed. The two-step selection\nscheme is formulated by testing for valid information identity between the\nresponse and the predictors, from which the most parsimonious logistic model is\nderived from the least set of indispensable predictors and interaction effects.\nAs a byproduct, the minimum AIC model is easily found in a neighborhood of the\nselected model. The scheme is employed to yield the logistic model for\npredicting the acquisition of professional licenses in a survey of employed\nyouth workers."}, {"title": "How to estimate the sample mean and standard deviation from the five number summary?", "abstract": "In some clinical studies, researchers may report the five number summary\n(including the sample median, the first and third quartiles, and the minimum\nand maximum values) rather than the sample mean and standard deviation. To\nconduct meta-analysis for pooling studies, one needs to first estimate the\nsample mean and standard deviation from the five number summary. A number of\nstudies have been proposed in the recent literature to solve this problem.\nHowever, none of the existing estimators for the standard deviation is\nsatisfactory for practical use. After a brief review of the existing\nliterature, we point out that Wan et al.'s method (BMC Med Res Methodol 14:135,\n2014) has a serious limitation in estimating the standard deviation from the\nfive number summary. To improve it, we propose a smoothly weighted estimator by\nincorporating the sample size information and derive the optimal weight for the\nnew estimator. For ease of implementation, we also provide an approximation\nformula of the optimal weight and a shortcut formula for estimating the\nstandard deviation from the five number summary. The performance of the\nproposed estimator is evaluated through two simulation studies. In comparison\nwith Wan et al.'s estimator, our new estimator provides a more accurate\nestimate for normal data and performs favorably for non-normal data. In real\ndata analysis, our new method is also able to provide a more accurate estimate\nof the true sample standard deviation than the existing method. In this paper,\nwe propose an optimal estimator of the standard deviation from the five number\nsummary. Together with the optimal mean estimator in Luo et al. (Stat Methods\nMed Res, in press, 2017), our new methods have improved the existing literature\nand will make a solid contribution to meta-analysis and evidence-based\nmedicine."}, {"title": "A Constructive Procedure for Modeling Categorical Variables: Log-Linear and Logit Models", "abstract": "Association between categorical variables in contingency tables is analyzed\nusing the information identities based on multivariate multinomial\ndistributions. A scheme of geometric decompositions of the information\nidentities is developed to identify indispensable predictors and interaction\neffects in the construction of concise log-linear and logit models; it suggests\na new approach for selecting parsimonious log-linear and logit models which\nwould facilitate the search for the minimum AIC models as a byproduct. The\nproposed constructive schemes are illustrated along with the analysis of a\ncontingency data table collected in a study on the risk factors of ischemic\ncerebral stroke."}, {"title": "Cluster-weighted latent class modeling", "abstract": "Usually in Latent Class Analysis (LCA), external predictors are taken to be\ncluster conditional probability predictors (LC models with covariates), and/or\nscore conditional probability predictors (LC regression models). In such cases,\ntheir distribution is not of interest. Class specific distribution is of\ninterest in the distal outcome model, when the distribution of the external\nvariable(s) is assumed to dependent on LC membership. In this paper, we\nconsider a more general formulation, typical in cluster-weighted models, which\nembeds both the latent class regression and the distal outcome models. This\nallows us to test simultaneously both whether the distribution of the\ncovariate(s) differs across classes, and whether there are significant direct\neffects of the covariate(s) on the indicators, by including most of the\ninformation about the covariate(s) - latent variable relationship. We show the\nadvantages of the proposed modeling approach through a set of population\nstudies and an empirical application on assets ownership of Italian households."}, {"title": "All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously", "abstract": "Variable importance (VI) tools describe how much covariates contribute to a\nprediction model's accuracy. However, important variables for one\nwell-performing model (for example, a linear model\n$f(\\mathbf{x})=\\mathbf{x}^{T}\\beta$ with a fixed coefficient vector $\\beta$)\nmay be unimportant for another model. In this paper, we propose model class\nreliance (MCR) as the range of VI values across all well-performing model in a\nprespecified class. Thus, MCR gives a more comprehensive description of\nimportance by accounting for the fact that many prediction models, possibly of\ndifferent parametric forms, may fit the data well. In the process of deriving\nMCR, we show several informative results for permutation-based VI estimates,\nbased on the VI measures used in Random Forests. Specifically, we derive\nconnections between permutation importance estimates for a single prediction\nmodel, U-statistics, conditional variable importance, conditional causal\neffects, and linear model coefficients. We then give probabilistic bounds for\nMCR, using a novel, generalizable technique. We apply MCR to a public data set\nof Broward County criminal records to study the reliance of recidivism\nprediction models on sex and race. In this application, MCR can be used to help\ninform VI for unknown, proprietary models."}, {"title": "Bayesian Constraint Relaxation", "abstract": "Prior information often takes the form of parameter constraints. Bayesian\nmethods include such information through prior distributions having constrained\nsupport. By using posterior sampling algorithms, one can quantify uncertainty\nwithout relying on asymptotic approximations. However, sharply constrained\npriors are (a) not necessary in some settings; and (b) tend to limit modeling\nscope to a narrow set of distributions that are tractable computationally.\nInspired by the vast literature that replaces the slab-and-spike prior with a\ncontinuous approximation, we propose to replace the sharp indicator function of\nthe constraint with an exponential kernel, thereby creating a\nclose-to-constrained neighborhood within the Euclidean space in which the\nconstrained subspace is embedded. This kernel decays with distance from the\nconstrained space at a rate depending on a relaxation hyperparameter. By\navoiding the sharp constraint, we enable use of off-the-shelf posterior\nsampling algorithms, such as Hamiltonian Monte Carlo, facilitating automatic\ncomputation in broad models. We study the constrained and relaxed distributions\nunder multiple settings, and theoretically quantify their differences. We\nillustrate the method through multiple novel modeling examples."}, {"title": "Principal component analysis for big data", "abstract": "Big data is transforming our world, revolutionizing operations and analytics\neverywhere, from financial engineering to biomedical sciences. The complexity\nof big data often makes dimension reduction techniques necessary before\nconducting statistical inference. Principal component analysis, commonly\nreferred to as PCA, has become an essential tool for multivariate data analysis\nand unsupervised dimension reduction, the goal of which is to find a lower\ndimensional subspace that captures most of the variation in the dataset. This\narticle provides an overview of methodological and theoretical developments of\nPCA over the last decade, with focus on its applications to big data analytics.\nWe first review the mathematical formulation of PCA and its theoretical\ndevelopment from the view point of perturbation analysis. We then briefly\ndiscuss the relationship between PCA and factor analysis as well as its\napplications to large covariance estimation and multiple testing. PCA also\nfinds important applications in many modern machine learning problems, and we\nfocus on community detection, ranking, mixture model and manifold learning in\nthis paper."}, {"title": "High Dimensional Elliptical Sliced Inverse Regression in non-Gaussian Distributions", "abstract": "Sliced inverse regression (SIR) is the most widely-used sufficient dimension\nreduction method due to its simplicity, generality and computational\nefficiency. However, when the distribution of the covariates deviates from the\nmultivariate normal distribution, the estimation efficiency of SIR is rather\nlow. In this paper, we propose a robust alternative to SIR - called elliptical\nsliced inverse regression (ESIR) for analysing high dimensional, elliptically\ndistributed data. There are wide range of applications of the elliptically\ndistributed data, especially in finance and economics where the distribution of\nthe data is often heavy-tailed. To tackle the heavy-tailed elliptically\ndistributed covariates, we novelly utilize the multivariate Kendall's tau\nmatrix in a framework of so-called generalized eigenvector problem for\nsufficient dimension reduction. Methodologically, we present a practical\nalgorithm for our method. Theoretically, we investigate the asymptotic behavior\nof the ESIR estimator and obtain the corresponding convergence rate under high\ndimensional setting. Quantities of simulation results show that ESIR\nsignificantly improves the estimation efficiency in heavy-tailed scenarios. A\nstock exchange data analysis also demonstrates the effectiveness of our method.\nMoreover, ESIR can be easily extended to most other sufficient dimension\nreduction methods."}, {"title": "The Expected Parameter Change (EPC) for Local Dependence Assessment in Binary Data Latent Class Models", "abstract": "Binary data latent class models crucially assume local independence,\nviolations of which can seriously bias the results. We present two tools for\nmonitoring local dependence in binary data latent class models: the \"Expected\nParameter Change\" (EPC) and a generalized EPC, estimating the substantive size\nand direction of possible local dependencies. The asymptotic and finite sample\nbehavior of the measures is studied, and two applications to the U.S. Census\nestimation of Hispanic ethnicity and medical experts' ratings of x-rays\ndemonstrate its value in arriving at a model that balances realism and\nparsimony."}, {"title": "On the one parameter unit-Lindley distribution and its associated regression model for proportion data", "abstract": "In this paper considering the transformation $X=\\frac{Y}{1+Y}$, where $Y\n\\sim\\text{Lindley}(\\theta)$, we propose the unit-Lindley distribution and\ninvestigate some of its mathematical properties. A important fact associated\nwith this new distribution is that is possible to obtain the analytical\nexpression for bias correction of the maximum likelihood estimator. Moreover,\nit belongs to the exponential family. This distribution allows us to\nincorporate covariates directly in the mean and consequently to quantify the\ninfluence on the average of the response variable. Finally, a practical\napplication is present and it is shown that our model fits much better than the\nBeta regression."}, {"title": "Test Error Estimation after Model Selection Using Validation Error", "abstract": "When performing supervised learning with the model selected using validation\nerror from sample splitting and cross validation, the minimum value of the\nvalidation error can be biased downward. We propose two simple methods that use\nthe errors produced in the validating step to estimate the test error after\nmodel selection, and we focus on the situations where we select the model by\nminimizing the validation error and the randomized validation error. Our\nmethods do not require model refitting, and the additional computational cost\nis negligible. In the setting of sample splitting, we show that, the proposed\ntest error estimates have biases of size $o(1/\\sqrt{n})$ under suitable\nassumptions. We also propose to use the bootstrap to construct confidence\nintervals for the test error based on this result. We apply our proposed\nmethods to a number of simulations and examine their performance."}, {"title": "Max-infinitely divisible models and inference for spatial extremes", "abstract": "For many environmental processes, recent studies have shown that the\ndependence strength is decreasing when quantile levels increase. This implies\nthat the popular max-stable models are inadequate to capture the rate of joint\ntail decay, and to estimate joint extremal probabilities beyond observed\nlevels. We here develop a more flexible modeling framework based on the class\nof max-infinitely divisible processes, which extend max-stable processes while\nretaining dependence properties that are natural for maxima. We propose two\nparametric constructions for max-infinitely divisible models, which relax the\nmax-stability property but remain close to some popular max-stable models\nobtained as special cases. The first model considers maxima over a finite,\nrandom number of independent observations, while the second model generalizes\nthe spectral representation of max-stable processes. Inference is performed\nusing a pairwise likelihood. We illustrate the benefits of our new modeling\nframework on Dutch wind gust maxima calculated over different time units.\nResults strongly suggest that our proposed models outperform other natural\nmodels, such as the Student-t copula process and its max-stable limit, even for\nlarge block sizes."}, {"title": "A method for Bayesian regression modelling of composition data", "abstract": "Many scientific and industrial processes produce data that is best analysed\nas vectors of relative values, often called compositions or proportions. The\nDirichlet distribution is a natural distribution to use for composition or\nproportion data. It has the advantage of a low number of parameters, making it\nthe parsimonious choice in many cases. In this paper we consider the case where\nthe outcome of a process is Dirichlet, dependent on one or more explanatory\nvariables in a regression setting. We explore some existing approaches to this\nproblem, and then introduce a new simulation approach to fitting such models,\nbased on the Bayesian framework. We illustrate the advantages of the new\napproach through simulated examples and an application in sport science. These\nadvantages include: increased accuracy of fit, increased power for inference,\nand the ability to introduce random effects without additional complexity in\nthe analysis."}, {"title": "Variance prior forms for high-dimensional Bayesian variable selection", "abstract": "Consider the problem of high dimensional variable selection for the Gaussian\nlinear model when the unknown error variance is also of interest. In this\npaper, we show that the use of conjugate shrinkage priors for Bayesian variable\nselection can have detrimental consequences for such variance estimation. Such\npriors are often motivated by the invariance argument of Jeffreys (1961).\nRevisiting this work, however, we highlight a caveat that Jeffreys himself\nnoticed; namely that biased estimators can result from inducing dependence\nbetween parameters a priori. In a similar way, we show that conjugate priors\nfor linear regression, which induce prior dependence, can lead to such\nunderestimation in the Bayesian high-dimensional regression setting. Following\nJeffreys, we recommend as a remedy to treat regression coefficients and the\nerror variance as independent a priori. Using such an independence prior\nframework, we extend the Spike-and-Slab Lasso of Rockova and George (2018) to\nthe unknown variance case. This extended procedure outperforms both the fixed\nvariance approach and alternative penalized likelihood methods on simulated\ndata. On the protein activity dataset of Clyde and Parmigiani (1998), the\nSpike-and-Slab Lasso with unknown variance achieves lower cross-validation\nerror than alternative penalized likelihood methods, demonstrating the gains in\npredictive accuracy afforded by simultaneous error variance estimation."}, {"title": "Generalized Linear Models with Linear Constraints for Microbiome Compositional Data", "abstract": "Motivated by regression analysis for microbiome compositional data, this\npaper considers generalized linear regression analysis with compositional\ncovariates, where a group of linear constraints on regression coefficients are\nimposed to account for the compositional nature of the data and to achieve\nsubcompositional coherence. A penalized likelihood estimation procedure using a\ngeneralized accelerated proximal gradient method is developed to efficiently\nestimate the regression coefficients. A de-biased procedure is developed to\nobtain asymptotically unbiased and normally distributed estimates, which leads\nto valid confidence intervals of the regression coefficients. Simulations\nresults show the correctness of the coverage probability of the confidence\nintervals and smaller variances of the estimates when the appropriate linear\nconstraints are imposed. The methods are illustrated by a microbiome study in\norder to identify bacterial species that are associated with inflammatory bowel\ndisease (IBD) and to predict IBD using fecal microbiome."}, {"title": "Strong Sure Screening of Ultra-high Dimensional Categorical Data", "abstract": "Feature screening for ultra high dimensional feature spaces plays a critical\nrole in the analysis of data sets whose predictors exponentially exceed the\nnumber of observations. Such data sets are becoming increasingly prevalent in\nareas such as bioinformatics, medical imaging, and social network analysis.\nFrequently, these data sets have both categorical response and categorical\ncovariates, yet extant feature screening literature rarely considers such data\ntypes. We propose a new screening procedure rooted in the Cochran-Armitage\ntrend test. Our method is specifically applicable for data where both the\nresponse and predictors are categorical. Under a set of reasonable conditions,\nwe demonstrate that our screening procedure has the strong sure screening\nproperty, which extends the seminal results of Fan and Lv. A series of four\nsimulations are used to investigate the performance of our method relative to\nthree other screening methods. We also apply a two-stage iterative approach to\na real data example by first employing our proposed method, and then further\nscreening a subset of selected covariates using lasso, adaptive-lasso and\nelastic net regularization."}, {"title": "Graphical Models for Processing Missing Data", "abstract": "This paper reviews recent advances in missing data research using graphical\nmodels to represent multivariate dependencies. We first examine the limitations\nof traditional frameworks from three different perspectives:\n\\textit{transparency, estimability and testability}. We then show how\nprocedures based on graphical models can overcome these limitations and provide\nmeaningful performance guarantees even when data are Missing Not At Random\n(MNAR). In particular, we identify conditions that guarantee consistent\nestimation in broad categories of missing data problems, and derive procedures\nfor implementing this estimation. Finally we derive testable implications for\nmissing data models in both MAR (Missing At Random) and MNAR categories."}, {"title": "A framework for measuring dependence between random vectors", "abstract": "A framework for quantifying dependence between random vectors is introduced.\nWith the notion of a collapsing function, random vectors are summarized by\nsingle random variables, called collapsed random variables in the framework.\nUsing this framework, a general graphical assessment of independence between\ngroups of random variables for arbitrary collapsing functions is provided.\nMeasures of association computed from the collapsed random variables are then\nused to measure the dependence between random vectors. To this end, suitable\ncollapsing functions are presented. Furthermore, the notion of a collapsed\ndistribution function and collapsed copula are introduced and investigated for\ncertain collapsing functions. This investigation yields a multivariate\nextension of the Kendall distribution and its corresponding Kendall copula for\nwhich some properties and examples are provided. In addition, non-parametric\nestimators for the collapsed measures of dependence are provided along with\ntheir corresponding asymptotic properties. Finally, data applications to\nbioinformatics and finance are presented."}, {"title": "Sharp instruments for classifying compliers and generalizing causal effects", "abstract": "It is well-known that, without restricting treatment effect heterogeneity,\ninstrumental variable (IV) methods only identify \"local\" effects among\ncompliers, i.e., those subjects who take treatment only when encouraged by the\nIV. Local effects are controversial since they seem to only apply to an\nunidentified subgroup; this has led many to denounce these effects as having\nlittle policy relevance. However, we show that such pessimism is not always\nwarranted: it is possible in some cases to accurately predict who compliers\nare, and obtain tight bounds on more generalizable effects in identifiable\nsubgroups. We propose methods for doing so and study their estimation error and\nasymptotic properties, showing that these tasks can in theory be accomplished\neven with very weak IVs. We go on to introduce a new measure of IV quality\ncalled \"sharpness\", which reflects the variation in compliance explained by\ncovariates, and captures how well one can identify compliers and obtain tight\nbounds on identifiable subgroup effects. We develop an estimator of sharpness,\nand show that it is asymptotically efficient under weak conditions. Finally we\nexplore finite-sample properties via simulation, and apply the methods to study\ncanvassing effects on voter turnout. We propose that sharpness should be\npresented alongside strength to assess IV quality."}, {"title": "On the precision matrix of an irregularly sampled AR(1) process", "abstract": "Irregularly sampled AR(1) processes appear in many computationally demanding\napplications. This text provides an analytical expression for the precision\nmatrix of such a process, and gives efficient algorithms for density evaluation\nand simulation, implemented in the R package irregulAR1."}, {"title": "Robust inference with knockoffs", "abstract": "We consider the variable selection problem, which seeks to identify important\nvariables influencing a response $Y$ out of many candidate features $X_1,\n\\ldots, X_p$. We wish to do so while offering finite-sample guarantees about\nthe fraction of false positives - selected variables $X_j$ that in fact have no\neffect on $Y$ after the other features are known. When the number of features\n$p$ is large (perhaps even larger than the sample size $n$), and we have no\nprior knowledge regarding the type of dependence between $Y$ and $X$, the\nmodel-X knockoffs framework nonetheless allows us to select a model with a\nguaranteed bound on the false discovery rate, as long as the distribution of\nthe feature vector $X=(X_1,\\dots,X_p)$ is exactly known. This model selection\nprocedure operates by constructing \"knockoff copies'\" of each of the $p$\nfeatures, which are then used as a control group to ensure that the model\nselection algorithm is not choosing too many irrelevant features. In this work,\nwe study the practical setting where the distribution of $X$ could only be\nestimated, rather than known exactly, and the knockoff copies of the $X_j$'s\nare therefore constructed somewhat incorrectly. Our results, which are free of\nany modeling assumption whatsoever, show that the resulting model selection\nprocedure incurs an inflation of the false discovery rate that is proportional\nto our errors in estimating the distribution of each feature $X_j$ conditional\non the remaining features $\\{X_k:k\\neq j\\}$. The model-X knockoff framework is\ntherefore robust to errors in the underlying assumptions on the distribution of\n$X$, making it an effective method for many practical applications, such as\ngenome-wide association studies, where the underlying distribution on the\nfeatures $X_1,\\dots,X_p$ is estimated accurately but not known exactly."}, {"title": "Maximum Likelihood for Gaussian Process Classification and Generalized Linear Mixed Models under Case-Control Sampling", "abstract": "Modern data sets in various domains often include units that were sampled\nnon-randomly from the population and have a latent correlation structure. Here\nwe investigate a common form of this setting, where every unit is associated\nwith a latent variable, all latent variables are correlated, and the\nprobability of sampling a unit depends on its response. Such settings often\narise in case-control studies, where the sampled units are correlated due to\nspatial proximity, family relations, or other sources of relatedness. Maximum\nlikelihood estimation in such settings is challenging from both a computational\nand statistical perspective, necessitating approximations that take the\nsampling scheme into account. We propose a family of approximate likelihood\napproaches which combine composite likelihood and expectation propagation. We\ndemonstrate the efficacy of our solutions via extensive simulations. We utilize\nthem to investigate the genetic architecture of several complex disorders\ncollected in case-control genetic association studies, where hundreds of\nthousands of genetic variants are measured for every individual, and the\nunderlying disease liabilities of individuals are correlated due to genetic\nsimilarity. Our work is the first to provide a tractable likelihood-based\nsolution for case-control data with complex dependency structures."}, {"title": "Average Power and $λ$-power in Multiple Testing Scenarios when the Benjamini-Hochberg False Discovery Rate Procedure is Used", "abstract": "We discuss several approaches to defining power in studies designed around\nthe Benjamini-Hochberg (BH) false discovery rate (FDR) procedure. We focus\nprimarily on the \\textit{average power} and the $\\lambda$-\\textit{power}, which\nare the expected true positive fraction and the probability that the true\npositive fraction exceeds $\\lambda$, respectively. We prove results concerning\nstrong consistency and asymptotic normality for the positive call fraction\n(PCF), the true positive fraction (TPF) and false discovery fraction (FDF).\nConvergence of their corresponding expected values, including a convergence\nresult for the average power, follow as a corollaries. After reviewing what is\nknown about convergence in distribution of the errors of the plugin procedure,\n(Genovese, 2004), we prove central limit theorems for fully empirical versions\nof the PCF, TPF, and FDF, using a result for stopped stochastic processes. The\ncentral limit theorem (CLT) for the TPF is used to obtain an approximate\nexpression for the $\\lambda$-power, while the CLT for the FDF is used to\nintroduce an approximate procedure for determining a suitably small nominal FDR\nthat results in a speicified bound on the FDF with stipulated high probability.\nThe paper also contains the results of a large simulation study covering a\nfairly substantial portion of the space of possible inputs encountered in\napplication of the results in the design of a biomarker study, a micro-array\nexperiment and a GWAS study."}, {"title": "Multiple Imputation: A Review of Practical and Theoretical Findings", "abstract": "Multiple imputation is a straightforward method for handling missing data in\na principled fashion. This paper presents an overview of multiple imputation,\nincluding important theoretical results and their practical implications for\ngenerating and using multiple imputations. A review of strategies for\ngenerating imputations follows, including recent developments in flexible joint\nmodeling and sequential regression/chained equations/fully conditional\nspecification approaches. Finally, we compare and contrast different methods\nfor generating imputations on a range of criteria before identifying promising\navenues for future research."}, {"title": "On the goodness-of-fit of generalized linear geostatistical models", "abstract": "We propose a generalization of Zhang's coefficient of determination to\ngeneralized linear geostatistical models and illustrate its application to\nriver-blindness mapping. The generalized coefficient of determination has a\nmore intuitive interpretation than other measures of predictive performance and\nallows to assess the individual contribution of each explanatory variable and\nthe random effects to spatial prediction. The developed methodology is also\nmore widely applicable to any generalized linear mixed model."}, {"title": "A Simple and Efficient Estimation Method for Models with Nonignorable Missing Data", "abstract": "This paper proposes a simple and efficient estimation procedure for the model\nwith non-ignorable missing data studied by Morikawa and Kim (2016). Their\nsemiparametrically efficient estimator requires explicit nonparametric\nestimation and so suffers from the curse of dimensionality and requires a\nbandwidth selection. We propose an estimation method based on the Generalized\nMethod of Moments (hereafter GMM). Our method is consistent and asymptotically\nnormal regardless of the number of moments chosen. Furthermore, if the number\nof moments increases appropriately our estimator can achieve the semiparametric\nefficiency bound derived in Morikawa and Kim (2016), but under weaker\nregularity conditions. Moreover, our proposed estimator and its consistent\ncovariance matrix are easily computed with the widely available GMM package. We\npropose two data-based methods for selection of the number of moments. A small\nscale simulation study reveals that the proposed estimation indeed out-performs\nthe existing alternatives in finite samples."}, {"title": "TFisher Tests: Optimal and Adaptive Thresholding for Combining $p$-Values", "abstract": "For testing a group of hypotheses, tremendous $p$-value combination methods\nhave been developed and widely applied since 1930's. Some methods (e.g., the\nminimal $p$-value) are optimal for sparse signals, and some others (e.g.,\nFisher's combination) are optimal for dense signals. To address a wide spectrum\nof signal patterns, this paper proposes a unifying family of statistics, called\nTFisher, with general $p$-value truncation and weighting schemes. Analytical\ncalculations for the $p$-value and the statistical power of TFisher under\ngeneral hypotheses are given. Optimal truncation and weighting parameters are\nstudied based on Bahadur Efficiency (BE) and the proposed Asymptotic Power\nEfficiency (APE), which is superior to BE for studying the signal detection\nproblem. A soft-thresholding scheme is shown to be optimal for signal detection\nin a large space of signal patterns. When prior information of signal pattern\nis unavailable, an omnibus test, oTFisher, can adapt to the given data.\nSimulations evidenced the accuracy of calculations and validated the\ntheoretical properties. The TFisher tests were applied to analyzing a whole\nexome sequencing data of amyotrophic lateral sclerosis. Relevant tests and\ncalculations have been implemented into an R package $TFisher$ and published on\nthe CRAN."}, {"title": "On a statistical approach to mate choices in reproduction", "abstract": "We provide a probabilistic approach to modeling the movements of subjects\nthrough multiple stages, with \"stays\" or survival at each stage for a random\nlength of time, and ending at a desired final stage. We use conditional Markov\nchains with exponential survival times to model the movement of each subject.\nThis is motivated by a study to learn about of the choices that different types\nof female turkeys make in choosing a male turkey, and in particular, the\ndifferences in male choices between groups of females. In this paper, we\npropose a model for the subjects' movements toward the final stage, and provide\nmaximum likelihood estimation of the model parameters. We also provide results\nrelating to certain questions of interest, such as the distribution of the\nnumber of subjects reaching a stage and the probability that a subject reaches\nthe final stage, and develop methods for estimating these quantities and\ntesting statistical hypotheses of interest."}, {"title": "Robust Inference for Seemingly Unrelated Regression Models", "abstract": "Seemingly unrelated regression models generalize linear regression models by\nconsidering multiple regression equations that are linked by contemporaneously\ncorrelated disturbances. Robust inference for seemingly unrelated regression\nmodels is considered. MM-estimators are introduced to obtain estimators that\nhave both a high breakdown point and a high normal efficiency. A fast and\nrobust bootstrap procedure is developed to obtain robust inference for these\nestimators. Confidence intervals for the model parameters as well as hypothesis\ntests for linear restrictions of the regression coefficients in seemingly\nunrelated regression models are constructed. Moreover, in order to evaluate the\nneed for a seemingly unrelated regression model, a robust procedure is proposed\nto test for the presence of correlation among the disturbances. The performance\nof the fast and robust bootstrap inference is evaluated empirically in\nsimulation studies and illustrated on real data."}, {"title": "On the number of signals in multivariate time series", "abstract": "We assume a second-order source separation model where the observed\nmultivariate time series is a linear mixture of latent, temporally uncorrelated\ntime series with some components pure white noise. To avoid the modelling of\nnoise, we extract the non-noise latent components using some standard method,\nallowing the modelling of the extracted univariate time series individually. An\nimportant question is the determination of which of the latent components are\nof interest in modelling and which can be considered as noise. Bootstrap-based\nmethods have recently been used in determining the latent dimension in various\nmethods of unsupervised and supervised dimension reduction and we propose a set\nof similar estimation strategies for second-order stationary time series.\nSimulation studies and a sound wave example are used to show the method's\neffectiveness."}, {"title": "Smoothing splines on Riemannian manifolds, with applications to 3D shape space", "abstract": "There has been increasing interest in statistical analysis of data lying in\nmanifolds. This paper generalizes a smoothing spline fitting method to\nRiemannian manifold data based on the technique of unrolling and unwrapping\noriginally proposed in Jupp and Kent (1987) for spherical data. In particular\nwe develop such a fitting procedure for shapes of configurations in general\n$m$-dimensional Euclidean space, extending our previous work for two\ndimensional shapes. We show that parallel transport along a geodesic on Kendall\nshape space is linked to the solution of a homogeneous first-order differential\nequation, some of whose coefficients are implicitly defined functions. This\nfinding enables us to approximate the procedure of unrolling and unwrapping by\nsimultaneously solving such equations numerically, and so to find numerical\nsolutions for smoothing splines fitted to higher dimensional shape data. This\nfitting method is applied to the analysis of some dynamic 3D peptide data."}, {"title": "Compositional Correlation for Detecting Real Associations Among Time Series", "abstract": "Correlation remains to be one of the most widely used statistical tools for\nassessing the strength of relationships between data series. This paper\npresents a novel compositional correlation method for detecting linear and\nnonlinear relationships by considering the averages of all parts of all\npossible compositions of the data series instead of considering the averages of\nthe whole series. The approach enables cumulative contribution of all local\nassociations to the resulting correlation value. The method is applied on two\ndifferent datasets: a set of four simple nonlinear polynomial functions and the\nexpression time series data of 4381 budding yeast (saccharomyces cerevisiae)\ngenes. The obtained results show that the introduced compositional correlation\nmethod is capable of determining real direct and inverse linear, nonlinear and\nmonotonic relationships. Comparisons with Pearson's correlation, Spearman's\ncorrelation, distance correlation and the simulated annealing genetic algorithm\nmaximal information coefficient (SGMIC) have shown that the presented method is\ncapable of detecting important associations which were not detected by the\ncompared methods."}, {"title": "Factor graph fragmentization of expectation propagation", "abstract": "Expectation propagation is a general approach to fast approximate inference\nfor graphical models. The existing literature treats models separately when it\ncomes to deriving and coding expectation propagation inference algorithms. This\ncomes at the cost of similar, long-winded algebraic steps being repeated and\nslowing down algorithmic development. We demonstrate how factor graph\nfragmentization can overcome this impediment. This involves adoption of the\nmessage passing on a factor graph approach to expectation propagation and\nidentification of factor graph sub-graphs, which we call fragments, that are\ncommon to wide classes of models. Key fragments and their corresponding\nmessages are catalogued which means that their algebra does not need to be\nrepeated. This allows compartmentalization of coding and efficient software\ndevelopment."}, {"title": "Panel data analysis via mechanistic models", "abstract": "Panel data, also known as longitudinal data, consist of a collection of time\nseries. Each time series, which could itself be multivariate, comprises a\nsequence of measurements taken on a distinct unit. Mechanistic modeling\ninvolves writing down scientifically motivated equations describing the\ncollection of dynamic systems giving rise to the observations on each unit. A\ndefining characteristic of panel systems is that the dynamic interaction\nbetween units should be negligible. Panel models therefore consist of a\ncollection of independent stochastic processes, generally linked through shared\nparameters while also having unit-specific parameters. To give the scientist\nflexibility in model specification, we are motivated to develop a framework for\ninference on panel data permitting the consideration of arbitrary nonlinear,\npartially observed panel models. We build on iterated filtering techniques that\nprovide likelihood-based inference on nonlinear partially observed Markov\nprocess models for time series data. Our methodology depends on the latent\nMarkov process only through simulation; this plug-and-play property ensures\napplicability to a large class of models. We demonstrate our methodology on a\ntoy example and two epidemiological case studies. We address inferential and\ncomputational issues arising due to the combination of model complexity and\ndataset size."}, {"title": "An Efficient Algorithm for Elastic I-optimal Design of Generalized Linear Models", "abstract": "The generalized linear models (GLMs) are widely used in statistical analysis\nand the related design issues are undoubtedly challenging. The state-of-the-art\nworks mostly apply to design criteria on the estimates of regression\ncoefficients. The prediction accuracy is usually critical in modern decision\nmaking and artificial intelligence applications. It is of importance to study\noptimal designs from the prediction aspects for generalized linear models. In\nthis work, we consider the Elastic I-optimality as a prediction-oriented design\ncriterion for generalized linear models, and develop efficient algorithms for\nsuch $\\text{EI}$-optimal designs. By investigating theoretical properties for\nthe optimal weights of any set of design points and extending the general\nequivalence theorem to the $\\text{EI}$-optimality for GLMs, the proposed\nefficient algorithm adequately combines the Fedorov-Wynn algorithm and\nmultiplicative algorithm. It achieves great computational efficiency with\nguaranteed convergence. Numerical examples are conducted to evaluate the\nfeasibility and computational efficiency of the proposed algorithm."}, {"title": "Anchor regression: heterogeneous data meets causality", "abstract": "We consider the problem of predicting a response variable from a set of\ncovariates on a data set that differs in distribution from the training data.\nCausal parameters are optimal in terms of predictive accuracy if in the new\ndistribution either many variables are affected by interventions or only some\nvariables are affected, but the perturbations are strong. If the training and\ntest distributions differ by a shift, causal parameters might be too\nconservative to perform well on the above task. This motivates anchor\nregression, a method that makes use of exogeneous variables to solve a\nrelaxation of the causal minimax problem by considering a modification of the\nleast-squares loss. The procedure naturally provides an interpolation between\nthe solutions of ordinary least squares and two-stage least squares. We prove\nthat the estimator satisfies predictive guarantees in terms of distributional\nrobustness against shifts in a linear class; these guarantees are valid even if\nthe instrumental variables assumptions are violated. If anchor regression and\nleast squares provide the same answer (anchor stability), we establish that OLS\nparameters are invariant under certain distributional changes. Anchor\nregression is shown empirically to improve replicability and protect against\ndistributional shifts."}, {"title": "Bayesian method for causal inference in spatially-correlated multivariate time series", "abstract": "Measuring the causal impact of an advertising campaign on sales is an\nessential task for advertising companies. Challenges arise when companies run\nadvertising campaigns in multiple stores which are spatially correlated, and\nwhen the sales data have a low signal-to-noise ratio which makes the\nadvertising effects hard to detect. This paper proposes a solution to address\nboth of these challenges. A novel Bayesian method is proposed to detect weaker\nimpacts and a multivariate structural time series model is used to capture the\nspatial correlation between stores through placing a $\\mathcal{G}$-Wishart\nprior on the precision matrix. The new method is to compare two posterior\ndistributions of a latent variable---one obtained by using the observed data\nfrom the test stores and the other one obtained by using the data from their\ncounterfactual potential outcomes. The counterfactual potential outcomes are\nestimated from the data of synthetic controls, each of which is a linear\ncombination of sales figures at many control stores over the causal period.\nControl stores are selected using a revised Expectation-Maximization variable\nselection (EMVS) method. A two-stage algorithm is proposed to estimate the\nparameters of the model. To prevent the prediction intervals from being\nexplosive, a stationarity constraint is imposed on the local linear trend of\nthe model through a recently proposed prior. The benefit of using this prior is\ndiscussed in this paper. A detailed simulation study shows the effectiveness of\nusing our proposed method to detect weaker causal impact. The new method is\napplied to measure the causal effect of an advertising campaign for a consumer\nproduct sold at stores of a large national retail chain."}, {"title": "Bayesian Inference of Local Projections with Roughness Penalty Priors", "abstract": "A local projection is a statistical framework that accounts for the\nrelationship between an exogenous variable and an endogenous variable, measured\nat different time points. Local projections are often applied in impulse\nresponse analyses and direct forecasting. While local projections are becoming\nincreasingly popular because of their robustness to misspecification and their\nflexibility, they are less statistically efficient than standard methods, such\nas vector autoregression. In this study, we seek to improve the statistical\nefficiency of local projections by developing a fully Bayesian approach that\ncan be used to estimate local projections using roughness penalty priors. By\nincorporating such prior-induced smoothness, we can use information contained\nin successive observations to enhance the statistical efficiency of an\ninference. We apply the proposed approach to an analysis of monetary policy in\nthe United States, showing that the roughness penalty priors successfully\nestimate the impulse response functions and improve the predictive accuracy of\nlocal projections."}, {"title": "Penalised maximum likelihood estimation in multistate models for interval-censored data", "abstract": "Multistate models can be used to describe transitions over time across\nstates. In the presence of interval-censored times for transitions, the\nlikelihood is constructed using transition probabilities. Models are specified\nusing proportional hazards model for the transitions. Time-dependency is\nusually defined by parametric models, which can be too restrictive.\nNonparametric hazards specification with splines allow for flexible modelling\nof time-dependency without making strong model assumptions. Penalised maximum\nlikelihood is used to estimate the models. Selecting the optimal amount of\nsmoothing is challenging as the problem involves multiple penalties. We propose\nan automatic and efficient method to estimate multistate models with splines in\nthe presence of interval-censoring. The method is illustrated with a data\nanalysis and a simulation study."}, {"title": "Nonparametric method for space conditional density estimation in moderately large dimensions", "abstract": "In this paper, we consider the problem of estimating a conditional density in\nmoderately large dimensions. Much more informative than regression functions,\nconditional densities are of main interest in recent methods, particularly in\nthe Bayesian framework (studying the posterior distribution, finding its\nmodes...). Considering a recently studied family of kernel estimators, we\nselect a pointwise multivariate bandwidth by revisiting the greedy algorithm\nRodeo (Regularisation Of Derivative Expectation Operator). The method addresses\nseveral issues: being greedy and computationally efficient by an iterative\nprocedure, avoiding the curse of high dimensionality under some suitably\ndefined sparsity conditions by early variable selection during the procedure,\nconverging at a quasi-optimal minimax rate."}, {"title": "Distribution-free runs-based control charts", "abstract": "We propose distribution-free runs-based control charts for detecting location\nshifts. Using the fact that given the number of total successes, the outcomes\nof a sequence of Bernoulli trials are random permutations, we are able to\ncontrol the conditional probability of a signal detected at current time given\nthat there is not alarm before at a pre-determined level. This leads to a\ndesired in-control average run length and data-dependent control limits. Two\ncommon runs statistics, the longest run statistic and the scan statitsic, are\nstudied in detail and their exact conditional distributions given the number of\ntotal successes are obtained using the finite Markov chain imbedding technique.\nNumerical results are given to evaluate the performance of the proposed control\ncharts."}, {"title": "Incorporating Prior Information with Fused Sparse Group Lasso: Application to Prediction of Clinical Measures from Neuroimages", "abstract": "Predicting clinical variables from whole-brain neuroimages is a high\ndimensional problem that requires some type of feature selection or extraction.\nPenalized regression is a popular embedded feature selection method for high\ndimensional data. For neuroimaging applications, spatial regularization using\nthe $\\ell_1$ or $\\ell_2$ norm of the image gradient has shown good performance,\nyielding smooth solutions in spatially contiguous brain regions. However,\nrecently enormous resources have been devoted to establishing structural and\nfunctional brain connectivity networks that can be used to define spatially\ndistributed yet related groups of voxels. We propose using the fused sparse\ngroup lasso penalty to encourage structured, sparse, interpretable solutions by\nincorporating prior information about spatial and group structure among voxels.\nWe present optimization steps for fused sparse group lasso penalized regression\nusing the alternating direction method of multipliers algorithm. With\nsimulation studies and in application to real fMRI data from the Autism Brain\nImaging Data Exchange, we demonstrate conditions under which fusion and group\npenalty terms together outperform either of them alone. Supplementary materials\nfor this article are available online."}, {"title": "Missing at random: a stochastic process perspective", "abstract": "We offer a natural and extensible measure-theoretic treatment of missingness\nat random. Within the standard missing data framework, we give a novel\ncharacterisation of the observed data as a stopping-set sigma algebra. We\ndemonstrate that the usual missingness at random conditions are equivalent to\nrequiring particular stochastic processes to be adapted to a set-indexed\nfiltration of the complete data: measurability conditions that suffice to\nensure the likelihood factorisation necessary for ignorability. Our rigorous\nstatement of the missing at random conditions also clarifies a common\nconfusion: what is fixed, and what is random?"}, {"title": "A tractable Multi-Partitions Clustering", "abstract": "In the framework of model-based clustering, a model allowing several latent\nclass variables is proposed. This model assumes that the distribution of the\nobserved data can be factorized into several independent blocks of variables.\nEach block is assumed to follow a latent class model ({\\it i.e.,} mixture with\nconditional independence assumption). The proposed model includes variable\nselection, as a special case, and is able to cope with the mixed-data setting.\nThe simplicity of the model allows to estimate the repartition of the variables\ninto blocks and the mixture parameters simultaneously, thus avoiding to run EM\nalgorithms for each possible repartition of variables into blocks. For the\nproposed method, a model is defined by the number of blocks, the number of\nclusters inside each block and the repartition of variables into block. Model\nselection can be done with two information criteria, the BIC and the MICL, for\nwhich an efficient optimization is proposed. The performances of the model are\ninvestigated on simulated and real data. It is shown that the proposed method\ngives a rich interpretation of the dataset at hand ({\\it i.e.,} analysis of the\nrepartition of the variables into blocks and analysis of the clusters produced\nby each block of variables)."}, {"title": "On the estimation of variance parameters in non-standard generalised linear mixed models: Application to penalised smoothing", "abstract": "We present a novel method for the estimation of variance parameters in\ngeneralised linear mixed models. The method has its roots in Harville (1977)'s\nwork, but it is able to deal with models that have a precision matrix for the\nrandom-effect vector that is linear in the inverse of the variance parameters\n(i.e., the precision parameters). We call the method SOP (Separation of\nOverlapping Precision matrices). SOP is based on applying the method of\nsuccessive approximations to easy-to-compute estimate updates of the variance\nparameters. These estimate updates have an appealing form: they are the ratio\nof a (weighted) sum of squares to a quantity related to effective degrees of\nfreedom. We provide the sufficient and necessary conditions for these estimates\nto be strictly positive. An important application field of SOP is penalised\nregression estimation of models where multiple quadratic penalties act on the\nsame regression coefficients. We discuss in detail two of those models:\npenalised splines for locally adaptive smoothness and for hierarchical curve\ndata. Several data examples in these settings are presented."}, {"title": "An Evaluation of Bounding Approaches for Generalization", "abstract": "Statisticians have recently developed propensity score methods to improve\ngeneralizations from randomized experiments that do not employ random sampling.\nHowever, these methods typically rely on assumptions whose plausibility may be\nquestionable in practice. In this article, we introduce and discuss bounding,\nan approach that is based on alternative assumptions that may be more plausible\nin a given study. The bounding framework nonparametrically estimates population\nparameters using a range of plausible values that are consistent with the\nobserved characteristics of the data. We illustrate how the bounds can be\ntightened using three approaches: imposing an alternative assumption based on\nmonotonicity, redefining the population of inference, and using propensity\nscore stratification. Using the results from two simulation studies, we examine\nthe conditions under which bounds for the population parameter are tightened.\nWe conclude with an application of bounding to SimCalc, a cluster randomized\ntrial that evaluated the effectiveness of a technology aid on mathematics\nachievement."}, {"title": "Discovering the Signal Subgraph: An Iterative Screening Approach on Graphs", "abstract": "Supervised learning on graphs is a challenging task due to the high\ndimensionality and inherent structural dependencies in the data, where each\nedge depends on a pair of vertices. Existing conventional methods are designed\nfor standard Euclidean data and do not account for the structural information\ninherent in graphs. In this paper, we propose an iterative vertex screening\nmethod to achieve dimension reduction across multiple graph datasets with\nmatched vertex sets and associated graph attributes. Our method aims to\nidentify a signal subgraph to provide a more concise representation of the full\ngraphs, potentially benefiting subsequent vertex classification tasks. The\nmethod screens the rows and columns of the adjacency matrix concurrently and\nstops when the resulting distance correlation is maximized. We establish the\ntheoretical foundation of our method by proving that it estimates the true\nsignal subgraph with high probability. Additionally, we establish the\nconvergence rate of classification error under the Erdos-Renyi random graph\nmodel and prove that the subsequent classification can be asymptotically\noptimal, outperforming the entire graph under high-dimensional conditions. Our\nmethod is evaluated on various simulated datasets and real-world human and\nmurine graphs derived from functional and structural magnetic resonance images.\nThe results demonstrate its excellent performance in estimating the\nground-truth signal subgraph and achieving superior classification accuracy."}, {"title": "Strong Sure Screening of Ultra-high Dimensional Data with Interaction Effects", "abstract": "Ultrahigh dimensional data sets are becoming increasingly prevalent in areas\nsuch as bioinformatics, medical imaging, and social network analysis. Sure\nindependent screening of such data is commonly used to analyze such data.\nNevertheless, few methods exist for screening for interactions among\npredictors. Moreover, extant interaction screening methods prove to be highly\ninaccurate when applied to data sets exhibiting strong interactive effects, but\nweak marginal effects, on the response. We propose a new interaction screening\nprocedure based on joint cumulants which is not inhibited by such limitations.\nUnder a collection of sensible conditions, we demonstrate that our interaction\nscreening procedure has the strong sure screening property. Four simulations\nare used to investigate the performance of our method relative to two other\ninteraction screening methods. We also apply a two-stage analysis to a real\ndata example by first employing our proposed method, and then further examining\na subset of selected covariates using multifactor dimensionality reduction."}, {"title": "Sensitivity of codispersion to noise and error in ecological and environmental data", "abstract": "Codispersion analysis is a new statistical method developed to assess spatial\ncovariation between two spatial processes that may not be isotropic or\nstationary. Its application to anisotropic ecological datasets have provided\nnew insights into mechanisms underlying observed patterns of species\ndistributions and the relationship between individual species and underlying\nenvironmental gradients. However, the performance of the codispersion\ncoefficient when there is noise or measurement error (\"contamination\") in the\ndata has been addressed only theoretically. Here, we use Monte Carlo\nsimulations and real datasets to investigate the sensitivity of codispersion to\nfour types of contamination commonly seen in many real-world environmental and\necological studies. Three of these involved examining codispersion of a spatial\ndataset with a contaminated version of itself. The fourth examined differences\nin codisperson between plants and soil conditions, where the estimates of soil\ncharacteristics were based on complete or thinned datasets. In all cases, we\nfound that estimates of codispersion were robust when contamination, such as\ndata thinning, was relatively low (<15\\%), but were sensitive to larger\npercentages of contamination. We also present a useful method for imputing\nmissing spatial data and discuss several aspects of the codispersion\ncoefficient when applied to noisy data to gain more insight about the\nperformance of codispersion in practice."}, {"title": "Discrete Weibull generalised additive model: an application to count fertility data", "abstract": "Fertility plans, measured by the number of planned children, have been found\nto be affected by education and family background via complex tail\ndependencies. This challenge was previously met with the use of non-parametric\njittering approaches. This paper shows how a novel generalized additive model\nbased on a discrete Weibull distribution provides partial effects of the\ncovariates on fertility plans which are comparable to jittering, without the\ninherent drawback of crossing conditional quantiles. The model has some\nadditional desirable features: both over- and under-dispersed data can be\nmodelled by this distribution, the conditional quantiles have a simple analytic\nform and the likelihood is the same of that of a continuous Weibull\ndistribution with interval-censored data. The latter means that efficient\nimplementations are already available, in the R package gamlss, for a range of\nmodels and inferential procedures, and at a fraction of the time compared to\nthe jittering and COM-Poisson approaches, showing potential for the wide\napplicability of this approach to the modelling of count data."}, {"title": "L1-Penalized Censored Gaussian Graphical Model", "abstract": "Graphical lasso is one of the most used estimators for inferring genetic\nnetworks. Despite its diffusion, there are several fields in applied research\nwhere the limits of detection of modern measurement technologies make the use\nof this estimator theoretically unfounded, even when the assumption of a\nmultivariate Gaussian distribution is satisfied. Typical examples are data\ngenerated by polymerase chain reactions and flow cytometer. The combination of\ncensoring and high-dimensionality make inference of the underlying genetic\nnetworks from these data very challenging. In this paper we propose an\n$\\ell_1$-penalized Gaussian graphical model for censored data and derive two\nEM-like algorithms for inference. By an extensive simulation study, we evaluate\nthe computational efficiency of the proposed algorithms and show that our\nproposal overcomes existing competitors when censored data are available. We\napply the proposed method to gene expression data coming from microfluidic\nRT-qPCR technology in order to make inference on the regulatory mechanisms of\nblood development."}, {"title": "Uncertainty quantification for computer models with spatial output using calibration-optimal bases", "abstract": "The calibration of complex computer codes using uncertainty quantification\n(UQ) methods is a rich area of statistical methodological development. When\napplying these techniques to simulators with spatial output, it is now standard\nto use principal component decomposition to reduce the dimensions of the\noutputs in order to allow Gaussian process emulators to predict the output for\ncalibration. We introduce the `terminal case', in which the model cannot\nreproduce observations to within model discrepancy, and for which standard\ncalibration methods in UQ fail to give sensible results. We show that even when\nthere is no such issue with the model, the standard decomposition on the\noutputs can and usually does lead to a terminal case analysis. We present a\nsimple test to allow a practitioner to establish whether their experiment will\nresult in a terminal case analysis, and a methodology for defining\ncalibration-optimal bases that avoid this whenever it is not inevitable. We\npresent the optimal rotation algorithm for doing this, and demonstrate its\nefficacy for an idealised example for which the usual principal component\nmethods fail. We apply these ideas to the CanAM4 model to demonstrate the\nterminal case issue arising for climate models. We discuss climate model tuning\nand the estimation of model discrepancy within this context, and show how the\noptimal rotation algorithm can be used in developing practical climate model\ntuning tools."}, {"title": "Generating survival times using Cox proportional hazards models with cyclic time-varying covariates, with application to a multiple-dose monoclonal antibody clinical trial", "abstract": "In two harmonized efficacy studies to prevent HIV infection through multiple\ninfusions of the monoclonal antibody VRC01, a key objective is to evaluate\nwhether the serum concentration of VRC01, which changes cyclically over time\nalong with the infusion schedule, is associated with the rate of HIV infection.\nSimulation studies are needed in the development of such survival models. In\nthis paper, we consider simulating event time data with a continuous\ntime-varying covariate whose values vary with time through multiple drug\nadministration cycles, and whose effect on survival changes differently before\nand after a threshold within each cycle. The latter accommodates settings with\na zero-protection biomarker threshold above which the drug provides a varying\nlevel of protection depending on the biomarker level, but below which the drug\nprovides no protection. We propose two simulation approaches: one based on\nsimulating survival data under a single-dose regimen first before data are\naggregated over multiple doses, and another based on simulating survival data\ndirectly under a multiple-dose regimen. We generate time-to-event data\nfollowing a Cox proportional hazards model based on inverting the cumulative\nhazard function and a log link function for relating the hazard function to the\ncovariates. The method's validity is assessed in two sets of simulation\nexperiments. The results indicate that the proposed procedures perform well in\nproducing data that conform to their cyclic nature and assumptions of the Cox\nproportional hazards model."}, {"title": "More powerful post-selection inference, with application to the Lasso", "abstract": "Investigators often use the data to generate interesting hypotheses and then\nperform inference for the generated hypotheses. P-values and confidence\nintervals must account for this explorative data analysis. A fruitful method\nfor doing so is to condition any inferences on the components of the data used\nto generate the hypotheses, thus preventing information in those components\nfrom being used again. Some currently popular methods \"over-condition\", leading\nto wide intervals. We show how to perform the minimal conditioning in a\ncomputationally tractable way. In high dimensions, even this minimal\nconditioning can lead to intervals that are too wide to be useful, suggesting\nthat up to now the cost of hypothesis generation has been underestimated. We\nshow how to generate hypotheses in a strategic manner that sharply reduces the\ncost of data exploration and results in useful confidence intervals. Our\ndiscussion focuses on the problem of post-selection inference after fitting a\nlasso regression model, but we also outline its extension to a much more\ngeneral setting."}, {"title": "Structure and Sensitivity in Differential Privacy: Comparing K-Norm Mechanisms", "abstract": "Differential privacy (DP), provides a framework for provable privacy\nprotection against arbitrary adversaries, while allowing the release of summary\nstatistics and synthetic data. We address the problem of releasing a noisy\nreal-valued statistic vector $T$, a function of sensitive data under DP, via\nthe class of $K$-norm mechanisms with the goal of minimizing the noise added to\nachieve privacy. First, we introduce the sensitivity space of $T$, which\nextends the concepts of sensitivity polytope and sensitivity hull to the\nsetting of arbitrary statistics $T$. We then propose a framework consisting of\nthree methods for comparing the $K$-norm mechanisms: 1) a multivariate\nextension of stochastic dominance, 2) the entropy of the mechanism, and 3) the\nconditional variance given a direction, to identify the optimal $K$-norm\nmechanism. In all of these criteria, the optimal $K$-norm mechanism is\ngenerated by the convex hull of the sensitivity space. Using our methodology,\nwe extend the objective perturbation and functional mechanisms and apply these\ntools to logistic and linear regression, allowing for private releases of\nstatistical results. Via simulations and an application to a housing price\ndataset, we demonstrate that our proposed methodology offers a substantial\nimprovement in utility for the same level of risk."}, {"title": "Uncertainty Estimation in Functional Linear Models", "abstract": "Functional data analysis is proved to be useful in many scientific\napplications. The physical process is observed as curves and often there are\nseveral curves observed due to multiple subjects, providing the replicates in\nstatistical sense. The recent literature develops several techniques for\nregistering the curves and associated model estimation. However, very little\nhas been investigated for statistical inference, specifically uncertainty\nestimation. In this article, we consider functional linear mixed modeling\napproach to combine several curves. We concentrate measuring uncertainty when\nthe functional linear mixed models are used for prediction. Although measuring\nthe uncertainty is paramount interest in any statistical prediction, there is\nno closed form expression available for functional mixed effects models. In\nmany real life applications only a finite number of curves can be observed. In\nsuch situations it is important to asses the error rate for any valid\nstatistical statement. We derive theoretically valid approximation of\nuncertainty measurements that are suitable along with modified estimation\ntechniques. We illustrate our methods by numerical examples and compared with\nother existing literature as appropriate. Our method is computationally simple\nand often outperforms the other methods."}, {"title": "Test Martingales for bounded random variables", "abstract": "Given a random sample from a random variable $T$ which is bounded from above,\n$T\\le\\tau$ a.s., we define processes that are positive supermartingales if\n$E(T)\\ge\\mu$. Such processes are called test martingales. Tests of the\nsupermartingale hypothesis implicitly test the hypothesis $H_0:E(T)\\ge\\mu$. We\nconstruct test martingales that lead to tests with power 1. We also construct\nconfidence upper bounds. We extend the techniques to testing $H_0:E(T)=\\mu$ and\nconstructing confidence intervals.\n  In financial auditing random sampling is proposed as one of the possible\ntechniques to gather enough assurance to be able to state that there is no\n'material' misstatement in a financial report. The goal of our work is to\nprovide a mathematical context that could represent such process of gathering\nassurance by means of repeated random sampling."}, {"title": "Testing normality using the summary statistics with application to meta-analysis", "abstract": "As the most important tool to provide high-level evidence-based medicine,\nresearchers can statistically summarize and combine data from multiple studies\nby conducting meta-analysis. In meta-analysis, mean differences are frequently\nused effect size measurements to deal with continuous data, such as the Cohen's\nd statistic and Hedges' g statistic values. To calculate the mean difference\nbased effect sizes, the sample mean and standard deviation are two essential\nsummary measures. However, many of the clinical reports tend not to directly\nrecord the sample mean and standard deviation. Instead, the sample size,\nmedian, minimum and maximum values and/or the first and third quartiles are\nreported. As a result, researchers have to transform the reported information\nto the sample mean and standard deviation for further compute the effect size.\nSince most of the popular transformation methods were developed upon the\nnormality assumption of the underlying data, it is necessary to perform a\npre-test before transforming the summary statistics. In this article, we had\nintroduced test statistics for three popular scenarios in meta-analysis. We\nsuggests medical researchers to perform a normality test of the selected\nstudies before using them to conduct further analysis. Moreover, we applied\nthree different case studies to demonstrate the usage of the newly proposed\ntest statistics. The real data case studies indicate that the new test\nstatistics are easy to apply in practice and by following the recommended path\nto conduct the meta-analysis, researchers can obtain more reliable conclusions."}, {"title": "The Lazy Bootstrap. A Fast Resampling Method for Evaluating Latent Class Model Fit", "abstract": "The latent class model is a powerful unsupervised clustering algorithm for\ncategorical data. Many statistics exist to test the fit of the latent class\nmodel. However, traditional methods to evaluate those fit statistics are not\nalways useful. Asymptotic distributions are not always known, and empirical\nreference distributions can be very time consuming to obtain. In this paper we\npropose a fast resampling scheme with which any type of model fit can be\nassessed. We illustrate it here on the latent class model, but the methodology\ncan be applied in any situation.\n  The principle behind the lazy bootstrap method is to specify a statistic\nwhich captures the characteristics of the data that a model should capture\ncorrectly. If those characteristics in the observed data and in model-generated\ndata are very different we can assume that the model could not have produced\nthe observed data. With this method we achieve the flexibility of tests from\nthe Bayesian framework, while only needing maximum likelihood estimates. We\nprovide a step-wise algorithm with which the fit of a model can be assessed\nbased on the characteristics we as researcher find important. In a Monte Carlo\nstudy we show that the method has very low type I errors, for all illustrated\nstatistics. Power to reject a model depended largely on the type of statistic\nthat was used and on sample size. We applied the method to an empirical data\nset on clinical subgroups with risk of Myocardial infarction and compared the\nresults directly to the parametric bootstrap. The results of our method were\nhighly similar to those obtained by the parametric bootstrap, while the\nrequired computations differed three orders of magnitude in favour of our\nmethod."}, {"title": "Sampling techniques for big data analysis in finite population inference", "abstract": "In analyzing big data for finite population inference, it is critical to\nadjust for the selection bias in the big data. In this paper, we propose two\nmethods of reducing the selection bias associated with the big data sample. The\nfirst method uses a version of inverse sampling by incorporating auxiliary\ninformation from external sources, and the second one borrows the idea of data\nintegration by combining the big data sample with an independent probability\nsample. Two simulation studies show that the proposed methods are unbiased and\nhave better coverage rates than their alternatives. In addition, the proposed\nmethods are easy to implement in practice."}, {"title": "A scalable estimate of the extra-sample prediction error via approximate leave-one-out", "abstract": "The paper considers the problem of out-of-sample risk estimation under the\nhigh dimensional settings where standard techniques such as $K$-fold cross\nvalidation suffer from large biases. Motivated by the low bias of the\nleave-one-out cross validation (LO) method, we propose a computationally\nefficient closed-form approximate leave-one-out formula (ALO) for a large class\nof regularized estimators. Given the regularized estimate, calculating ALO\nrequires minor computational overhead. With minor assumptions about the data\ngenerating process, we obtain a finite-sample upper bound for $|\\text{LO} -\n\\text{ALO}|$. Our theoretical analysis illustrates that $|\\text{LO} -\n\\text{ALO}| \\rightarrow 0$ with overwhelming probability, when $n,p \\rightarrow\n\\infty$, where the dimension $p$ of the feature vectors may be comparable with\nor even greater than the number of observations, $n$. Despite the\nhigh-dimensionality of the problem, our theoretical results do not require any\nsparsity assumption on the vector of regression coefficients. Our extensive\nnumerical experiments show that $|\\text{LO} - \\text{ALO}|$ decreases as $n,p$\nincrease, revealing the excellent finite sample performance of ALO. We further\nillustrate the usefulness of our proposed out-of-sample risk estimation method\nby an example of real recordings from spatially sensitive neurons (grid cells)\nin the medial entorhinal cortex of a rat."}, {"title": "Multivariate Specification Tests Based on a Dynamic Rosenblatt Transform", "abstract": "This paper considers parametric model adequacy tests for nonlinear\nmultivariate dynamic models. It is shown that commonly used Kolmogorov-type\ntests do not take into account cross-sectional nor time-dependence structure,\nand a test, based on multi-parameter empirical processes, is proposed that\novercomes these problems. The tests are applied to a nonlinear LSTAR-type model\nof joint movements of UK output growth and interest rate spreads. A simulation\nexperiment illustrates the properties of the tests in finite samples.\nAsymptotic properties of the test statistics under the null of correct\nspecification and under the local alternative, and justification of a\nparametric bootstrap to obtain critical values, are provided."}, {"title": "Change Point Analysis of Correlation in Non-stationary Time Series", "abstract": "A restrictive assumption in change point analysis is \"stationarity under the\nnull hypothesis of no change-point\", which is crucial for asymptotic theory but\nnot very realistic from a practical point of view. For example, if change point\nanalysis for correlations is performed, it is not necessarily clear that the\nmean, marginal variance or higher order moments are constant, even if there is\nno change in the correlation. This paper develops change point analysis for the\ncorrelation structures under less restrictive assumptions. In contrast to\nprevious work, our approach does not require that the mean, variance and fourth\norder joint cumulants are constant under the null hypothesis. Moreover, we also\naddress the problem of detecting relevant change points."}, {"title": "On the construction of unbiased estimators for the group testing problem", "abstract": "Debiased estimation has long been an area of research in the group testing\nliterature. This has led to the development of several estimators with the goal\nof bias minimization and, recently, an unbiased estimator based on sequential\nbinomial sampling. Previous research, however, has focused heavily on the\nsimple case where no misclassification is assumed and only one trait is to be\ntested. In this paper, we consider the problem of unbiased estimation in these\nbroader areas, giving constructions of such estimators for several cases. We\nshow that, outside of the standard case addressed previously in the literature,\nit is impossible to find any proper unbiased estimator, that is, an estimator\ngiving only values in the parameter space. This is shown to hold generally\nunder any binomial or multinomial sampling plans"}, {"title": "A Distribution-Free Test of Independence and Its Application to Variable Selection", "abstract": "Motivated by the importance of measuring the association between the response\nand predictors in high dimensional data, In this article, we propose a new mean\nvariance test of independence between a categorical random variable and a\ncontinuous one based on mean variance index. The mean variance index is zero if\nand only if two variables are independent. Under the independence, we derive an\nexplicit form of its asymptotic null distribution, which provides us with an\nefficient and fast way to compute the empirical p-value in practice. The number\nof classes of the categorical variable is allowed to diverge slowly to the\ninfinity. It is essentially a rank test and thus distribution-free. No\nassumption on the distributions of two random variables is required and the\ntest statistic is invariant under one-to-one transformations. It is resistent\nto heavy-tailed distributions and extreme values. We assess its performance by\nMonte Carlo simulations and demonstrate that the proposed test achieves a\nhigher power in comparison with the existing tests. We apply the proposed MV\ntest to a high dimensional colon cancer gene expression data to detect the\nsignificant genes associated with the tissue syndrome."}, {"title": "Solving estimating equations with copulas", "abstract": "Thanks to their ability to capture complex dependence structures, copulas are\nfrequently used to glue random variables into a joint model with arbitrary\nmarginal distributions. More recently, they have been applied to solve\nstatistical learning problems such as regression or classification. Framing\nsuch approaches as solutions of estimating equations, we generalize them in a\nunified framework. We can then obtain simultaneous, coherent inferences across\nmultiple regression-like problems. We derive consistency, asymptotic normality,\nand validity of the bootstrap for corresponding estimators. The conditions\nallow for both continuous and discrete data as well as parametric,\nnonparametric, and semiparametric estimators of the copula and marginal\ndistributions. The versatility of this methodology is illustrated by several\ntheoretical examples, a simulation study, and an application to financial\nportfolio allocation."}, {"title": "Another Look at Statistical Calibration: A Non-Asymptotic Theory and Prediction-Oriented Optimality", "abstract": "We provide another look at the statistical calibration problem in computer\nmodels. This viewpoint is inspired by two overarching practical considerations\nof computer models: (i) many computer models are inadequate for perfectly\nmodeling physical systems, even with the best-tuned calibration parameters;\n(ii) only a finite number of data points are available from the physical\nexperiment associated with a computer model. Following this new line of\nthinking, we provide a non-asymptotic theory and derive a prediction-oriented\ncalibration method. Our calibration method minimizes the predictive mean\nsquared error for a finite sample size with statistical guarantees. We\nintroduce an algorithm to perform the proposed calibration method and connect\nit to existing Bayesian calibration methods. Synthetic and real examples are\nprovided to corroborate the derived theory and illustrate some advantages of\nthe proposed calibration method."}, {"title": "Practical Bayesian Modeling and Inference for Massive Spatial Datasets On Modest Computing Environments", "abstract": "With continued advances in Geographic Information Systems and related\ncomputational technologies, statisticians are often required to analyze very\nlarge spatial datasets. This has generated substantial interest over the last\ndecade, already too vast to be summarized here, in scalable methodologies for\nanalyzing large spatial datasets. Scalable spatial process models have been\nfound especially attractive due to their richness and flexibility and,\nparticularly so in the Bayesian paradigm, due to their presence in hierarchical\nmodel settings. However, the vast majority of research articles present in this\ndomain have been geared toward innovative theory or more complex model\ndevelopment. Very limited attention has been accorded to approaches for easily\nimplementable scalable hierarchical models for the practicing scientist or\nspatial analyst. This article is submitted to the Practice section of the\njournal with the aim of developing massively scalable Bayesian approaches that\ncan rapidly deliver Bayesian inference on spatial process that are practically\nindistinguishable from inference obtained using more expensive alternatives. A\nkey emphasis is on implementation within very standard (modest) computing\nenvironments (e.g., a standard desktop or laptop) using easily available\nstatistical software packages without requiring message-parsing interfaces or\nparallel programming paradigms. Key insights are offered regarding assumptions\nand approximations concerning practical efficiency."}, {"title": "Predicting outcomes for games of skill by redefining what it means to win", "abstract": "The Elo rating system is a highly successful ranking algorithm for games of\nskill where, by construction, one team wins and the other loses. A primary\nlimitation of the original Elo algorithm is its inability to predict\ninformation beyond a match's win-loss probability. Specifically, the victor is\nawarded the same point bounty if he beats a team by 1 point or 10 points; only\nthe rating difference between the team and its opponent affects the match\nbounty. In this work, we explain that Elo ratings and predictions can be\nnaturally extended to include margin-of-victory information by simply\nredefining \"what it means to win.\" We create ratings for each value of the\nmargin-of-victory and use these ratings to predict the full distribution of\npoint spread outcomes for matches which have not yet been played."}, {"title": "A novel approach to estimate the Cox model with temporal covariates and its application to medical cost data", "abstract": "We propose a novel approach to estimate the Cox model with temporal\ncovariates. Our new approach treats the temporal covariates as arising from a\nlongitudinal process which is modeled jointly with the event time. Different\nfrom the literature, the longitudinal process in our model is specified as a\nbounded variational process and determined by a family of Initial Value\nProblems associated with an Ordinary Differential Equation. Our specification\nhas the advantage that only the observation of the temporal covariates at the\ntime to event and the time to event itself are required to fit the model, while\nit is fine but not necessary to have more longitudinal observations. This fact\nmakes our approach very useful for many medical outcome datasets, like the New\nYork State Statewide Planning and Research Cooperative System and the National\nInpatient Sample, where it is important to find the hazard rate of being\ndischarged given the accumulative cost but only the total cost at the discharge\ntime is available due to the protection of patient information. Our estimation\nprocedure is based on maximizing the full information likelihood function. The\nresulting estimators are shown to be consistent and asymptotically normally\ndistributed. Variable selection techniques, like Adaptive LASSO, can be easily\nmodified and incorporated into our estimation procedure. The oracle property is\nverified for the resulting estimator of the regression coefficients.\nSimulations and a real example illustrate the practical utility of the proposed\nmodel. Finally, a couple of potential extensions of our approach are discussed."}, {"title": "Stochastic Kriging for Inadequate Simulation Models", "abstract": "Stochastic kriging is a popular metamodeling technique for representing the\nunknown response surface of a simulation model. However, the simulation model\nmay be inadequate in the sense that there may be a non-negligible discrepancy\nbetween it and the real system of interest. Failing to account for the model\ndiscrepancy may conceivably result in erroneous prediction of the real system's\nperformance and mislead the decision-making process. This paper proposes a\nmetamodel that extends stochastic kriging to incorporate the model discrepancy.\nBoth the simulation outputs and the real data are used to characterize the\nmodel discrepancy. The proposed metamodel can provably enhance the prediction\nof the real system's performance. We derive general results for experiment\ndesign and analysis, and demonstrate the advantage of the proposed metamodel\nrelative to competing methods. Finally, we study the effect of Common Random\nNumbers (CRN). The use of CRN is well known to be detrimental to the prediction\naccuracy of stochastic kriging in general. By contrast, we show that the effect\nof CRN in the new context is substantially more complex. The use of CRN can be\neither detrimental or beneficial depending on the interplay between the\nmagnitude of the observation errors and other parameters involved."}, {"title": "Bayes Calculations from Quantile Implied Likelihood", "abstract": "In statistical practice, a realistic Bayesian model for a given data set can\nbe defined by a likelihood function that is analytically or computationally\nintractable, due to large data sample size, high parameter dimensionality, or\ncomplex likelihood functional form. This in turn poses challenges to the\ncomputation and inference of the posterior distribution of the model\nparameters. For such a model, a tractable likelihood function is introduced\nwhich approximates the exact likelihood through its quantile function. It is\ndefined by an asymptotic chi-square confidence distribution for a pivotal\nquantity, which is generated by the asymptotic normal distribution of the\nsample quantiles given model parameters. This Quantile Implied Likelihood (QIL)\ngives rise to an approximate posterior distribution which can be estimated by\nusing penalized log-likelihood maximization or any suitable Monte Carlo\nalgorithm. The QIL approach to Bayesian Computation is illustrated through the\nBayesian analysis of simulated and real data sets having sample sizes that\nreach the millions. The analyses involve various models for univariate or\nmultivariate iid or non-iid data, with low or high parameter dimensionality,\nmany of which are defined by intractable likelihoods. The probability models\ninclude the Student's t, g-and-h, and g-and-k distributions; the Bayesian logit\nregression model with many covariates; exponential random graph model, a\ndoubly-intractable model for networks; the multivariate skew normal model, for\nrobust inference of the inverse-covariance matrix when it is large relative to\nthe sample size; and the Wallenius distribution model."}, {"title": "Assessing Prediction Error at Interpolation and Extrapolation Points", "abstract": "Common model selection criteria, such as $AIC$ and its variants, are based on\nin-sample prediction error estimators. However, in many applications involving\npredicting at interpolation and extrapolation points, in-sample error cannot be\nused for estimating the prediction error. In this paper new prediction error\nestimators, $tAI$ and $Loss(w_{t})$ are introduced. These estimators generalize\nprevious error estimators, however are also applicable for assessing prediction\nerror in cases involving interpolation and extrapolation. Based on the\nprediction error estimators, two model selection criteria with the same spirit\nas $AIC$ are suggested. The advantages of our suggested methods are\ndemonstrated in simulation and real data analysis of studies involving\ninterpolation and extrapolation in a Linear Mixed Model framework."}, {"title": "Randomization Tests that Condition on Non-Categorical Covariate Balance", "abstract": "A benefit of randomized experiments is that covariate distributions of\ntreatment and control groups are balanced on average, resulting in simple\nunbiased estimators for treatment effects. However, it is possible that a\nparticular randomization yields covariate imbalances that researchers want to\naddress in the analysis stage through adjustment or other methods. Here we\npresent a randomization test that conditions on covariate balance by only\nconsidering treatment assignments that are similar to the observed one in terms\nof covariate balance. Previous conditional randomization tests have only\nallowed for categorical covariates, while our randomization test allows for any\ntype of covariate. Through extensive simulation studies, we find that our\nconditional randomization test is more powerful than unconditional\nrandomization tests and other conditional tests. Furthermore, we find that our\nconditional randomization test is valid (1) unconditionally across levels of\ncovariate balance, and (2) conditional on particular levels of covariate\nbalance. Meanwhile, unconditional randomization tests are valid for (1) but not\n(2). Finally, we find that our conditional randomization test is similar to a\nrandomization test that uses a model-adjusted test statistic."}, {"title": "INLA goes extreme: Bayesian tail regression for the estimation of high spatio-temporal quantiles", "abstract": "This work has been motivated by the challenge of the 2017 conference on\nExtreme-Value Analysis (EVA2017), with the goal of predicting daily\nprecipitation quantiles at the $99.8\\%$ level for each month at observed and\nunobserved locations. We here develop a Bayesian generalized additive modeling\nframework tailored to estimate complex trends in marginal extremes observed\nover space and time. Our approach is based on a set of regression equations\nlinked to the exceedance probability above a high threshold and to the size of\nthe excess, the latter being modeled using the generalized Pareto (GP)\ndistribution suggested by Extreme-Value Theory. Latent random effects are\nmodeled additively and semi-parametrically using Gaussian process priors, which\nprovides high flexibility and interpretability. Fast and accurate estimation of\nposterior distributions may be performed thanks to the Integrated Nested\nLaplace approximation (INLA), efficiently implemented in the R-INLA software,\nwhich we also use for determining a nonstationary threshold based on a model\nfor the body of the distribution. We show that the GP distribution meets the\ntheoretical requirements of INLA, and we then develop a penalized complexity\nprior specification for the tail index, which is a crucial parameter for\nextrapolating tail event probabilities. This prior concentrates mass close to a\nlight exponential tail while allowing heavier tails by penalizing the distance\nto the exponential distribution. We illustrate this methodology through the\nmodeling of spatial and seasonal trends in daily precipitation data provided by\nthe EVA2017 challenge. Capitalizing on R-INLA's fast computation capacities and\nlarge distributed computing resources, we conduct an extensive cross-validation\nstudy to select model parameters governing the smoothness of trends. Our\nresults outperform simple benchmarks and are comparable to the best-scoring\napproach."}, {"title": "Re-thinking non-inferiority: a practical trial design for optimising treatment duration", "abstract": "Background: trials to identify the minimal effective treatment duration are\nneeded in different therapeutic areas, including bacterial infections, TB and\nHepatitis--C. However, standard non-inferiority designs have several\nlimitations, including arbitrariness of non-inferiority margins, choice of\nresearch arms and very large sample sizes.\n  Methods: we recast the problem of finding an appropriate non-inferior\ntreatment duration in terms of modelling the entire duration-response curve\nwithin a pre-specified range. We propose a multi-arm randomised trial design,\nallocating patients to different treatment durations. We use fractional\npolynomials and spline-based methods to flexibly model the duration-response\ncurve. We compare different methods in terms of a scaled version of the area\nbetween true and estimated prediction curves. We evaluate sensitivity to key\ndesign parameters, including sample size, number and position of arms.\n  Results: a total sample size of $\\sim 500$ patients divided into a moderate\nnumber of equidistant arms (5-7) is sufficient to estimate the\nduration-response curve within a $5\\%$ error margin in $95\\%$ of the\nsimulations. Fractional polynomials provide similar or better results than\nspline-based methods in most scenarios.\n  Conclusions: our proposed practical randomised trial design is an alternative\nto standard non-inferiority designs, avoiding many of their limitations, and\nyet being fairly robust to different possible duration-response curves. The\ntrial outcome is the whole duration-response curve, which could be used by\nclinicians and policy makers to make informed decisions, facilitating a move\naway from a forced binary hypothesis testing paradigm."}, {"title": "Exceedance-based nonlinear regression of tail dependence", "abstract": "The probability and structure of co-occurrences of extreme values in\nmultivariate data may critically depend on auxiliary information provided by\ncovariates. In this contribution, we develop a flexible generalized additive\nmodeling framework based on high threshold exceedances for estimating\ncovariate-dependent joint tail characteristics for regimes of asymptotic\ndependence and asymptotic independence. The framework is based on suitably\ndefined marginal pretransformations and projections of the random vector along\nthe directions of the unit simplex, which lead to convenient univariate\nrepresentations of multivariate exceedances based on the exponential\ndistribution. Good performance of our estimators of a nonparametrically\ndesigned influence of covariates on extremal coefficients and tail dependence\ncoefficients are shown through a simulation study. We illustrate the usefulness\nof our modeling framework on a large dataset of nitrogen dioxide measurements\nrecorded in France between 1999 and 2012, where we use the generalized additive\nframework for modeling marginal distributions and tail dependence in monthly\nmaxima. Our results imply asymptotic independence of data observed at different\nstations, and we find that the estimated coefficients of tail dependence\ndecrease as a function of spatial distance and show distinct patterns for\ndifferent years and for different types of stations (traffic vs. background)."}, {"title": "A Wilks' theorem for grouped data", "abstract": "Consider $n$ independent measurements, with the additional information of the\ntimes at which measurements are performed. This paper deals with testing\nstatistical hypotheses when $n$ is large and only a small amount of\nobservations concentrated in short time intervals are relevant to the study. We\ndefine a testing procedure in terms of multiple likelihood ratio (LR)\nstatistics obtained by splitting the observations into groups, and in\naccordance with the following principles: P1) each LR statistic is formed by\ngathering the data included in $G$ consecutive vectors of observations, where\n$G$ is a suitable time window defined a priori with respect to an arbitrary\nchoice of the `origin of time'; P2) the null statistical hypothesis is rejected\nonly if at least $k$ LR statistics are sufficiently small, for a suitable\nchoice of $k$. We show that the application of the classical Wilks' theorem may\nbe affected by the arbitrary choice of the \"origin of time\", in connection with\nP1). We then introduce a Wilks' theorem for grouped data which leads to a\ntesting procedure that overcomes the problem of the arbitrary choice of the\n`origin of time', while fulfilling P1) and P2). Such a procedure is more\npowerful than the corresponding procedure based on Wilks' theorem."}, {"title": "The additive hazard estimator is consistent for continuous-time marginal structural models", "abstract": "Marginal structural models (MSMs) allow for causal analysis of longitudinal\ndata. The MSMs were originally developed as discrete time models. Recently,\ncontinuous-time MSMs were presented as a conceptually appealing alternative for\nsurvival analysis. In applied analyses, it is often assumed that the\ntheoretical treatment weights are known, but these weights are usually unknown\nand must be estimated from the data. Here we provide a sufficient condition for\na class of continuous-time MSMs to be consistent even when the weights are\nestimated, and we show how additive hazard models can be used to estimate such\nweights. Our results suggest that the continuous-time weights perform better\nthan IPTW when the underlying treatment process is continuous. Furthermore, we\nmay wish to transform effect estimates of hazards to other scales that are\neasier to interpret causally. We show that a general transformation strategy\ncan be used on weighted cumulative hazard estimates to obtain a range of other\nparameters in survival analysis, and demonstrate how this strategy can be\napplied on data using our R packages ahw and transform.hazards."}, {"title": "An Imputation-Consistency Algorithm for High-Dimensional Missing Data Problems and Beyond", "abstract": "Missing data are frequently encountered in high-dimensional problems, but\nthey are usually difficult to deal with using standard algorithms, such as the\nexpectation-maximization (EM) algorithm and its variants. To tackle this\ndifficulty, some problem-specific algorithms have been developed in the\nliterature, but there still lacks a general algorithm. This work is to fill the\ngap: we propose a general algorithm for high-dimensional missing data problems.\nThe proposed algorithm works by iterating between an imputation step and a\nconsistency step. At the imputation step, the missing data are imputed\nconditional on the observed data and the current estimate of parameters; and at\nthe consistency step, a consistent estimate is found for the minimizer of a\nKullback-Leibler divergence defined on the pseudo-complete data. For high\ndimensional problems, the consistent estimate can be found under sparsity\nconstraints. The consistency of the averaged estimate for the true parameter\ncan be established under quite general conditions. The proposed algorithm is\nillustrated using high-dimensional Gaussian graphical models, high-dimensional\nvariable selection, and a random coefficient model."}, {"title": "Mixtures of Factor Analyzers with Fundamental Skew Symmetric Distributions", "abstract": "Mixtures of factor analyzers (MFA) provide a powerful tool for modelling\nhigh-dimensional datasets. In recent years, several generalizations of MFA have\nbeen developed where the normality assumption of the factors and/or of the\nerrors was relaxed to allow for skewness in the data. However, due to the form\nof the adopted component densities, the distribution of the factors/errors in\nmost of these models is typically limited to modelling skewness oncentrated in\na single direction. Here, we introduce a more flexible finite mixture of factor\nanalyzers based on the class of scale mixtures of canonical fundamental skew\nnormal (SMCFUSN) distributions. This very general class of skew distributions\ncan capture various types of skewness and asymmetry in the data. In particular,\nthe proposed mixture model of SMCFUSN factor analyzers(SMCFUSNFA) can\nsimultaneously accommodate multiple directions of skewness. As such, it\nencapsulates many commonly used models as special and/or limiting cases, such\nas models of some versions of skew normal and skew t-factor analyzers, and skew\nhyperbolic factor analyzers. For illustration, we focus on the t-distribution\nmember of the class of SMCFUSN distributions, leading to mixtures of canonical\nfundamental skew t-factor analyzers (CFUSTFA). Parameter estimation can be\ncarried out by maximum likelihood via an EM-type algorithm. The usefulness and\npotential of the proposed model are demonstrated using two real datasets."}, {"title": "Interpolating Population Distributions using Public-use Data: An Application to Income Segregation using American Community Survey Data", "abstract": "Income segregation measures the extent to which households choose to live\nnear other households with similar incomes. Sociologists theorize that income\nsegregation can exacerbate the impacts of income inequality, and have developed\nindices to measure it at the metro area level, including the information theory\nindex introduced in \\citet{reardon2011income}, and the divergence index\npresented in \\citet{roberto2015divergence}. To study their differences, we\nconstruct both indices using recent American Community Survey (ACS) estimates\nof features of the income distribution. Since the elimination of the decennial\ncensus long form, methods of computing these estimates must be updated to use\nACS estimates and account for survey error. We propose a model-based method to\ninterpolate estimates of features of the income distribution that accounts for\nthis error. This method improves on previous approaches by allowing for the use\nof more types of estimates, and by providing uncertainty quantification. We\napply this method to estimate U.S. census tract-level income distributions\nusing ACS tabulations, and in turn use these to construct both income\nsegregation indices. We find major differences between the two indices in the\nrelative ranking of metro areas, as well as differences in how both indices\ncorrelate with the Gini index."}, {"title": "Correlation Estimation System Minimization Compared to Least Squares Minimization in Simple Linear Regression", "abstract": "A general method of minimization using correlation coefficients and order\nstatistics is evaluated relative to least squares procedures in the estimation\nof parameters for normal data in simple linear regression."}, {"title": "A Bayesian Approach to Multi-State Hidden Markov Models: Application to Dementia Progression", "abstract": "People are living longer than ever before, and with this arises new\ncomplications and challenges for humanity. Among the most pressing of these\nchallenges is of understanding the role of aging in the development of\ndementia. This paper is motivated by the Mayo Clinic Study of Aging data for\n4742 subjects since 2004, and how it can be used to draw inference on the role\nof aging in the development of dementia. We construct a hidden Markov model\n(HMM) to represent progression of dementia from states associated with the\nbuildup of amyloid plaque in the brain, and the loss of cortical thickness. A\nhierarchical Bayesian approach is taken to estimate the parameters of the HMM\nwith a truly time-inhomogeneous infinitesimal generator matrix, and response\nfunctions of the continuous-valued biomarker measurements are cut-point\nagnostic. A Bayesian approach with these features could be useful in many\ndisease progression models. Additionally, an approach is illustrated for\ncorrecting a common bias in delayed enrollment studies, in which some or all\nsubjects are not observed at baseline. Standard software is incapable of\naccounting for this critical feature, so code to perform the estimation of the\nmodel described below is made available online."}, {"title": "More Efficient Estimation for Logistic Regression with Optimal Subsample", "abstract": "In this paper, we propose improved estimation method for logistic regression\nbased on subsamples taken according the optimal subsampling probabilities\ndeveloped in Wang et al. 2018 Both asymptotic results and numerical results\nshow that the new estimator has a higher estimation efficiency. We also develop\na new algorithm based on Poisson subsampling, which does not require to\napproximate the optimal subsampling probabilities all at once. This is\ncomputationally advantageous when available random-access memory is not enough\nto hold the full data. Interestingly, asymptotic distributions also show that\nPoisson subsampling produces a more efficient estimator if the sampling rate,\nthe ratio of the subsample size to the full data sample size, does not converge\nto zero. We also obtain the unconditional asymptotic distribution for the\nestimator based on Poisson subsampling. The proposed approach requires to use a\npilot estimator to correct biases of un-weighted estimators. We further show\nthat even if the pilot estimator is inconsistent, the resulting estimators are\nstill consistent and asymptotically normal if the model is correctly specified."}, {"title": "Data-adaptive doubly robust instrumental variable methods for treatment effect heterogeneity", "abstract": "We consider the estimation of the average treatment effect in the treated as\na function of baseline covariates, where there is a valid (conditional)\ninstrument.\n  We describe two doubly robust (DR) estimators: a locally efficient\ng-estimator, and a targeted minimum loss-based estimator (TMLE). These two DR\nestimators can be viewed as generalisations of the two-stage least squares\n(TSLS) method to semi-parametric models that make weaker assumptions. We\nexploit recent theoretical results that extend to the g-estimator the use of\ndata-adaptive fits for the nuisance parameters.\n  A simulation study is used to compare standard TSLS with the two DR\nestimators' finite-sample performance, (1) when fitted using parametric\nnuisance models, and (2) using data-adaptive nuisance fits, obtained from the\nSuper Learner, an ensemble machine learning method.\n  Data-adaptive DR estimators have lower bias and improved coverage, when\ncompared to incorrectly specified parametric DR estimators and TSLS. When the\nparametric model for the treatment effect curve is correctly specified, the\ng-estimator outperforms all others, but when this model is misspecified, TMLE\nperforms best, while TSLS can result in large biases and zero coverage.\n  Finally, we illustrate the methods by reanalysing the COPERS (COping with\npersistent Pain, Effectiveness Research in Self-management) trial to make\ninference about the causal effect of treatment actually received, and the\nextent to which this is modified by depression at baseline."}, {"title": "Forecasting under model uncertainty:Non-homogeneous hidden Markov models with Polya-Gamma data augmentation", "abstract": "We consider two-state Non-Homogeneous Hidden Markov Models (NHHMMs) for\nforecasting univariate time series. Given a set of predictors, the time series\nare modeled via predictive regressions with state dependent coefficients and\ntime-varying transition probabilities that depend on the predictors via a\nlogistic function. In a hidden Markov setting, inference for logistic\nregression coefficients becomes complicated and in some cases impossible due to\nconvergence issues. In this paper, we aim to address this problem using a new\nlatent variable scheme that utilizes the P\\'{o}lya-Gamma class of\ndistributions. We allow for model uncertainty regarding the predictors that\naffect the series both linearly -- in the mean -- and non-linearly -- in the\ntransition matrix. Predictor selection and inference on the model parameters\nare based on a MCMC scheme with reversible jump steps. Single-step and\nmultiple-steps-ahead predictions are obtained by the most probable model,\nmedian probability model or a Bayesian Model Averaging approach. Using\nsimulation experiments, we illustrate the performance of our algorithm in\nvarious setups, in terms of mixing properties, model selection and predictive\nability. An empirical study on realized volatility data shows that our\nmethodology gives improved forecasts compared to benchmark models."}, {"title": "A Minimum Message Length Criterion for Robust Linear Regression", "abstract": "This paper applies the minimum message length principle to inference of\nlinear regression models with Student-t errors. A new criterion for variable\nselection and parameter estimation in Student-t regression is proposed. By\nexploiting properties of the regression model, we derive a suitable\nnon-informative proper uniform prior distribution for the regression\ncoefficients that leads to a simple and easy-to-apply criterion. Our proposed\ncriterion does not require specification of hyperparameters and is invariant\nunder both full rank transformations of the design matrix and linear\ntransformations of the outcomes. We compare the proposed criterion with several\nstandard model selection criteria, such as the Akaike information criterion and\nthe Bayesian information criterion, on simulations and real data with promising\nresults."}, {"title": "Bootstrap validation of links of a minimum spanning tree", "abstract": "We describe two different bootstrap methods applied to the detection of a\nminimum spanning tree obtained from a set of multivariate variables. We show\nthat two different bootstrap procedures provide partly distinct information\nthat can be highly informative about the investigated complex system. Our case\nstudy, based on the investigation of daily returns of a portfolio of stocks\ntraded in the US equity markets, shows the degree of robustness and\ncompleteness of the information extracted with popular information filtering\nmethods such as the minimum spanning tree and the planar maximally filtered\ngraph. The first method performs a \"row bootstrap\" whereas the second method\nperforms a \"pair bootstrap\". We show that the parallel use of the two methods\nis suggested especially for complex systems presenting both a nested\nhierarchical organization together with the presence of global feedback\nchannels."}, {"title": "A whitening approach to probabilistic canonical correlation analysis for omics data integration", "abstract": "Background: Canonical correlation analysis (CCA) is a classic statistical\ntool for investigating complex multivariate data. Correspondingly, it has found\nmany diverse applications, ranging from molecular biology and medicine to\nsocial science and finance. Intriguingly, despite the importance and\npervasiveness of CCA, only recently a probabilistic understanding of CCA is\ndeveloping, moving from an algorithmic to a model-based perspective and\nenabling its application to large-scale settings.\n  Results: Here, we revisit CCA from the perspective of statistical whitening\nof random variables and propose a simple yet flexible probabilistic model for\nCCA in the form of a two-layer latent variable generative model. The advantages\nof this variant of probabilistic CCA include non-ambiguity of the latent\nvariables, provisions for negative canonical correlations, possibility of\nnon-normal generative variables, as well as ease of interpretation on all\nlevels of the model. In addition, we show that it lends itself to\ncomputationally efficient estimation in high-dimensional settings using\nregularized inference. We test our approach to CCA analysis in simulations and\napply it to two omics data sets illustrating the integration of gene expression\ndata, lipid concentrations and methylation levels.\n  Conclusions: Our whitening approach to CCA provides a unifying perspective on\nCCA, linking together sphering procedures, multivariate regression and\ncorresponding probabilistic generative models. Furthermore, we offer an\nefficient computer implementation in the \"whitening\" R package available at\nhttps://CRAN.R-project.org/package=whitening ."}, {"title": "A General Framework For Frequentist Model Averaging", "abstract": "Model selection strategies have been routinely employed to determine a model\nfor data analysis in statistics, and further study and inference then often\nproceed as though the selected model were the true model that were known a\npriori. This practice does not account for the uncertainty introduced by the\nselection process and the fact that the selected model can possibly be a wrong\none. Model averaging approaches try to remedy this issue by combining\nestimators for a set of candidate models. Specifically, instead of deciding\nwhich model is the 'right' one, a model averaging approach suggests to fit a\nset of candidate models and average over the estimators using certain data\nadaptive weights. In this paper we establish a general frequentist model\naveraging framework that does not set any restrictions on the set of candidate\nmodels. It greatly broadens the scope of the existing methodologies under the\nfrequentist model averaging development. Assuming the data is from an unknown\nmodel, we derive the model averaging estimator and study its limiting\ndistributions and related predictions while taking possible modeling biases\ninto account. We propose a set of optimal weights to combine the individual\nestimators so that the expected mean squared error of the average estimator is\nminimized. Simulation studies are conducted to compare the performance of the\nestimator with that of the existing methods. The results show the benefits of\nthe proposed approach over traditional model selection approaches as well as\nexisting model averaging methods."}, {"title": "Simultaneous Rank Tests in Analysis of Covariance Based on Pairwise Ranking", "abstract": "Nonparametric tests provide robust and powerful alternatives to the\ncorresponding least squares methods. There are two approaches to nonparametric\npairwise comparisons of treatment effects, the method based on pairwise\nrankings and the method based on overall ranking. The former is generally\nrecommended in the literature because of its strong control of familywise error\nrate. However, this method is developed only for one-way layouts and randomized\ncomplete blocks. By combining the method of aligned ranks and pairwise ranking,\nwe extend the Steel-Dwass pairwise comparisons to the analysis of covariance\nand factorial models for both one-sided and two-sided comparisons as well as\ntesting for treatment versus control. Unlike the traditional two-sample\nstandardization of test statistics, we propose a weighted estimate of the scale\nparameter for ranks and show through simulation that it has superior small\nsample performance by controlling the familywise error rate at nominal level.\nThis method provides an improvement for large sample approximation of\nSteel-Dwass method for one-way layouts. The marginal and joint asymptotic\ndistributions are derived and power comparisons are made with the method of\naligned rank transformation and the least squares method."}, {"title": "Estimating Diffusion With Compound Poisson Jumps Based On Self-normalized Residuals", "abstract": "We consider parametric estimation of the continuous part of a class of\nergodic diffusions with jumps based on high-frequency samples. Various papers\npreviously proposed threshold based methods, which enable us to distinguish\nwhether observed increments have jumps or not at each small-time interval,\nhence to estimate the unknown parameters separately. However, a data-adapted\nand quantitative choice of the threshold parameter is known to be a subtle and\nsensitive problem. In this paper, we present a simple alternative based on the\nJarque-Bera normality test for the Euler residuals. Different from the\nthreshold based method, the proposed method does not require any sensitive fine\ntuning, hence is of practical value. It is shown that under suitable conditions\nthe proposed estimator is asymptotically equivalent to an estimator constructed\nby the unobserved fluctuation of the continuous part of the solution process,\nhence is asymptotically efficient. Some numerical experiments are conducted to\nobserve finite-sample performance of the proposed method."}, {"title": "Exact and efficient inference for Partial Bayes problems", "abstract": "Bayesian methods are useful for statistical inference. However, real-world\nproblems can be challenging using Bayesian methods when the data analyst has\nonly limited prior knowledge. In this paper we consider a class of problems,\ncalled Partial Bayes problems, in which the prior information is only partially\navailable. Taking the recently proposed Inferential Model approach, we develop\na general inference framework for Partial Bayes problems, and derive both exact\nand efficient solutions. In addition to the theoretical investigation,\nnumerical results and real applications are used to demonstrate the superior\nperformance of the proposed method."}, {"title": "Detecting weak signals by combining small P-values in genetic association studies", "abstract": "We approach the problem of combining top-ranking association statistics or\nP-value from a new perspective which leads to a remarkably simple and powerful\nmethod. Statistical methods, such as the Rank Truncated Product (RTP), have\nbeen developed for combining top-ranking associations and this general strategy\nproved to be useful in applications for detecting combined effects of multiple\ndisease components. To increase power, these methods aggregate signals across\ntop ranking SNPs, while adjusting for their total number assessed in a study.\nAnalytic expressions for combined top statistics or P-values tend to be\nunwieldy, which complicates interpretation, practical implementation, and\nhinders further developments. Here, we propose the Augmented Rank Truncation\n(ART) method that retains main characteristics of the RTP but is substantially\nsimpler to implement. ART leads to an efficient form of the adaptive algorithm,\nan approach where the number of top ranking SNPs is varied to optimize power.\nWe illustrate our methods by strengthening previously reported associations of\n$\\mu$-opioid receptor variants with sensitivity to pain."}, {"title": "Uplifted cool gas and heating by mixing in cooling flows", "abstract": "We analyze our earlier three-dimensional hydrodynamical numerical simulation\nof jet-inflated bubbles in cooling flow clusters, and find that dense gas that\nwas not heated by the jets' activity and that resides around the hot\njet-inflated bubbles can be identified as uplifted gas as observed in some\nclusters. During the build up of the dense gas around the hot bubble, mixing of\nhot bubble gas with other regions of the intracluster medium (ICM) heats the\nICM. The vortices that mix the ICM with the hot bubble gas also excite shock\nwaves, sound waves, and turbulence. Sound waves, shocks, turbulence, and\nuplifted gas, might be easier to detect than the mixing process and hence\nattract more attention, but we argue that the contributions of these processes\nto the heating of the ICM do not add up to the level of contribution of the\nmixing-heating process."}, {"title": "SILVERRUSH. VII. Subaru/HSC Identifications of 42 Protocluster Candidates at z~6-7 with the Spectroscopic Redshifts up to z=6.574: Implications for Cosmic Reionization", "abstract": "We report fourteen and twenty-eight protocluster candidates at z=5.7 and 6.6\nover 14 and 19 deg^2 areas, respectively, selected from 2,230 (259) Lya\nemitters (LAEs) photometrically (spectroscopically) identified with\nSubaru/Hyper Suprime-Cam (HSC) deep images (Keck, Subaru, and Magellan spectra\nand the literature data). Six out of the 42 protocluster candidates include\n1-12 spectroscopically confirmed LAEs at redshifts up to z=6.574. By the\ncomparisons with the cosmological Lya radiative transfer (RT) model reproducing\nLAEs with the reionization effects, we find that more than a half of these\nprotocluster candidates are progenitors of the present-day clusters with a mass\nof > 10^14 M_sun. We then investigate the correlation between LAE overdensity\ndelta and Lya rest-frame equivalent width EW_Lya^rest, because the cosmological\nLya RT model suggests that a slope of EW_Lya^rest-delta relation is steepened\ntowards the epoch of cosmic reionization (EoR), due to the existence of the\nionized bubbles around galaxy overdensities easing the escape of Lya emission\nfrom the partly neutral intergalactic medium (IGM). The available HSC data\nsuggest that the slope of the EW_Lya^rest-delta correlation does not evolve\nfrom the post-reionization epoch z=5.7 to the EoR z=6.6 beyond the moderately\nlarge statistical errors. There is a possibility that we would detect the\nevolution of the EW_Lya^rest - delta relation from z=5.7 to 7.3 by the upcoming\nHSC observations providing large samples of LAEs at z=6.6-7.3."}, {"title": "Astronomical Distance Determination in the Space Age. Secondary distance indicators", "abstract": "The formal division of the distance indicators into primary and secondary\nleads to difficulties in description of methods which can actually be used in\ntwo ways: with, and without the support of the other methods for scaling. Thus\ninstead of concentrating on the scaling requirement we concentrate on all\nmethods of distance determination to extragalactic sources which are\ndesignated, at least formally, to use for individual sources. Among those, the\nSupernovae Ia is clearly the leader due to its enormous success in\ndetermination of the expansion rate of the Universe. However, new methods are\nrapidly developing, and there is also a progress in more traditional methods.\nWe give a general overview of the methods but we mostly concentrate on the most\nrecent developments in each field, and future expectations."}, {"title": "Orbital and escape dynamics in barred galaxies - III. The 3D system: Correlations between the basins of escape and the NHIMs", "abstract": "The escape dynamics of the stars in a barred galaxy composed of a spherically\nsymmetric central nucleus, a bar, a flat thin disk and a dark matter halo\ncomponent is investigated by using a realistic three degrees of freedom (3-dof)\ndynamical model. Modern colour-coded diagrams are used for distinguishing\nbetween bounded and escaping motion. In addition, the smaller alignment index\n(SALI) method is deployed for determining the regular, sticky or chaotic nature\nof bounded orbits. We reveal the basins of escape corresponding to the escape\nthrough the two symmetrical escape channels around the Lagrange points $L_2$\nand $L_3$ and also we relate them with the corresponding distribution of the\nescape times of the orbits. Furthermore, we demonstrate how the stable\nmanifolds, around the index-1 saddle points, accurately define the fractal\nbasin boundaries observed in the colour-coded diagrams. The development\nscenario of the fundamental vertical Lyapunov periodic orbit is thoroughly\nexplored for obtaining a more complete view of the unfolding of the singular\nbehaviour of the dynamics at the cusp values of the parameters. Finally, we\nexamine how the combination of the most important parameters of the bar (such\nas the semi-major axis and the angular velocity) influences the observed\nstellar structures (rings and spirals) which are formed by escaping stars\nguided by the invariant manifolds near the saddle points."}, {"title": "A direct calibration of the IRX-β relation in Lyman-break Galaxies at z=3-5", "abstract": "We use a sample of 4178 Lyman break galaxies (LBGs) at z = 3, 4 and 5 in the\nUKIRT Infrared Deep Sky Survey (UKIDSS) Ultra Deep Survey (UDS) field to\ninvestigate the relationship between the observed slope of the stellar\ncontinuum emission in the ultraviolet, {\\beta}, and the thermal dust emission,\nas quantified via the so-called 'infrared excess' (IRX = LIR/LUV). Through a\nstacking analysis we directly measure the 850-{\\mu}m flux density of LBGs in\nour deep (0.9mJy) James Clerk Maxwell Telescope (JCMT) SCUBA-2 850-{\\mu}m map,\nas well as deep public Herschel/SPIRE 250-, 350- and 500-{\\mu}m imaging. We\nestablish functional forms for the IRX-{\\beta} relation to z ~ 5, confirming\nthat there is no significant redshift evolution of the relation and that the\nresulting average IRX-{\\beta} curve is consistent with a Calzetti-like\nattenuation law. We compare our results with recent work in the literature,\nfinding that discrepancies in the slope of the IRX-{\\beta} relation are driven\nby biases in the methodology used to determine the ultraviolet slopes.\nConsistent results are found when IRX-{\\beta} is evaluated by stacking in bins\nof stellar mass, M, and we argue that the near-linear IRX-M relationship is a\nbetter proxy for correcting observed UV luminosities to total star formation\nrates, provided an accurate handle on M can be had, and also gives clues as to\nthe physical driver of the role of dust-obscured star formation in\nhigh-redshift galaxies."}, {"title": "SDSS-IV MaNGA: Galaxy Pair Fraction and Correlated Active Galactic Nuclei", "abstract": "We have identified 105 galaxy pairs at z ~ 0.04 with the MaNGA integral-field\nspectroscopic data. The pairs have projected separations between 1 kpc and 30\nkpc, and are selected to have radial velocity offsets less than 600 km/s and\nstellar mass ratio between 0.1 and 1. The pair fraction increases with both the\nphysical size of the integral-field unit and the stellar mass, consistent with\ntheoretical expectations. We provide the best-fit analytical function of the\npair fraction and find that ~3% of M* galaxies are in close pairs. For both\nisolated galaxies and paired galaxies, active galactic nuclei (AGN) are\nselected using emission-line ratios and H_alpha equivalent widths measured\ninside apertures at a fixed physical size. We find AGNs in ~24% of the paired\ngalaxies and binary AGNs in ~13% of the pairs. To account for the selection\nbiases in both the pair sample and the MaNGA sample, we compare the AGN\ncomoving volume densities with those expected from the mass- and\nredshift-dependent AGN fractions. We find a strong (~5x) excess of binary AGNs\nover random pairing and a mild (~20%) deficit of single AGNs. The binary AGN\nexcess increases from ~2x to ~6x as the projected separation decreases from\n10-30 kpc to 1-10 kpc. Our results indicate that pairing of galaxies preserves\nthe AGN duty cycle in individual galaxies but increases the population of\nbinary AGNs through correlated activities. We suggest tidally-induced\ngalactic-scale shocks and AGN cross-ionization as two plausible channels to\nproduce low-luminosity narrow-line-selected binary AGNs."}, {"title": "MOCCA-SURVEY Database I. Unravelling black hole subsystems in globular clusters", "abstract": "In this paper, we discuss how globular clusters (GCs) structural and\nobservational properties can be used to infer the presence of a black hole\nsystem (BHS) inhabiting their inner regions. We propose a novel way to identify\nthe BHS size, defined as the GC radius containing a mass contributed equally\nfrom stars and stellar BHs. Using this definition, similar to the well-known\nconcept of \"influence radius\", we found a \"fundamental plane\" connecting the\nBHS typical density with the GC central surface density profile, total\nluminosity and observational half-mass radius. Our approach allows us to define\na unique way to connect the observational GCs parameters with their dark\ncontent. Comparing our results with observed Milky Way GCs, we found that many\nof them likely host, at the present time, as many as several hundreds of BHs.\nThese BHS are characterized by a relatively low typical density, $\\rho_\\bhs\n\\sim 10-10^5\\Ms$ pc$^{-3}$ and composed of relatively massive BHs, with average\nmasses in the range $m_\\bhs = 14-22\\Ms$. We also show that a similar approach\ncan be used to find Milky Way GCs potentially hosting an intermediate-mass\nblack hole."}, {"title": "Black-hole-regulated star formation in massive galaxies", "abstract": "Super-massive black holes, with masses larger than a million times that of\nthe Sun, appear to inhabit the centers of all massive galaxies.\nCosmologically-motivated theories of galaxy formation need feedback from these\nsuper-massive black holes to regulate star formation. In the absence of such\nfeedback, state-of-the-art numerical simulations dramatically fail to reproduce\nthe number density and properties of massive galaxies in the local Universe.\nHowever, there is no observational evidence of this strongly coupled\nco-evolution between super-massive black holes and star formation, impeding our\nunderstanding of baryonic processes within galaxies. Here we show that the star\nformation histories (SFHs) of nearby massive galaxies, as measured from their\nintegrated optical spectra, depend on the mass of the central super-massive\nblack hole. Our results suggest that black hole mass growth scales with gas\ncooling rate in the early Universe. The subsequent quenching of star formation\ntakes place earlier and more efficiently in galaxies hosting more massive\ncentral black holes. The observed relation between black hole mass and star\nformation efficiency applies to all generations of stars formed throughout a\ngalaxy's life, revealing a continuous interplay between black hole activity and\nbaryon cooling."}, {"title": "Re-visiting the extended Schmidt law: the important role of existing stars in regulating star formation", "abstract": "We revisit the proposed extended Schmidt law (Shi et al. 2011) which points\nthat the star formation efficiency in galaxies depends on the stellar mass\nsurface density, by investigating spatially-resolved star formation rates\n(SFRs), gas masses and stellar masses of star formation regions in a vast range\nof galactic environments, from the outer disks of dwarf galaxies to spiral\ndisks and to merging galaxies as well as individual molecular clouds in M33. We\nfind that these regions are distributed in a tight power-law as Sigma_SFR\n~(Sigma_star^0.5 Sigma_gas )^1.09, which is also valid for the integrated\nmeasurements of disk and merging galaxies at high-z. Interestingly, we show\nthat star formation regions in the outer disks of dwarf galaxies with Sigma_SFR\ndown to 10^(-5) Msun/yr/kpc^2, which are outliers of both Kennicutt-Schmidt and\nSilk-Elmegreen law, also follow the extended Schmidt law. Other outliers in the\nKennicutt-Schmidt law, such as extremely-metal poor star-formation regions,\nalso show significantly reduced deviations from the extended Schmidt law. These\nresults suggest an important role for existing stars in helping to regulate\nstar formation through the effect of their gravity on the mid-plane pressure in\na wide range of galactic environments."}, {"title": "The GALEX Ultraviolet Virgo Cluster Survey (GUViCS). VII.: BCG UV upturn and the FUV-NUV color up to redshift 0.35", "abstract": "CONTEXT:At low redshift, early-type galaxies often exhibit a rising flux with\ndecreasing wavelength in the 1000-2500 \\AA{} range, called \"UV upturn\". The\norigin of this phenomenon is debated, and its evolution with redshift is poorly\nconstrained. The observed GALEX FUV-NUV color can be used to probe the UV\nupturn approximately to redshift 0.5. AIMS: We provide constraints on the\nexistence of the UV upturn up to redshift $\\sim$ 0.4 in the brightest cluster\ngalaxies (BCG) located behind the Virgo cluster, using data from the GUViCS\nsurvey. METHODS:We estimate the GALEX far-UV (FUV) and near-UV (NUV) observed\nmagnitudes for BCGs from the maxBCG catalog in the GUViCS fields. We increase\nthe number of nonlocal galaxies identified as BCGs with GALEX photometry from a\nfew tens of galaxies to 166 (64 when restricting this sample to relatively\nsmall error bars). We also estimate a central color within a 20 arcsec\naperture. By using the $r$-band luminosity from the maxBCG catalog, we can\nseparate blue FUV-NUV due to recent star formation and candidate upturn cases.\nWe use Lick indices to verify their similarity to redshift 0 upturn cases.\nRESULTS: We clearly detect a population of blue FUV-NUV BCGs in the redshift\nrange 0.10-0.35, vastly improving the existing constraints at these epochs by\nincreasing the number of galaxies studied, and by exploring a redshift range\nwith no previous data (beyond 0.2), spanning one more Gyr in the past. These\ngalaxies bring new constraints that can help distinguish between assumptions\nconcerning the stellar populations causing the UV upturn phenomenon. The\nexistence of a large number of UV upturns around redshift 0.25 favors the\nexistence of a binary channel among the sources proposed in the literature."}, {"title": "An interacting galaxy pair at the origin of a light echo", "abstract": "In a low-density region of the Shapley supercluster we identified an\ninteracting galaxy pair at redshift z=0.04865 in which the Seyfert-2 nucleus of\nthe main galaxy (ShaSS 073) is exciting an extended emission line region (EELR,\n~170 kpc^2) in the disk of the less massive companion (ShaSS 622). New\nintegral-field spectroscopy and the multi-band data-set, spanning from\nfar-ultraviolet to far-infrared and radio wavelengths, allowed us to obtain a\ndetailed description of the ShaSS 622-073 system. The gas kinematics shows\nhints of interaction, although the overall velocity field shows a quite regular\nrotation in both galaxies, thus suggesting that we are observing their first\nencounter as confirmed by the estimated distance of 21 kpc between the two\ngalaxy centers. The detected ~2-3 kpc AGN outflow and the geometry of the EELR\nin ShaSS 622 support the presence of a hollow bicone structure. The status and\nsources of the ionization across the whole system have been analysed through\nphotoionization models and a Bayesian approach which prove a clear connection\nbetween the AGN and the EELR. The luminosity of the AGN (2.4x10^44 erg/s) is a\nfactor 20 lower than the power needed to excite the gas in the EELR (4.6x10^45\nerg/s) indicating a dramatic fading of the AGN in the last 3x10^4 yr. ShaSS\n073-622 provides all the ingredients listed in the recipe of a light echo where\na highly ionised region maintains memory of a preceding more energetic phase of\na now faded AGN. This is the first case of a light echo observed between two\ngalaxies."}, {"title": "Intermediate Mass Black Holes: A brief review", "abstract": "Intermediate mass black holes (IMBHs) are an (as yet) elusive class of black\nholes that are expected to lie in the $10^2-10^5\\,M_{\\odot}$ range, between the\nfirmly established stellar-mass black holes and ${\\gtrsim}10^6\\,M_{\\odot}$\nsupermassive black holes. Predicted by a variety of theoretical models, IMBHs\nare the potential seeds of supermassive black holes and are expected to power\nsome of the brightest extra-nuclear X-ray sources. This brief review is the\nresult of a presentation and subsequent discussion of IMBHs that took place\nduring the 12th International Frascati workshop on \"Multifrequency Behaviour of\nHigh Energy Cosmic Sources\". The manuscript aims to provide a concise and\nup-to-date review of the different evolutionary scenarios for the creation of\nIMBHs. Throughout the text I emphasize the importance of the identification and\nclassification of IMBHSs in our effort to understand the formation of\nsupermassive black holes and their co-evolution with their host galaxies."}, {"title": "Diffusion and mixing in globular clusters", "abstract": "Collisional relaxation describes the stochastic process with which a\nself-gravitating system near equilibrium evolves in phase space due to the\nfluctuating gravitational field of the system. The characteristic timescale of\nthis process is called the relaxation time. In this paper, we highlight the\ndifference between two measures of the relaxation time in globular clusters:\n(i) the diffusion time with which the isolating integrals of motion (i.e.\nenergy E and angular momentum magnitude L) of individual stars change\nstochastically and (ii) the asymptotic timescale required for a family of\norbits to mix in the cluster. More specifically, the former corresponds to the\ninstantaneous rate of change of a star's E or L, while the latter corresponds\nto the timescale for the stars to statistically forget their initial\nconditions. We show that the diffusion timescales of E and L vary\nsystematically around the commonly used half-mass relaxation time in different\nregions of the cluster by a factor of ~10 and ~100, respectively, for more than\n20% of the stars. We define the mixedness of an orbital family at any given\ntime as the correlation coefficient between its E or L probability distribution\nfunctions and those of the whole cluster. Using Monte Carlo simulations, we\nfind that mixedness converges asymptotically exponentially with a decay\ntimescale that is ~10 times the half-mass relaxation time."}, {"title": "SHARDS: Constraints on the dust attenuation law of star-forming galaxies at z~2", "abstract": "We make use of SHARDS, an ultra-deep (<26.5AB) galaxy survey that provides\noptical photo-spectra at resolution R~50, via medium band filters (FWHM~150A).\nThis dataset is combined with ancillary optical and NIR fluxes to constrain the\ndust attenuation law in the rest-frame NUV region of star-forming galaxies\nwithin the redshift window 1.5<z<3. We focus on the NUV bump strength (B) and\nthe total-to-selective extinction ratio (Rv), targeting a sample of 1,753\ngalaxies. By comparing the data with a set of population synthesis models\ncoupled to a parametric dust attenuation law, we constrain Rv and B, as well as\nthe colour excess, E(B-V). We find a correlation between Rv and B, that can be\ninterpreted either as a result of the grain size distribution, or a variation\nof the dust geometry among galaxies. According to the former, small dust grains\nare associated with a stronger NUV bump. The latter would lead to a range of\nclumpiness in the distribution of dust within the interstellar medium of\nstar-forming galaxies. The observed wide range of NUV bump strengths can lead\nto a systematic in the interpretation of the UV slope ($\\beta$) typically used\nto characterize the dust content. In this study we quantify these variations,\nconcluding that the effects are $\\Delta\\beta$~0.4."}, {"title": "Metal Abundances of KISS Galaxies. VI. New Metallicity Relations for the KISS Sample of Star-Forming Galaxies", "abstract": "We present updated metallicity relations for the spectral database of\nstar-forming galaxies (SFGs) found in the KPNO International Spectroscopic\nSurvey (KISS). New spectral observations of emission-line galaxies (ELGs)\nobtained from a variety of telescope facilities provide oxygen abundance\ninformation. A nearly four-fold increase in the number of KISS objects with\nrobust metallicities relative to our previous analysis provides for an\nempirical abundance calibration to compute self-consistent metallicity\nestimates for all SFGs in the sample with adequate spectral data. In addition,\na sophisticated spectral energy distribution (SED) fitting routine has provided\nrobust calculations of stellar mass. With these new and/or improved galaxy\ncharacteristics, we have developed luminosity-metallicity ($L$-$Z$) relations,\nmass-metallicity ($M_{*}$-$Z$) relations, and the so-called Fundamental\nMetallicity Relation (FMR) for over 1,450 galaxies from the KISS sample. This\nKISS $M_{*}$-$Z$ relation is presented for the first time and demonstrates\nmarkedly lower scatter than the KISS $L$-$Z$ relation. We find that our\nrelations agree reasonably well with previous publications, modulo modest\noffsets due to differences in the SEL metallicity calibrations used. We\nillustrate an important bias present in previous $L$-$Z$ and $M_{*}$-$Z$\nstudies involving direct-method ($T_{e}$) abundances that may result in\nsystematically lower slopes in these relations. Our KISS FMR shows consistency\nwith those found in the literature, albeit with a larger scatter. This is\nlikely a consequence of the KISS sample being biased toward galaxies with high\nlevels of activity."}, {"title": "MUSE spectroscopy and deep observations of a unique compact JWST target, lensing cluster CLIO", "abstract": "We present the results of a VLT MUSE/FORS2 and Spitzer survey of a unique\ncompact lensing cluster CLIO at z = 0.42, discovered through the GAMA survey\nusing spectroscopic redshifts. Compact and massive clusters such as this are\nunderstudied, but provide a unique prospective on dark matter distributions and\nfor finding background lensed high-z galaxies. The CLIO cluster was identified\nfor follow up observations due to its almost unique combination of high mass\nand dark matter halo concentration, as well as having observed lensing arcs\nfrom ground based images. Using dual band optical and infra-red imaging from\nFORS2 and Spitzer, in combination with MUSE optical spectroscopy we identify 89\ncluster members and find background sources out to z = 6.49. We describe the\nphysical state of this cluster, finding a strong correlation between\nenvironment and galaxy spectral type. Under the assumption of a NFW profile, we\nmeasure the total mass of CLIO to be M$_{200} = (4.49 \\pm 0.25) \\times 10^{14}$\nM$_\\odot$. We build and present an initial strong-lensing model for this\ncluster, and measure a relatively low intracluster light (ICL) fraction of 7.21\n$\\pm$ 1.53% through galaxy profile fitting. Due to its strong potential for\nlensing background galaxies and its low ICL, the CLIO cluster will be a target\nfor our 110 hour JWST 'Webb Medium-Deep Field' (WMDF) GTO program."}, {"title": "A Disk Origin for the Monoceros Ring and A13 Stellar Overdensities", "abstract": "The Monoceros Ring (also known as the Galactic Anticenter Stellar Structure)\nand A13 are stellar overdensities at estimated heliocentric distances of $d\n\\sim 11$ kpc and 15 kpc observed at low Galactic latitudes towards the\nanticenter of our Galaxy. While these overdensities were initially thought to\nbe remnants of a tidally-disrupted satellite galaxy, an alternate scenario is\nthat they are composed of stars from the Milky Way (MW) disk kicked out to\ntheir current location due to interactions between a satellite galaxy and the\ndisk. To test this scenario, we study the stellar populations of the Monoceros\nRing and A13 by measuring the number of RR Lyrae and M giant stars associated\nwith these overdensities. We obtain low-resolution spectroscopy for RR Lyrae\nstars in the two structures and measure radial velocities to compare with\npreviously measured velocities for M giant stars in the regions of the\nMonoceros Ring and A13, to assess the fraction of RR Lyrae to M giant stars\n($f_{RR:MG}$) in A13 and Mon/GASS. We perform velocity modeling on 153 RR Lyrae\nstars (116 in the Monoceros Ring and 37 in A13) and find that both structures\nhave very low $f_{RR:MG}$. The results support a scenario in which stars in A13\nand Mon/GASS formed in the MW disk. We discuss a possible association between\nMon/GASS, A13, and the Triangulum-Andromeda overdensity based on their similar\nvelocity distributions and $f_{RR:MG}$."}, {"title": "Deep CFHT Y-band Imaging of VVDS-F22 Field: II. Quasar Selection and Quasar Luminosity Function", "abstract": "We report the result of a faint quasar survey in a one square degree field.\nThe aim is to test the Y-K/g-z and J-K/i-Y color selection criteria for quasars\nat faint magnitude, to obtain a complete sample of quasars based on deep\noptical and near-infrared color-color selection, and to measure the faint end\nof quasar luminosity function (QLF) over a wide redshift range. We carried out\na quasar survey based on the Y-K/g-z and J-K/i-Y quasar selection criteria,\nusing the deep Y-band data obtained from our CFHT/WIRCam Y-band images in a\ntwo-degree field within the F22 field of the VIMOS VLT deep survey, optical\nco-added data from Sloan Digital Sky Survey Stripe 82 and deep near-infrared\ndata from the UKIDSS Deep Extragalactic Survey in the same field. We discovered\n25 new quasars at 0.5 < z < 4.5 and i < 22.5 mag within one square degree\nfield. The survey significantly increases the number of faint quasars in this\nfield, especially at z ~ 2-3. It confirms that our color selections are highly\ncomplete in a wide redshift range (z < 4.5), especially over the quasar number\ndensity peak at z ~ 2-3, even for faint quasars. Combining all previous known\nquasars and new discoveries, we construct a sample with 109 quasars, and\nmeasure the binned QLF and parametric QLF. Although the sample is small, our\nresults agree with a pure luminosity evolution at lower redshift and luminosity\nevolution and density evolution model at redshift z > 2.5."}, {"title": "ALMA observation of the disruption of molecular gas in M87", "abstract": "We present the results from ALMA observations centred $40^{\\prime\\prime}$ (3\nkpc in projection) southeast of the nucleus of M87. We report the detection of\nextended CO (2-1) line emission with a total flux of $(5.5 \\pm 0.6) \\times\n10^{-18}$ erg s$^{-1}$ cm$^{-2}$ and corresponding molecular gas mass\n$M_{H_2}=(4.7 \\pm 0.4) \\times 10^5 M_\\odot$, assuming a Galactic CO to H$_2$\nconversion factor. ALMA data indicate a line-of-sight velocity of $-129\\pm3$ km\ns$^{-1}$, in good agreement with measurements based on the [CII] and\nH$\\alpha$+[NII] lines, and a velocity dispersion of $\\sigma=27\\pm3$ km\ns$^{-1}$. The CO(2-1) emission originates only outside the radio lobe of the\nAGN seen in the 6~cm VLA image, while the filament prolongs further inwards at\nother wavelengths. The molecular gas in M87 appears to be destroyed or excited\nby AGN activity, either by direct interaction with the radio plasma, or by the\nshock driven by the lobe into the X-ray emitting atmosphere. This is an\nimportant piece of the puzzle in understanding the impact of the central AGN on\nthe amount of the coldest gas from which star formation can proceed."}, {"title": "History of Globulettes in the Milky Way", "abstract": "Globulettes are small (radii $< 10$ kAU) dark dust clouds, seen against the\nbackground of bright nebulae. A majority of the objects have planetary mass.\nThese objects may be a source of brown dwarfs and free floating planetary mass\nobjects in the galaxy. In this paper we investigate how many globulettes could\nhave formed in the Milky Way and how they could contribute to the total\npopulation of free floating planets. In order to do that we examine H-alpha\nimages of 27 H~II regions. In these images, we find 778 globulettes.\n  We find that a conservative value of the number of globulettes formed is\n$5.7\\times 10^{10}$. If 10 \\% of the globulettes form free floating planets\nthen they have contributed with $5.7\\times 10^{9}$ free floating planets in the\nMilky Way. A less conservative number of globulettes would mean that the\nglobulettes could contribute $2.0\\times 10^{10}$ free floating planets. Thus\nthe globulettes could represent a non-negligible source of free floating\nplanets in the Milky Way."}, {"title": "Star formation history of the Galactic bulge from deep HST imaging of low reddening windows", "abstract": "Despite the huge amount of photometric and spectroscopic efforts targetting\nthe Galactic bulge over the past few years, its age distribution remains\ncontroversial owing to both the complexity of determining the age of individual\nstars and the difficult observing conditions. Taking advantage of the recent\nrelease of very deep, proper-motion-cleaned colour--magnitude diagrams (CMDs)\nof four low reddening windows obtained with the Hubble Space Telescope (HST),\nwe used the CMD-fitting technique to calculate the star formation history (SFH)\nof the bulge at -2deg > b > -4deg along the minor axis. We find that over 80\npercent of the stars formed before 8 Gyr ago, but that a significant fraction\nof the super-solar metallicity stars are younger than this age. Considering\nonly the stars that are within reach of the current generation of spectrographs\n(i.e. V < 21), we find that 10 percent of the bulge stars are younger than 5\nGyr, while this fraction rises to 20-25 percent in the metal-rich peak. The\nage-metallicity relation is well parametrized by a linear fit implying an\nenrichment rate of dZ/dt ~ 0.005 Gyr$^{-1}$. Our metallicity distribution\nfunction accurately reproduces that observed by several spectroscopic surveys\nof Baade's window, with the bulk of stars having metal-content in the range\n[Fe/H] ~ -0.7 to ~0.6, along with a sparse tail to much lower metallicities."}, {"title": "Gaia DR1 completeness within 250 pc and star formation history of the Solar neighbourhood", "abstract": "Taking advantage of the Gaia DR1, we combined TGAS parallaxes with the\nTycho-2 and APASS photometry to calculate the star formation history (SFH) of\nthe solar neighbourhood within 250 pc using the colour-magnitude diagram\nfitting technique. Our dynamically-evolved SFH is in excellent agreement with\nthat calculated from the Hipparcos catalogue within 80 pc of the Sun, showing\nan enhanced star formation rate (SFR) in the past ~4 Gyr. We then correct the\nSFR for the disc thickening with age to obtain a SFR that is representative of\nthe whole solar cylinder, and show that even with an extreme correction our\nresults are not consistent with an exponentially decreasing SFR as found by\nrecent studies. Finally, we discuss how this technique can be applied out to ~5\nkpc thanks to the next Gaia data releases, which will allow us to quantify the\nSFH of the thin disc, thick disc and halo in situ."}, {"title": "Brightest group galaxies-II: the relative contribution of BGGs to the total baryon content of groups at z<1.3", "abstract": "We performed a detailed study of the evolution of the star formation rate\n(SFR) and stellar mass of the brightest group galaxies (BGGs) and their\nrelative contribution to the total baryon budget within $R_{200}$\n($f^{BGG}_{b,200}$). The sample comprises 407 BGGs selected from X-ray galaxy\ngroups ($M_{200}=10^{12.8}-10^{14} \\;M_{\\odot}$) out to $z\\sim1.3$ identified\nin the COSMOS, XMM-LSS, and AEGIS fields. We find that BGGs constitute two\ndistinct populations of quiescent and star-forming galaxies and their mean SFR\nis $\\sim2$ dex higher than the median SFR at $ z<1.3 $. Both the mean and the\nmedian SFRs decline with time by $>2$ dex. The mean (median) of stellar mass of\nBGGs has grown by $0.3$ dex since $z=1.3$ to the present day. We show that up\nto $\\sim45\\% $ of the stellar mass growth in a star-forming BGG can be due to\nits star-formation activity. With respect to $f^{BGG}_{b,200}$, we find it to\nincrease with decreasing redshift by $\\sim0.35$ dex while decreasing with halo\nmass in a redshift dependent manner. We show that the slope of the relation\nbetween $f^{BGG}_{b,200}$ and halo mass increases negatively with decreasing\nredshift. This trend is driven by an insufficient star-formation in BGGs,\ncompared to the halo growth rate. We separately show the BGGs with the 20\\%\nhighest $f^{BGG}_{b,200}$ are generally non-star-forming galaxies and grow in\nmass by processes not related to star formation (e.g., dry mergers and tidal\nstriping). We present the $ M_\\star-M_h $ and $ M_\\star/M_h-M_h $ relations and\ncompare them with semi-analytic model predictions and a number of results from\nthe literature. We quantify the intrinsic scatter in stellar mass of BGGs at\nfixed halo mass ($\\sigma_{log M_{\\star}}$) and find that $\\sigma_{log\nM_{\\star}}$ increases from 0.3 dex at $ z\\sim0.2 $ to 0.5 dex at $ z\\sim1.0 $\ndue to the bimodal distribution of stellar mass."}, {"title": "An ALMA study of the Orion Integral Filament: I. Evidence for narrow fibers in a massive cloud", "abstract": "Abridged. Are all filaments bundles of fibers? To address this question, we\nhave investigated the gas organization within the paradigmatic Integral Shape\nFilament (ISF). We combined two new ALMA Cycle 3 mosaics with previous IRAM 30m\nobservations to produce a high-dynamic range N$_2$H$^+$(1-0) emission map of\nthe ISF tracing its high-density material and velocity structure down to scales\nof 0.009 pc. From the analysis of the gas kinematics, we identify a total of 55\ndense fibers in the central region of the ISF. Independently of their location,\nthese fibers are characterized by transonic internal motions, lengths of ~0.15\npc, and masses per-unit-length close to those expected in hydrostatic\nequilibrium. The ISF fibers are spatially organized forming a dense bundle with\nmultiple hub-like associations likely shaped by the local gravitational\npotential. Within this complex network, the ISF fibers show a compact radial\nemission profile with a median FWHM of 0.035 pc systematically narrower than\nthe previously proposed universal 0.1 pc filament width. Our ALMA observations\nreveal complex bundles of fibers in the ISF, suggesting strong similarities\nbetween the internal substructure of this massive filament and previously\nstudied lower-mass objects. The fibers show identical dynamic properties in\nboth low- and high-mass regions, and their widespread detection suggests a\npreferred organizational mechanism of gas in which the physical fiber\ndimensions (width and length) are self-regulated depending on their intrinsic\ngas density. Combined with previous works, we identify a systematic increase of\nthe surface density of fibers as a function of the total mass per-unit-length\nin filamentary clouds. Based on this empirical correlation, we propose a\nunified star-formation scenario where the observed differences between low- and\nhigh-mass clouds emerge naturally from the initial concentration of fibers."}, {"title": "The GALAH survey: properties of the Galactic disk(s) in the solar neighbourhood", "abstract": "Using data from the GALAH pilot survey, we determine properties of the\nGalactic thin and thick disks near the solar neighbourhood. The data cover a\nsmall range of Galactocentric radius ($7.9 \\leq R_\\mathrm{GC} \\leq 9.5$ kpc),\nbut extend up to 4 kpc in height from the Galactic plane, and several kpc in\nthe direction of Galactic anti-rotation (at longitude $260 ^\\circ \\leq \\ell\n\\leq 280^\\circ$). This allows us to reliably measure the vertical density and\nabundance profiles of the chemically and kinematically defined `thick' and\n`thin' disks of the Galaxy. The thin disk (low-$\\alpha$ population) exhibits a\nsteep negative vertical metallicity gradient, at d[M/H]/d$z=-0.18 \\pm 0.01$ dex\nkpc$^{-1}$, which is broadly consistent with previous studies. In contrast, its\nvertical $\\alpha$-abundance profile is almost flat, with a gradient of\nd[$\\alpha$/M]/d$z$ = $0.008 \\pm 0.002$ dex kpc$^{-1}$. The steep vertical\nmetallicity gradient of the low-$\\alpha$ population is in agreement with models\nwhere radial migration has a major role in the evolution of the thin disk. The\nthick disk (high-$\\alpha$ population) has a weaker vertical metallicity\ngradient d[M/H]/d$z = -0.058 \\pm 0.003$ dex kpc$^{-1}$. The $\\alpha$-abundance\nof the thick disk is nearly constant with height, d[$\\alpha$/M]/d$z$ = $0.007\n\\pm 0.002$ dex kpc$^{-1}$. The negative gradient in metallicity and the small\ngradient in [$\\alpha$/M] indicate that the high-$\\alpha$ population experienced\na settling phase, but also formed prior to the onset of major SNIa enrichment.\nWe explore the implications of the distinct $\\alpha$-enrichments and narrow\n[$\\alpha$/M] range of the sub-populations in the context of thick disk\nformation."}, {"title": "Photoevaporating PDR models with the Hydra PDR Code", "abstract": "Recent Herschel and ALMA observations of Photodissociation Regions (PDRs)\nhave revealed the presence of a high thermal pressure (P ~ 10^7-10^8 K cm-3)\nthin compressed layer at the PDR surface where warm molecular tracer emission\n(e.g. CH+, SH+, high-J CO, H2,...) originate. These high pressures (unbalanced\nby the surrounding environment) and a correlation between pressure and incident\nFUV field (G0) seem to indicate a dynamical origin with the radiation field\nplaying an important role in driving the dynamics. We investigate whether\nphotoevaporation of the illuminated edge of a molecular cloud could explain\nthese high pressures and pressure-UV field correlation. We developed a 1D\nhydrodynamical PDR code coupling hydrodynamics, EUV and FUV radiative transfer\nand time-dependent thermo-chemical evolution. We applied it to a 1D\nplane-parallel photoevaporation scenario where a UV-illuminated molecular cloud\ncan freely evaporate in a surrounding low-pressure medium. We find that\nphotoevaporation can produce high thermal pressures and the observed P-G0\ncorrelation, almost independently from the initial gas density. In addition, we\nfind that constant-pressure PDR models are a better approximation to the\nstructure of photoevaporating PDRs than constant-density PDR models, although\nmoderate pressure gradients are present. Strong density gradients from the\nmolecular to the neutral atomic region are found, which naturally explain the\nlarge density contrasts (1-2 orders of magnitude) derived from observations of\ndifferent tracers. The photoevaporating PDR is preceded by a low velocity shock\n(a few km/s) propagating into the molecular cloud. Photoevaporating PDR models\noffer a promising explanation to the recent observational evidence of dynamical\neffects in PDRs."}, {"title": "Metal-poor star formation triggered by the feedback effects from Pop III stars", "abstract": "Metal enrichment by the first-generation (Pop III) stars is the very first\nstep of the matter cycle in the structure formation and it is followed by the\nformation of extremely metal-poor (EMP) stars. To investigate the enrichment\nprocess by the Pop III stars, we carry out a series of numerical simulations\nincluding the feedback effects of photoionization and supernovae (SNe) of Pop\nIII stars with a range of masses of minihaloes (MHs), M_halo , and Pop III\nstars, M_PopIII . We find that the metal-rich ejecta reaches neighbouring\nhaloes and external enrichment (EE) occurs when the halo binding energy is\nsufficiently below the SN explosion energy, E_SN . The neighbouring haloes are\nonly superficially enriched, and the metallicity of the clouds is [Fe/H] < -5.\nOtherwise, the SN ejecta falls back and recollapses to form enriched cloud,\ni.e. internal enrichment (IE) process takes place. In case that a Pop III star\nexplodes as a core-collapse SNe (CCSNe), MHs undergo IE, and the metallicity in\nthe recollapsing region is -5 < [Fe/H] < -3 in most cases. We conclude that IE\nfrom a single CCSN can explain the formation of EMP stars. For pair-instability\nSNe (PISNe), EE takes place for all relevant mass range of MHs, consistent with\nno observational sign of PISNe among EMP stars."}, {"title": "The Large-scale Effect of Environment on Galactic Conformity", "abstract": "We use a volume-limited galaxy sample from the SDSS Data Release 7 to explore\nthe dependence of galactic conformity on the large-scale environment, measured\non $\\sim$ 4 Mpc scales. We find that the star formation activity of neighbour\ngalaxies depends more strongly on the environment than on the activity of their\nprimary galaxies. In under-dense regions most neighbour galaxies tend to be\nactive, while in over-dense regions neighbour galaxies are mostly passive,\nregardless of the activity of their primary galaxies. At a given stellar mass,\npassive primary galaxies reside in higher density regions than active primary\ngalaxies, leading to the apparently strong conformity signal. The dependence of\nthe activity of neighbour galaxies on environment can be explained by the\ncorresponding dependence of the fraction of satellite galaxies. Similar results\nare found for galaxies in a semi-analytical model, suggesting that no new\nphysics is required to explain the observed large-scale conformity."}, {"title": "Constraining physical conditions for the PDR of Trumpler 14 in the Carina Nebula", "abstract": "We investigate the physical conditions of the CO gas near the young star\ncluster, Trumpler 14 of the Carina Nebula. The observations presented in this\nwork are taken with the Fourier Transform Spectrometer (FTS) of the Spectral\nand Photometric Imaging REceiver (SPIRE) onboard the Herschel Space\nObservatory. Our field of view covers the edge of a cavity carved by Trumpler\n14 about $1\\,\\mathrm{Myr}$ ago and marks the transition from HII regions to\nphoto-dissociation regions. With the state-of-the-art Meudon PDR code, we\nsuccessfully derive the physical conditions, which include the thermal pressure\n($P$) and the scaling factor of radiation fields ($G_{\\mathrm{UV}}$), from the\nobserved CO spectral line energy distributions~(SLEDs) in the observed region.\nThe derived $G_{\\mathrm{UV}}$ values generally show an excellent agreement with\nthe UV radiation fields created by nearby OB-stars and thus confirm that the\nmain excitation source of the observed CO emission are the UV-photons provided\nby the massive stars. The derived thermal pressure is between\n$0.5-3\\,\\times\\,10^{8}\\,\\mathrm{K\\,cm^{-3}}$ with the highest values found\nalong the ionization front in Car I-E region facing Trumpler 14, hinting that\nthe cloud structure is similar to the recent observations of the Orion Bar.\nComparing the derived thermal pressure with the radiation fields, we report the\nfirst observationally-derived and spatially-resolved $P \\sim\n2\\times10^4\\,G_{\\mathrm{UV}}$ relationship. As direct comparisons of the\nmodeling results to the observed $^{13}\\mathrm{CO}$, [OI] $63\\,\\mathrm{\\mu m}$,\nand [CII] $158\\,\\mathrm{\\mu m}$ intensities are not straightforward, we urge\nthe readers to be cautious when constraining the physical conditions of PDRs\nwith combinations of $^{12}\\mathrm{CO}$, $^{13}\\mathrm{CO}$, [CI], [OI]\n$63\\,\\mathrm{\\mu m}$, and [CII] $158\\,\\mathrm{\\mu m}$ observations."}, {"title": "ALMA reveals molecular cloud N55 in the Large Magellanic Cloud as a site of massive star formation", "abstract": "We present the molecular cloud properties of N55 in the Large Magellanic\nCloud using $^{12}$CO(1-0) and $^{13}$CO(1-0) observations obtained with\nAtacama Large Millimeter Array. We have done a detailed study of molecular gas\nproperties, to understand how the cloud properties of N55 differ from Galactic\nclouds. Most CO emission appears clumpy in N55, and molecular cores that have\nYSOs show larger linewidths and masses. The massive clumps are associated with\nhigh and intermediate mass YSOs. The clump masses are determined by local\nthermodynamic equilibrium and virial analysis of the $^{12}$CO and $^{13}$CO\nemissions. These mass estimates lead to the conclusion that, (a) the clumps are\nin self-gravitational virial equilibrium, and (b) the $^{12}$CO(1-0)-to-H$_2$\nconversion factor, X$_{\\rm CO}$, is 6.5$\\times$10$^{20}$cm$^{-2}$(K km\ns$^{-1}$)$^{-1}$. This CO-to-H$_2$ conversion factor for N55 clumps is measured\nat a spatial scale of $\\sim$0.67 pc, which is about two times higher than the\nX$_{\\rm CO}$ value of Orion cloud at a similar spatial scale. The core mass\nfunction of N55 clearly show a turnover below 200M$_{\\odot}$, separating the\nlow-mass end from the high-mass end. The low-mass end of the $^{12}$CO mass\nspectrum is fitted with a power law of index 0.5$\\pm$0.1, while for $^{13}$CO\nit is fitted with a power law index 0.6$\\pm$0.2. In the high-mass end, the core\nmass spectrum is fitted with a power index of 2.0$\\pm$0.3 for $^{12}$CO, and\nwith 2.5$\\pm$0.4 for $^{13}$CO. This power-law behavior of the core mass\nfunction in N55 is consistent with many Galactic clouds."}, {"title": "Studying the highly bent spectra of FR II-type radio galaxies with the KDA EXT model", "abstract": "The KDA (Kaiser, Dennett-Thorpe & Alexander, 1997) EXT model, that is, the\nextension of the KDA model of FR (Faranoff & Riley) II-type source evolution,\nis applied and confronted with the observational data for selected FR II-type\nradio sources with significantly aged radio spectra. A sample of FR II-type\nradio galaxies with radio spectra strongly bent at their highest frequencies is\nused for testing the usefulness of the KDA EXT model.The dynamical evolution of\nFR II-type sources predicted with the KDA EXT model is briefly presented and\ndiscussed. The results are then compared to the ones obtained with the\nclassical KDA approach, assuming the source's continuous injection and\nself-similarity. The results and corresponding diagrams obtained for the eight\nsample sources indicate that the KDA EXT model predicts the observed radio\nspectra significantly better than the best spectral fit provided by the\noriginal KDA model."}, {"title": "The Herschel-ATLAS: magnifications and physical sizes of $500\\,μ$m-selected strongly lensed galaxies", "abstract": "We perform lens modelling and source reconstruction of Submillimeter Array\n(SMA) data for a sample of 12 strongly lensed galaxies selected at 500$\\mu$m in\nthe Herschel Astrophysical Terahertz Large Area Survey H-ATLAS. A previous\nanalysis of the same dataset used a single S\\`ersic profile to model the light\ndistribution of each background galaxy. Here we model the source brightness\ndistribution with an adaptive pixel scale scheme, extended to work in the\nFourier visibility space of interferometry. We also present new SMA\nobservations for seven other candidate lensed galaxies from the H-ATLAS sample.\nOur derived lens model parameters are in general consistent with previous\nfindings. However, our estimated magnification factors, ranging from 3 to 10,\nare lower. The discrepancies are observed in particular where the reconstructed\nsource hints at the presence of multiple knots of emission. We define an\neffective radius of the reconstructed sources based on the area in the source\nplane where emission is detected above 5$\\sigma$. We also fit the reconstructed\nsource surface brightness with an elliptical Gaussian model. We derive a median\nvalue $r_{eff}\\,\\sim 1.77\\,$kpc and a median Gaussian full width at half\nmaximum $\\sim1.47\\,$kpc. After correction for magnification, our sources have\nintrinsic star formation rates SFR$\\,\\sim900-3500\\,M_{\\odot}yr^{-1}$, resulting\nin a median star formation rate surface density\n$\\Sigma_{SFR}\\sim132\\,M_{\\odot}$ yr$^{-1}$ kpc$^{-2}$ (or $\\sim 218\\,M_{\\odot}$\nyr$^{-1}$ kpc$^{-2}$ for the Gaussian fit). This is consistent with what\nobserved for other star forming galaxies at similar redshifts, and is\nsignificantly below the Eddington limit for a radiation pressure regulated\nstarburst."}, {"title": "A closer look at the quadruply lensed quasar PSOJ0147: spectroscopic redshifts and microlensing effect", "abstract": "I present timely spectroscopic follow-up of the newly discovered, quadruply\nlensed quasar PSOJ0147 from the Pan-STARRS 1 survey. The newly acquired optical\nspectra with GMOS onboard the Gemini north telescope allow us to pin down the\nredshifts of both the foreground lensing galaxy and the background lensed\nquasar to be z=0.572 and 2.341, providing firm basis for cosmography with\nfuture high cadence photometric monitoring. I also inspect difference spectra\nfrom two of the quasar images, revealing the microlensing effect. Long-term\nspectroscopic follow-ups will shed lights on the structure of the AGN and its\nenvironment."}, {"title": "First Data Release of the All-sky NOAO Source Catalog", "abstract": "Most of the sky has been imaged with NOAO's telescopes from both hemispheres.\nWhile the large majority of these data were obtained for PI-led projects and\nalmost all of the images are publicly available, only a small fraction have\nbeen released to the community via well-calibrated and easily accessible\ncatalogs. We are remedying this by creating a catalog of sources from most of\nthe public data taken on the CTIO-4m+DECam and the KPNO-4m+Mosaic3. This\ncatalog, called the NOAO Source Catalog (NSC), contains over 2.9 billion unique\nobjects, 34 billion individual source measurements, covers ~30,000 square\ndegrees of the sky, has depths of ~23rd magnitude in most broadband filters\nwith ~1-2% photometric precision, and astrometric accuracy of ~7 mas. In\naddition, ~2 billion objects and ~21,000 square degrees of sky have photometry\nin three or more bands. The NSC will be useful for exploring stellar streams,\ndwarf satellite galaxies, QSOs, high-proper motion stars, variable stars and\nother transients. The NSC catalog is publicly available via the NOAO Data Lab\nservice."}, {"title": "Dark matter contraction and stellar-mass-to-light ratio gradients in massive early-type galaxies", "abstract": "We present models for the dark and luminous mass structure of 12 strong\nlensing early-type galaxies (ETGs). We combine pixel-based modelling of\nmultiband HST/ACS imaging with Jeans modelling of kinematics obtained from\nKeck/ESI spectra to disentangle the dark and luminous contributions to the\nmass. Assuming a gNFW profile for the dark matter halo and a spatially constant\nstellar-mass-to-light ratio $\\Upsilon_{\\star}$ for the baryonic mass, we infer\ndistributions for $\\Upsilon_{\\star}$ consistent with IMFs that are heavier than\nthe Milky Way's (with a global mean mismatch parameter relative to a Chabrier\nIMF $\\mu_{\\alpha c} = 1.80 \\pm 0.14$) and halo inner density slopes which span\na large range but are generally cuspier than the dark-matter-only prediction\n($\\mu_{\\gamma'} = 2.01_{-0.22}^{+0.19}$). We investigate possible reasons for\noverestimating the halo slope, including the neglect of spatially varying\nstellar-mas-to-light ratios and/or stellar orbital anisotropy, and find that a\nquarter of the systems prefer radially declining stellar-mass-to-light ratio\ngradients, but that the overall effect on our inference on the halo slope is\nsmall. We suggest a coherent explanation of these results in the context of\ninside-out galaxy growth, and that the relative importance of different\nbaryonic processes in shaping the dark halo may depend on halo environment."}, {"title": "CHANG-ES X: Spatially-resolved Separation of Thermal Contribution from Radio Continuum Emission in Edge-on Galaxies", "abstract": "We analyze the application of star formation rate (SFR) calibrations using\nH$\\alpha$ and 22 micron infrared imaging data in predicting the thermal radio\ncomponent for a test sample of 3 edge-on galaxies (NGC 891, NGC 3044, and NGC\n4631) in the Continuum Halos in Nearby Galaxies -- an EVLA Survey (CHANG-ES).\nWe use a mixture of H$\\alpha$ and 24 micron calibration from Calzetti et al.\n(2007), and a linear 22 micron only calibration from Jarrett et al. (2013) on\nthe test sample. We apply these relations on a pixel-to-pixel basis to create\nthermal prediction maps in the two CHANG-ES bands: L- and C-band (1.5 GHz and\n6.0 GHz, respectively). We analyze the resulting non-thermal spectral index\nmaps, and find a characteristic steepening of the non-thermal spectral index\nwith vertical distance from the disk after application of all methods. We find\npossible evidence of extinction in the 22 micron data as compared to 70 micron\nSpitzer Multband Imaging Photometer (MIPS) imaging in NGC 891. We analyze a\nlarger sample of edge-on and face-on galaxy 25 micron to 100 micron flux\nratios, and find that the ratios for edge-ons are systematically lower by a\nfactor of 1.36, a result we attribute to excess extinction in the mid-IR in\nedge-ons. We introduce a new calibration for correcting the H$\\alpha$\nluminosity for dust when galaxies are edge-on or very dusty."}, {"title": "Limits on Ionized Gas in M81's Globular Clusters", "abstract": "We use NSF's Karl G. Jansky Very Large Array to constrain the mass of ionized\ngas in 206 globular star clusters (GCs) in M81, a nearby spiral galaxy. We\ndetect none of the GCs and impose a typical gas-mass upper limit of 550 solar\nmasses (3-sigma). These findings bear on GC evolution in M81."}, {"title": "Local Stellar Kinematics from RAVE data -- VIII. Effects of the Galactic Disc Perturbations on Stellar Orbits of Red Clump Stars", "abstract": "We aim to probe the dynamic structure of the extended Solar neighbourhood by\ncalculating the radial metallicity gradients from orbit properties, which are\nobtained for axisymmetric and non-axisymmetric potential models, of red clump\n(RC) stars selected from the RAdial Velocity Experiment's Fourth Data Release.\nDistances are obtained by assuming a single absolute magnitude value in\nnear-infrared, i.e. $M_{Ks}=-1.54\\pm0.04$ mag, for each RC star. Stellar orbit\nparameters are calculated by using the potential functions: (i) for the\nMWPotential2014 potential, (ii) for the same potential with perturbation\nfunctions of the Galactic bar and transient spiral arms. The stellar age is\ncalculated with a method based on Bayesian statistics. The radial metallicity\ngradients are evaluated based on the maximum vertical distance ($z_{max}$) from\nthe Galactic plane and the planar eccentricity ($e_p$) of RC stars for both of\nthe potential models. The largest radial metallicity gradient in the $0<z_{max}\n\\leq0.5$ kpc distance interval is $-0.065\\pm0.005$ dex kpc$^{-1}$ for a\nsubsample with $e_p\\leq0.1$, while the lowest value is $-0.014\\pm0.006$ dex\nkpc$^{-1}$ for the subsample with $e_p\\leq0.5$. We find that at $z_{max}>1$\nkpc, the radial metallicity gradients have zero or positive values and they do\nnot depend on $e_p$ subsamples. There is a large radial metallicity gradient\nfor thin disc, but no radial gradient found for thick disc. Moreover, the\nlargest radial metallicity gradients are obtained where the outer Lindblad\nresonance region is effective. We claim that this apparent change in radial\nmetallicity gradients in the thin disc is a result of orbital perturbation\noriginating from the existing resonance regions."}, {"title": "Infrared Contributions of X-Ray Selected Active Galactic Nuclei in Dusty Star-Forming Galaxies", "abstract": "We investigate the infrared contribution from supermassive black hole\nactivity versus host galaxy emission in the mid to far-infrared (IR) spectrum\nfor a large sample of X-ray bright active galactic nuclei (AGN) residing in\ndusty, star-forming host galaxies. We select 703 AGN with L_X = 10^42-46 ergs/s\nat 0.1 < z < 5 from the Chandra XBootes X-ray Survey with rich multi-band\nobservations in the optical to far-IR. This is the largest sample to date of\nX-ray AGN with mid and far-IR detections that uses spectral energy distribution\n(SED) decomposition to determine intrinsic AGN and host galaxy infrared\nluminosities. We determine weak or nonexistent relationships when averaging\nstar-formation activity as a function of AGN activity, but see stronger\npositive trends when averaging L_X in bins of star-forming activity for AGN at\nlow redshifts. We estimate an average dust covering factor of 33% based on\ninfrared SEDs and bolometric AGN luminosity, corresponding to a Type 2 AGN\npopulation of roughly a third. We also see a population of AGN that challenge\nthe inclination based unification model with individual dust covering factors\nthat contradict the nuclear obscuration expected from observed X-ray hardness\nratios. We see no strong connection between AGN fractions in the IR and\ncorresponding total infrared, 24 um, or X-ray luminosities. The average\nrest-frame AGN contribution as a function of IR wavelength shows significant\n(~80%) contributions in the mid-IR that trail off at lambda > 30 um.\nAdditionally, we provide a relation between observed L_X and pure AGN IR output\nfor high-z AGN allowing future studies to estimate AGN infrared contribution\nusing only observed X-ray flux density estimates."}, {"title": "The Radial Acceleration Relation and a Magnetostatic Analogy in Quasilinear MOND", "abstract": "Recently a remarkable relation has been demonstrated between the observed\nradial acceleration in disk galaxies and the acceleration predicted on the\nbasis of baryonic matter alone. Here we study this relation within the\nframework of the modified gravity model MOND. The field equations of MOND\nautomatically imply the radial acceleration relation for spherically symmetric\ngalaxies, but for disk galaxies deviations from the relation are expected. Here\nwe investigate whether these deviations are of sufficient magnitude to bring\nMOND into conflict with the observed relation. In the quasilinear formulation\nof MOND, to calculate the gravitational field of a given distribution of\nmatter, an intermediate step is to calculate the \"pristine field\", which is a\nsimple nonlinear function of the Newtonian field corresponding to the same\ndistribution of matter. Hence, to the extent that the quasilinear gravitational\nfield is approximately equal to the pristine field, the radial acceleration\nrelation will be satisfied. We show that the difference between the quasilinear\nand pristine fields obeys the equations of magnetostatics, the curl of the\npristine field serves as the source for the difference in the two fields, much\nas currents serve as sources for the magnetic field. Using the magnetostatic\nanalogy we numerically study the difference between the pristine and\nquasilinear fields for simple model galaxies with a Gaussian profile. Our\nprincipal finding is that the difference between the fields is small compared\nto the observational uncertainties and that quasilinear MOND is therefore\ncompatible with the observed radial acceleration relation."}, {"title": "Radial migration in a stellar galactic disc with thick components", "abstract": "We study how migration affects stars of a galaxy with a thin stellar disc and\nthicker stellar components. The simulated galaxy has a strong bar and lasting\nspiral arms. We find that the amplitude of the churning (change in angular\nmomentum) is similar for thin and thick components, and of limited amplitude,\nand that stars of all components can be trapped at the corotation of the bar.\nAt the exception of those stars trapped at the corotation, we find that stars\nthat are far from their initial guiding radius are more likely so due to\nblurring rather than churning effects. We compare the simulation to orbits\nintegration with a fixed gravitational potential rotating at a constant speed.\nIn the latter case, stars trapped at corotation are churned periodically\noutside and inside the corotation radius, with a zero net average. However, as\nthe bar speed of the simulated galaxy decreases and its corotation radius\nincreases, stars trapped at corotation for several Gyrs can be churned outwards\non average. We study the location of extreme migrators (stars experimenting the\nlargest churning) and find that extreme migrators come from regions on the\nleading side of the effective potential local maxima."}, {"title": "Galactic cold cores IX. Column density structures and radiative transfer modelling", "abstract": "The Galactic Cold Cores (GCC) project has made Herschel observations of\ninterstellar clouds where Planck detected compact sources of cold dust\nemission. Our aim is to characterise the structure of the clumps and their\nparent clouds. We also examine the accuracy to which the structure of dense\nclumps can be determined from submillimetre data. We use standard statistical\nmethods to characterise the GCC fields. Clumps are extracted using column\ndensity thresholding and we construct for each field a three-dimensional\nradiative transfer (RT) model. These are used to estimate the relative\nradiation field intensities, clump stability, and the uncertainty of column\ndensity estimates. We examine the radial column density profiles of the clumps.\nIn the GCC fields, the structure noise follows the relations previously\nestablished at larger scales. The fractal dimension has no significant\ndependence on column density and the values D = 1.25 +- 0.07 are only slightly\nlower than in typical molecular clouds. The column density PDFs exhibit large\nvariations, e.g. in the case of externally compressed clouds. At scales r>0.1\npc, the radial column density distributions of the clouds follow an average\nrelation of N~r^{-1}. In spite of a great variety of clump morphology, clumps\ntend to follow a similar N~r^{-1} relation below r~0.1 pc. RT calculations\nindicate only factor of 2.5 variation in the local radiation field intensity.\nThe fraction of gravitationally bound clumps increases significantly in regions\nwith A_V > 5 mag but most bound objects appear to be pressure-confined. The GCC\nhost clouds have statistical properties similar to general molecular clouds.\nThe gravitational stability, peak column density, and clump orientation are\nconnected to the cloud background while most other statistics (e.g. D and\nradial profiles) are insensitive to the environment."}, {"title": "Faraday rotation at low frequencies: magnetoionic material of the large FRII radio galaxy PKS J0636-2036", "abstract": "We present a low-frequency, broadband polarization study of the FRII radio\ngalaxy PKS J0636-2036 (z = 0.0551), using the Murchison Widefield Array (MWA)\nfrom 70 to 230 MHz. The northern and southern hotspots (separated by ~14.5' on\nthe sky) are resolved by the MWA (3'.3 resolution) and both are detected in\nlinear polarization across the full frequency range. A combination of Faraday\nrotation measure (RM) synthesis and broadband polarization model-fitting are\nused to constrain the Faraday depolarization properties of the source. For the\nintegrated southern hotspot emission, two RM component models are strongly\nfavoured over a single RM component, and the best-fitting model requires\nFaraday dispersions of approximately 0.7 and 1.2 rad/m$^2$ (with a mean RM of\n~50 rad/m$^2$). High resolution imaging at 5\" with the ATCA shows significant\nsub-structure in the southern hotspot and highlights some of the limitations in\nthe polarization modelling of the MWA data. Based on the observed\ndepolarization, combined with extrapolations of gas density scaling-relations\nfor group environments, we estimate magnetic field strengths in the\nintergalactic medium between ~0.04 and 0.5 {\\mu}G. We also comment on future\nprospects of detecting more polarized sources at low frequencies."}, {"title": "Tidal breakup of triple stars in the Galactic Centre", "abstract": "The last decade has seen the detection of fast moving stars in the Galactic\nhalo, the so-called hypervelocity stars (HVSs). While the bulk of this\npopulation is likely the result of a close encounter between a stellar binary\nand the supermassive black hole (MBH) in the Galactic Centre (GC), other\nmechanims may contribute fast stars to the sample. Few observed HVSs show\napparent ages which are shorter than the flight time from the GC, thereby\nmaking the binary disruption scenario unlikely. These stars may be the result\nof the breakup of a stellar triple in the GC which led to the ejection of a\nhypervelocity binary (HVB). If such binary evolves into a blue straggler star\ndue to internal processes after ejection, a rejuvenation is possible that make\nthe star appear younger once detected in the halo. A triple disruption may also\nbe responsible for the presence of HVBs, of which one candidate has now been\nobserved. We present a numerical study of triple disruptions by the MBH in the\nGC and find that the most likely outcomes are the production of single HVSs and\nsingle/binary stars bound to the MBH, while the production of HVBs has a\nprobability $\\lesssim 1\\%$ regardless of the initial parameters. Assuming a\ntriple fraction of $\\approx 10\\%$ results in an ejection rate of $\\lesssim 1\\\n\\mathrm{Gyr}^{-1}$, insufficient to explain the sample of HVSs with lifetimes\nshorter than their flight time. We conclude that alternative mechanisms are\nresponsible for the origin of such objects and HVBs in general."}, {"title": "Quenching or Bursting: the Role of Stellar Mass, Environment, and Specific Star Formation Rate to $z$ $\\sim$ 1", "abstract": "Using a novel approach, we study the quenching and bursting of galaxies as a\nfunction of stellar mass ($M_{*}$), local environment ($\\Sigma$), and specific\nstar-formation rate (sSFR) using a large spectroscopic sample of $\\sim$ 123,000\n$GALEX$/SDSS and $\\sim$ 420 $GALEX$/COSMOS/LEGA-C galaxies to $z$ $\\sim$ 1. We\nshow that out to $z$ $\\sim$ 1 and at fixed sSFR and local density, on average,\nless massive galaxies are quenching, whereas more massive systems are bursting,\nwith a quenching/bursting transition at log($M_{*}$/$M_{\\odot}$) $\\sim$ 10.5-11\nand likely a short quenching/bursting timescale ($\\lesssim$ 300 Myr). We find\nthat much of the bursting of star-formation happens in massive\n(log($M_{*}$/$M_{\\odot}$) $\\gtrsim$ 11), high sSFR galaxies\n(log(sSFR/Gyr$^{-1}$) $\\gtrsim$ -2), particularly those in the field\n(log($\\Sigma$/Mpc$^{-2}$) $\\lesssim$ 0; and among group galaxies, satellites\nmore than centrals). Most of the quenching of star-formation happens in\nlow-mass (log($M_{*}$/$M_{\\odot}$) $\\lesssim$ 9), low sSFR galaxies\n(log(sSFR/Gyr$^{-1}$) $\\lesssim$ -2), in particular those located in dense\nenvironments (log($\\Sigma$/Mpc$^{-2}$) $\\gtrsim$ 1), indicating the combined\neffects of $M_{*}$ and $\\Sigma$ in quenching/bursting of galaxies since $z$\n$\\sim$ 1. However, we find that stellar mass has stronger effects than\nenvironment on recent quenching/bursting of galaxies to $z$ $\\sim$ 1. At any\ngiven $M_{*}$, sSFR, and environment, centrals are quenchier (quenching faster)\nthan satellites in an average sense. We also find evidence for the strength of\nmass and environmental quenching being stronger at higher redshift. Our\npreliminary results have potential implications for the physics of\nquenching/bursting in galaxies across cosmic time."}, {"title": "LOFAR/H-ATLAS: The low-frequency radio luminosity - star-formation rate relation", "abstract": "Radio emission is a key indicator of star-formation activity in galaxies, but\nthe radio luminosity-star formation relation has to date been studied almost\nexclusively at frequencies of 1.4 GHz or above. At lower radio frequencies the\neffects of thermal radio emission are greatly reduced, and so we would expect\nthe radio emission observed to be completely dominated by synchrotron radiation\nfrom supernova-generated cosmic rays. As part of the LOFAR Surveys Key Science\nproject, the Herschel-ATLAS NGP field has been surveyed with LOFAR at an\neffective frequency of 150 MHz. We select a sample from the MPA-JHU catalogue\nof SDSS galaxies in this area: the combination of Herschel, optical and\nmid-infrared data enable us to derive star-formation rates (SFRs) for our\nsources using spectral energy distribution fitting, allowing a detailed study\nof the low-frequency radio luminosity--star-formation relation in the nearby\nUniverse. For those objects selected as star-forming galaxies (SFGs) using\noptical emission line diagnostics, we find a tight relationship between the 150\nMHz radio luminosity ($L_{150}$) and SFR. Interestingly, we find that a single\npower-law relationship between $L_{150}$ and SFR is not a good description of\nall SFGs: a broken power law model provides a better fit. This may indicate an\nadditional mechanism for the generation of radio-emitting cosmic rays. Also, at\ngiven SFR, the radio luminosity depends on the stellar mass of the galaxy.\nObjects which were not classified as SFGs have higher 150-MHz radio luminosity\nthan would be expected given their SFR, implying an important role for\nlow-level active galactic nucleus activity."}, {"title": "Variations of the stellar initial mass function in semi-analytical models II: the impact of Cosmic Ray regulation", "abstract": "Recent studies proposed that cosmic rays (CR) are a key ingredient in setting\nthe conditions for star formation, thanks to their ability to alter the thermal\nand chemical state of dense gas in the UV-shielded cores of molecular clouds.\nIn this paper, we explore their role as regulators of the stellar initial mass\nfunction (IMF) variations, using the semi-analytic model for GAlaxy Evolution\nand Assembly (GAEA). The new model confirms our previous results obtained using\nthe integrated galaxy-wide IMF (IGIMF) theory: both variable IMF models\nreproduce the observed increase of $\\alpha$-enhancement as a function of\nstellar mass and the measured $z=0$ excess of dynamical mass-to-light ratios\nwith respect to photometric estimates assuming a universal IMF. We focus here\non the mismatch between the photometrically-derived ($M^{\\rm app}_{\\star}$) and\nintrinsic ($M_{\\star}$) stellar masses, by analysing in detail the evolution of\nmodel galaxies with different values of $M_{\\star}/M^{\\rm app}_{\\star}$. We\nfind that galaxies with small deviations (i.e. formally consistent with a\nuniversal IMF hypothesis) are characterized by more extended star formation\nhistories and live in less massive haloes with respect to the bulk of the\ngalaxy population. While the IGIMF theory does not change significantly the\nmean evolution of model galaxies with respect to the reference model, a\nCR-regulated IMF implies shorter star formation histories and higher peaks of\nstar formation for objects more massive than $10^{10.5} M_\\odot$. However, we\nalso show that it is difficult to unveil this behaviour from observations, as\nthe key physical quantities are typically derived assuming a universal IMF."}, {"title": "The Next Generation Fornax Survey (NGFS): II. The Central Dwarf Galaxy Population", "abstract": "We present a photometric study of the dwarf galaxy population in the core\nregion ($< r_{\\rm vir}/4$) of the Fornax galaxy cluster based on deep $u'g'i'$\nphotometry from the Next Generation Fornax Cluster Survey. All imaging data\nwere obtained with the Dark Energy Camera mounted on the 4-meter Blanco\ntelescope at the Cerro-Tololo Interamerican Observatory. We identify 258 dwarf\ngalaxy candidates with luminosities $-17 < M_{g'} < -8$ mag, corresponding to\ntypical stellar masses of $9.5\\gtrsim \\log{\\cal M}_{\\star}/M_\\odot \\gtrsim\n5.5$, reaching $\\sim\\!3$ mag deeper in point-source luminosity and $\\sim\\!4$\nmag deeper in surface-brightness sensitivity compared to the classic Fornax\nCluster Catalog. Morphological analysis shows that surface-brightness profiles\nare well represented by single-component S\\'ersic models with average S\\'ersic\nindices of $\\langle n\\rangle_{u',g',i'}=(0.78-0.83) \\pm 0.02$, and average\neffective radii of $\\langle r_e\\rangle_{u',g',i'}\\!=(0.67-0.70) \\pm 0.02$ kpc.\nColor-magnitude relations indicate a flattening of the galaxy red sequence at\nfaint galaxy luminosities, similar to the one recently discovered in the Virgo\ncluster. A comparison with population synthesis models and the galaxy\nmass-metallicity relation reveals that the average faint dwarf galaxy is likely\nolder than ~5 Gyr. We study galaxy scaling relations between stellar mass,\neffective radius, and stellar mass surface density over a stellar mass range\ncovering six orders of magnitude. We find that over the sampled stellar mass\nrange several distinct mechanisms of galaxy mass assembly can be identified: i)\ndwarf galaxies assemble mass inside the half-mass radius up to $\\log{\\cal\nM}_{\\star}$ ~8.0, ii) isometric mass assembly in the range $8.0 < \\log{\\cal\nM}_{\\star}/M_\\odot < 10.5$, and iii) massive galaxies assemble stellar mass\npredominantly in their halos at $\\log{\\cal M}_{\\star}$ ~10.5 and above."}, {"title": "UV-Luminous, Star-Forming Hosts of z~2 Reddened Quasars in the Dark Energy Survey", "abstract": "We present the first rest-frame UV population study of 17 heavily reddened,\nhigh-luminosity (E(B-V)$_{\\rm{QSO}}\\gtrsim$ 0.5; L$_{\\rm{bol}}>$\n10$^{46}$ergs$^{-1}$) broad-line quasars at $1.5 < z < 2.7$. We combine the\nfirst year of deep, optical, ground-based observations from the Dark Energy\nSurvey (DES) with the near infrared VISTA Hemisphere Survey (VHS) and UKIDSS\nLarge Area Survey (ULAS) data, from which the reddened quasars were initially\nidentified. We demonstrate that the significant dust reddening towards the\nquasar in our sample allows host galaxy emission to be detected at the\nrest-frame UV wavelengths probed by the DES photometry. By exploiting this\nreddening effect, we disentangle the quasar emission from that of the host\ngalaxy via spectral energy distribution (SED) fitting. We find evidence for a\nrelatively unobscured, star-forming host galaxy in at least ten quasars, with a\nfurther three quasars exhibiting emission consistent with either star formation\nor scattered light. From the rest-frame UV emission, we derive instantaneous,\ndust-corrected star formation rates (SFRs) in the range 25 < SFR$_{\\rm{UV}}$ <\n365 M$_{\\odot}$yr$^{-1}$, with an average SFR$_{\\rm{UV}}$ = 130 $\\pm$ 95\nM$_{\\odot}$yr$^{-1}$. We find a broad correlation between SFR$_{\\rm{UV}}$ and\nthe bolometric quasar luminosity. Overall, our results show evidence for coeval\nstar formation and black hole accretion occurring in luminous, reddened quasars\nat the peak epoch of galaxy formation."}, {"title": "Millimeter mapping at z~1: dust-obscured bulge building and disk growth", "abstract": "A randomly chosen star in today's Universe is most likely to live in a galaxy\nwith a stellar mass between that of the Milky Way and Andromeda. Yet it remains\nuncertain how the structural evolution of these bulge-disk systems proceeded.\nMost of the unobscured star formation we observe building Andromdeda\nprogenitors at 0.7<z<1.5 occurs in disks, but >90% of their star formation is\nreprocessed by dust and remains unaccounted for. Here we map 500micron dust\ncontinuum emission in an Andromeda progenitor at z=1.25 to probe where it is\ngrowing through dust-obscured star formation. Combining resolved dust\nmeasurements from the NOEMA interferometer with Hubble Space Telescope Halpha\nmaps and multicolor imaging (including new UV data from the HDUV survey), we\nfind a bulge growing by dust-obscured star formation: while the unobscured star\nformation is centrally suppressed, the dust continuum is centrally\nconcentrated, filling in the ring-like structures evident in the Halpha and UV\nemission. Reflecting this, the dust emission is more compact than the\noptical/UV tracers of star formation with r_e(dust)=3.4kpc,\nr_e(Halpha)/r_e(dust)=1.4, and r_e(UV)/r_e(dust)=1.8. Crucially, however, the\nbulge and disk of this galaxy are building simultaneously; although the dust\nemission is more compact than the rest-optical emission\n(r_e(optical)/r_e(dust)=1.4), it is somewhat less compact than the stellar mass\n(r_e(M_*)/r_e(dust)=0.9). Taking the 500micron emission as a tracer of star\nformation, the expected structural evolution of this galaxy can be accounted\nfor by star formation: it will grow in size by Delta(r_e)/Delta(M_*)~0.3 and\ncentral surface density by Delta(Sigma_cen)/Delta(M_*)~0.9. Finally, our\nobservations are consistent with a picture in which merging and disk\ninstabilities drive gas to the center of galaxies, boosting global star\nformation rates above the main sequence and building bulges."}, {"title": "An Empirical Mass Function Distribution", "abstract": "The halo mass function, encoding the comoving number density of dark matter\nhalos of a given mass, plays a key role in understanding the formation and\nevolution of galaxies. As such, it is a key goal of current and future deep\noptical surveys to constrain the mass function down to mass scales which\ntypically host $L_\\star$ galaxies. Motivated by the proven accuracy of\nPress-Schechter-type mass functions, we introduce a related but purely\nempirical form consistent with standard formulae to better than 4\\% in the\nmedium-mass regime, $10^{10}-10^{13} h^{-1}M_\\odot$. In particular, our form\nconsists of 4 parameters, each of which has a simple interpretation, and can be\ndirectly related to parameters of the galaxy distribution, such as $L_\\star$.\n  Using this form within a hierarchical Bayesian likelihood model, we show how\nindividual mass-measurement errors can be successfully included in a typical\nanalysis whilst accounting for Eddington bias. We apply our form to a question\nof survey design in the context of a semi-realistic data model, illustrating\nhow it can be used to obtain optimal balance between survey depth and angular\ncoverage for constraints on mass function parameters.\n  Open-source {\\tt Python} and {\\tt R} codes to apply our new form are provided\nat \\url{http://mrpy.readthedocs.org} and\n\\url{https://cran.r-project.org/web/packages/tggd/index.html} respectively."}, {"title": "Robust Cross-correlation-based Measurement of Clump Sizes in Galaxies", "abstract": "Stars form in molecular complexes that are visible as giant clouds ($\\sim\n10^{5-6} \\mathrm{M}_\\odot$) in nearby galaxies and as giant clumps ($\\sim\n10^{8-9}\\mathrm{M}_\\odot$) in galaxies at redshifts $z\\approx1$$-$$3$.\nTheoretical inferences on the origin and evolution of these complexes often\nrequire robust measurements of their characteristic size, which is hard to\nmeasure at limited resolution and often ill-defined due to overlap and\nquasi-fractal substructure. We show that maximum and luminosity-weighted sizes\nof clumps seen in star formation maps (e.g.\\ H$\\alpha$) can be recovered\nstatistically using the two-point correlation function (2PCF), if an\napproximate stellar surface density map is taken as the normalizing random\nfield. After clarifying the link between Gaussian clumps and the 2PCF\nanalytically, we design a method for measuring the diameters of Gaussian clumps\nwith realistic quasi-fractal substructure. This method is tested using mock\nimages of clumpy disk galaxies at different spatial resolutions and perturbed\nby Gaussian white noise. We find that the 2PCF can recover the input clump\nscale at $\\sim20\\%$ accuracy, as long as this scale is larger than the spatial\nresolution. We apply this method to the local spiral galaxy NGC 5194, as well\nas to three clumpy turbulent galaxies from the DYNAMO-HST sample. In both\ncases, our statistical H$\\alpha$-clump size measurements agree with previous\nmeasurements and with the estimated Jeans lengths. However, the new\nmeasurements are free from subjective choices when fitting individual clumps."}, {"title": "Magnetic field evolution in dwarf and Magellanic-type galaxies", "abstract": "Low-mass galaxies radio observations show in many cases surprisingly high\nlevels of magnetic field. The mass and kinematics of such objects do not favour\nthe development of effective large-scale dynamo action. We attempted to check\nif the cosmic-ray-driven dynamo can be responsible for measured magnetization\nin this class of poorly investigated objects. We investigated how starburst\nevents on the whole, as well as when part of the galactic disk, influence the\nmagnetic field evolution. We created a model of a dwarf/Magellanic-type galaxy\ndescribed by gravitational potential constituted from two components: the stars\nand the dark-matter halo. The model is evolved by solving a three-dimensional\n(3D) magnetohydrodynamic equation with an additional cosmic-ray component,\nwhich is approximated as a fluid. The turbulence is generated in the system via\nsupernova explosions manifested by the injection of cosmic-rays.The\ncosmic-ray-driven dynamo works efficiently enough to amplify the magnetic field\neven in low-mass dwarf/Magellanic-type galaxies. The $e$-folding times of\nmagnetic energy growth are 0.50 and 0.25 Gyr for the slow (50 km/s) and fast\n(100 km/s) rotators, respectively. The amplification is being suppressed as the\nsystem reaches the equipartition level between kinetic, magnetic, and\ncosmic-ray energies. An episode of star formation burst amplifies the magnetic\nfield but only for a short time while increased star formation activity holds.\nWe find that a substantial amount of gas is expelled from the galactic disk,\nand that the starburst events increase the efficiency of this process."}, {"title": "Multiarm spirals on the periphery of disc galaxies", "abstract": "Spiral patterns in some disc galaxies have two arms in the centre, and three\nor more arms on the periphery. The same result is also obtained in numerical\nsimulations of stellar and gaseous discs. We argue that such patterns may occur\ndue to fast cooling of the gas, resulting in formation of giant molecular\nclouds. The timescale of this process is 50 Myr, the factor of 10 shorter than\nof ordinary secular instability. The giant molecular clouds give rise to\nmultiarm spirals through the mechanism of swing amplification."}, {"title": "The Mean Ultraviolet Spectrum of a Representative Sample of Faint z~3 Lyman Alpha Emitters", "abstract": "We discuss the rest-frame ultraviolet emission line spectra of a large (~100)\nsample of low luminosity redshift z~3.1 Lyman alpha emitters (LAEs) drawn from\na Subaru imaging survey in the SSA22 survey field. Our earlier work based on\nsmaller samples indicated that such sources have high [OIII]/[OII] line ratios\npossibly arising from a hard ionising spectrum that may be typical of similar\nsources in the reionisation era. With optical spectra secured from VLT/VIMOS,\nwe re-examine the nature of the ionising radiation in a larger sample using the\nstrength of the high ionisation diagnostic emission lines of CIII]1909,\nCIV1549, HeII1640, and OIII]1661,1666 in various stacked subsets. Our analysis\nconfirms earlier suggestions of a correlation between the strength of Ly-alpha\nand CIII] emission and we find similar trends with broad band UV luminosity and\nrest-frame UV colour. Using various diagnostic line ratios and our stellar\nphotoionisation models, we determine both the gas phase metallicity and\nhardness of the ionisation spectrum characterised by xi_ion - the number of\nLyman continuum photons per UV luminosity. We confirm our earlier suggestion\nthat xi_ion is significantly larger for LAEs than for continuum-selected Lyman\nbreak galaxies, particularly for those LAEs with the faintest UV luminosities.\nWe briefly discuss the implications for cosmic reionisation if the metal-poor\nintensely star-forming systems studied here are representative examples of\nthose at much higher redshift."}, {"title": "Evidence for feedback and stellar-dynamically regulated bursty star cluster formation: the case of the Orion Nebula Cluster", "abstract": "(abridged) A scenario for the formation of multiple co-eval populations\nseparated in age by about 1 Myr in very young clusters (VYCs, ages less than 10\nMyr) and with masses in the range 600-20000 Msun is outlined. It rests upon a\nconverging inflow of molecular gas building up a first population of pre-main\nsequence stars. The associated just-formed O stars ionise the inflow and\nsuppress star formation in the embedded cluster. However, they typically eject\neach other out of the embedded cluster within 10^6 yr, that is before the\nmolecular cloud filament can be ionised entirely. The inflow of molecular gas\ncan then resume forming a second population. This sequence of events can be\nrepeated multiply. This model is applied to the Orion Nebula Cluster (ONC), in\nwhich three well-separated pre-main sequences in the color-magnitude diagram of\nthe cluster have recently been discovered. The mass-inflow history is\nconstrained using this model and the number of OB stars ejected from each\npopulation are estimated for verification using Gaia data. As a consequence of\nthe proposed model, the three runaway O star systems, AE Aur, mu Col and iota\nOri, are considered as significant observational evidence for stellar-dynamical\nejections of massive stars from the oldest population in the ONC."}, {"title": "Accretion of satellites onto central galaxies in clusters: merger mass ratios and orbital parameters", "abstract": "We study the statistical properties of mergers between central and satellite\ngalaxies in galaxy clusters in the redshift range $0<z<1$, using a sample of\ndark-matter only cosmological N-body simulations from Le SBARBINE dataset.\nUsing a spherical overdensity algorithm to identify dark-matter haloes, we\nconstruct halo merger trees for different values of the over-density\n$\\Delta_c$. While the virial overdensity definition allows us to probe the\naccretion of satellites at the cluster virial radius $r_{vir}$, higher\noverdensities probe satellite mergers in the central region of the cluster,\ndown to $\\approx 0.06 r_{vir}$, which can be considered a proxy for the\naccretion of satellite galaxies onto central galaxies. We find that the\ncharacteristic merger mass ratio increases for increasing values of $\\Delta_c$:\nmore than $60\\%$ of the mass accreted by central galaxies since $z\\approx 1$\ncomes from major mergers. The orbits of satellites accreting onto central\ngalaxies tend to be more tangential and more bound than orbits of haloes\naccreting at the virial radius. The obtained distributions of merger mass\nratios and orbital parameters are useful to model the evolution of the\nhigh-mass end of the galaxy scaling relations without resorting to hydrodynamic\ncosmological simulations."}, {"title": "Predicting the binary black hole population of the Milky Way with cosmological simulations", "abstract": "Binary black holes are the primary endpoint of massive stellar evolution.\nTheir properties provide a unique opportunity to constrain binary evolution,\nwhich is still poorly understood. In this paper, we predict the inventory of\nbinary black holes and their merger products in/around the Milky Way, and\ndetail their main properties. We present the first combination of a\nhigh-resolution cosmological simulation of a Milky Way-mass galaxy with a\nbinary population synthesis model. The hydrodynamic simulation, taken from the\nFIRE project, provides a cosmologically realistic star formation history for\nthe galaxy and its stellar halo and satellites. We apply a\nmetallicity-dependent evolutionary model to the star particles to produce\nindividual binary black holes. We find that a million binary black holes have\nmerged in the model Milky Way, and 3 million binaries are still present, with\nan average mass of 28 Msun per binary. Because the black hole progenitors are\nbiased towards low metallicity stars, half reside in the stellar halo and\nsatellites and 40 per cent of the binaries were formed outside the main galaxy.\nThis trend increases with the masses of the black holes. The numbers and mass\ndistribution of the merged systems is compatible with the LIGO/Virgo\ndetections. Observations of these black holes will be challenging, both with\nelectromagnetic methods and LISA. We find that a cosmologically realistic star\nformation history, with self-consistent metal enrichment and Galactic accretion\nhistory, are key ingredients for determining binary black hole rates that can\nbe compared with observations to constrain massive binary evolution."}, {"title": "A Candidate $z\\sim10$ Galaxy Strongly Lensed into a Spatially Resolved Arc", "abstract": "The most distant galaxies known are at z~10-11, observed 400-500 Myr after\nthe Big Bang. The few z~10-11 candidates discovered to date have been\nexceptionally small- barely resolved, if at all, by the Hubble Space Telescope.\nHere we present the discovery of SPT0615-JD, a fortuitous z~10\n(z_phot=9.9+/-0.6) galaxy candidate stretched into an arc over ~2.5\" by the\neffects of strong gravitational lensing. Discovered in the Reionization Lensing\nCluster Survey (RELICS) Hubble Treasury program and companion S-RELICS Spitzer\nprogram, this candidate has a lensed H-band magnitude of 25.7+/-0.1 AB mag.\nWith a magnification of \\mu~4-7 estimated from our lens models, the de-lensed\nintrinsic magnitude is 27.6+/-0.3 AB mag, and the half-light radius is r_e<0.8\nkpc, both consistent with other z>9 candidates. The inferred stellar mass (log\n[M* /M_Sun]=9.7^{+0.7}_{-0.5}) and star formation rate (\\log [SFR/M_Sun\nyr^{-1}]=1.3^{+0.2}_{-0.3}) indicate that this candidate is a typical\nstar-forming galaxy on the z>6 SFR-M* relation. We note that three independent\nlens models predict two counterimages, at least one of which should be of a\nsimilar magnitude to the arc, but these counterimages are not yet detected.\nCounterimages would not be expected if the arc were at lower redshift. However,\nthe only spectral energy distributions capable of fitting the Hubble and\nSpitzer photometry well at lower redshifts require unphysical combinations of\nz~2 galaxy properties. The unprecedented lensed size of this z~10 candidate\noffers the potential for the James Webb Space Telescope to study the geometric\nand kinematic properties of a galaxy observed 500 Myr after the Big Bang."}, {"title": "Physical properties and scaling relations of molecular clouds: the effect of stellar feedback", "abstract": "Using hydrodynamical simulations of entire galactic discs similar to the\nMilky Way, reaching 4.6pc resolution, we study the origins of observed physical\nproperties of giant molecular clouds (GMCs). We find that efficient stellar\nfeedback is a necessary ingredient in order to develop a realistic interstellar\nmedium (ISM), leading to molecular cloud masses, sizes, velocity dispersions\nand virial parameters in excellent agreement with Milky Way observations. GMC\nscaling relations observed in the Milky Way, such as the mass-size ($M$--$R$),\nvelocity dispersion-size ($\\sigma$--$R$), and the $\\sigma$--$R\\Sigma$\nrelations, are reproduced in a feedback driven ISM when observed in projection,\nwith $M\\propto R^{2.3}$ and $\\sigma\\propto R^{0.56}$. When analysed in 3D, GMC\nscaling relations steepen significantly, indicating potential limitations of\nour understanding of molecular cloud 3D structure from observations.\nFurthermore, we demonstrate how a GMC population's underlying distribution of\nvirial parameters can strongly influence the scatter in derived scaling\nrelations. Finally, we show that GMCs with nearly identical global properties\nexist in different evolutionary stages, where a majority of clouds being either\ngravitationally bound or expanding, but with a significant fraction being\ncompressed by external ISM pressure, at all times."}, {"title": "Full-disc $^{13}$CO(1-0) mapping across nearby galaxies of the EMPIRE survey and the CO-to-H$_2$ conversion factor", "abstract": "Carbon monoxide (CO) provides crucial information about the molecular gas\nproperties of galaxies. While $^{12}$CO has been targeted extensively,\nisotopologues such as $^{13}$CO have the advantage of being less optically\nthick and observations have recently become accessible across full galaxy\ndiscs. We present a comprehensive new dataset of $^{13}$CO(1-0) observations\nwith the IRAM 30-m telescope of the full discs of 9 nearby spiral galaxies from\nthe EMPIRE survey at a spatial resolution of $\\sim$1.5kpc. $^{13}$CO(1-0) is\nmapped out to $0.7-1r_{25}$ and detected at high signal-to-noise throughout our\nmaps. We analyse the $^{12}$CO(1-0)-to-$^{13}$CO(1-0) ratio ($\\Re$) as a\nfunction of galactocentric radius and other parameters such as the\n$^{12}$CO(2-1)-to-$^{12}$CO(1-0) intensity ratio, the 70-to-160$\\mu$m flux\ndensity ratio, the star-formation rate surface density, the star-formation\nefficiency, and the CO-to-H$_2$ conversion factor. We find that $\\Re$ varies by\na factor of 2 at most within and amongst galaxies, with a median value of 11\nand larger variations in the galaxy centres than in the discs. We argue that\noptical depth effects, most likely due to changes in the mixture of\ndiffuse/dense gas, are favored explanations for the observed $\\Re$ variations,\nwhile abundance changes may also be at play. We calculate a spatially-resolved\n$^{13}$CO(1-0)-to-H$_2$ conversion factor and find an average value of\n$1.0\\times10^{21}$ cm$^{-2}$ (K.km/s)$^{-1}$ over our sample with a standard\ndeviation of a factor of 2. We find that $^{13}$CO(1-0) does not appear to be a\ngood predictor of the bulk molecular gas mass in normal galaxy discs due to the\npresence of a large diffuse phase, but it may be a better tracer of the mass\nthan $^{12}$CO(1-0) in the galaxy centres where the fraction of dense gas is\nlarger."}, {"title": "Merging massive black holes: the right place and the right time", "abstract": "The LIGO/Virgo detections of gravitational waves from merging black holes of\n$\\simeq$ 30 solar mass suggest progenitor stars of low metallicity\n(Z/Z$_{\\odot} \\lesssim 0.3$). In this talk I will provide constrains on where\nthe progenitors of GW150914 and GW170104 may have formed, based on advanced\nmodels of galaxy formation and evolution combined with binary population\nsynthesis models. First I will combine estimates of galaxy properties\n(star-forming gas metallicity, star formation rate and merger rate) across\ncosmic time to predict the low redshift BBH merger rate as a function of\npresent day host galaxy mass, formation redshift of the progenitor system and\ndifferent progenitor metallicities. I will show that the signal is dominated by\nbinaries formed at the peak of star formation in massive galaxies with and\nbinaries formed recently in dwarf galaxies. Then, I will present what very high\nresolution hydrodynamic simulations of different galaxy types can learn us\nabout their black hole populations."}, {"title": "Detailed abundance analysis of globular clusters in the Local Group: NGC 147, NGC 6822, and Messier 33", "abstract": "We present new abundance measurements for eleven GCs in the Local Group\ngalaxies NGC 147, NGC 6822, and Messier 33. These are combined with previously\npublished observations of four GCs in the Fornax and WLM galaxies. The\nabundances were determined from analysis of integrated-light spectra, obtained\nwith HIRES on the Keck I telescope and with UVES on the VLT. We find that the\nclusters with [Fe/H]<-1.5 are all alpha-enhanced at about the same level as\nMilky Way GCs. Their Na abundances are also generally enhanced relative to\nMilky Way halo stars, suggesting that these extragalactic GCs resemble their\nMilky Way counterparts in containing significant fractions of Na-rich stars.\nFor [Fe/H]>-1.5, the GCs in M33 are also alpha-enhanced, while the GCs that\nbelong to dwarfs (NGC 6822 SC7 and Fornax 4) have closer to Solar-scaled\nalpha-element abundances, thus mimicking the abundance trends observed in field\nstars in nearby dwarf galaxies. The abundance patterns in SC7 are remarkably\nsimilar to those in the Galactic GC Ruprecht 106, including significantly\nsub-solar [Na/Fe] and [Ni/Fe] ratios. In NGC 147, the GCs with [Fe/H]<-2.0\naccount for about 6% of the total luminosity of stars in the same metallicity\nrange, a lower fraction than those previously found in the Fornax and WLM\ngalaxies, but substantially higher than in the Milky Way halo."}, {"title": "The magnetic field structure in molecular cloud filaments", "abstract": "We explore the structure of magnetic field lines in and around filaments in\nsimulations of molecular clouds undergoing global, multi-scale gravitational\ncollapse. In these simulations, filaments are not in a static equilibrium, but\nare long-lived flow structures that accrete gas from their environment and\ndirect it toward clumps embedded in the filament or at the nodes at the\nconjunction with other filaments. In this context, the magnetic field is\ndragged by the collapsing gas, so its structure must reflect the flow that\ngenerates the filament. Around the filament, the gas is accreted onto it, and\nthe magnetic lines must then be perpendicular to the filament. As the gas\ndensity increases, the gas flow changes direction, becoming almost parallel to\nthe filament, and magnetic lines also tend to align with it. At the spine of\nthe filament, however, magnetic lines become perpendicular again since they\nmust connect to lines on the opposite side of the filament, resulting in\n\"U\"-shaped magnetic structures, which tend to be stretched by the longitudinal\nflow along the filament. Magnetic diffusive processes, however, allow the gas\nto continue to flow. Assuming a stationary state in which the ram pressure of\nthe flow balances the magnetic tension, the curvature of the field lines is\ndetermined by the diffusion rate. We derive an expression relating the\ncurvature of the field lines to the diffusive coefficient, which may be used to\nobservationally determine the nature of the diffusive process."}, {"title": "Continuum spectral energy distribution of GRB, SLSN, SB and AGN host galaxies at intermediate redshifts", "abstract": "Continuum SED models of gamma-ray burst (GRB) and obscured GRB host galaxies\nat moderately high redshifts are presented and compared with those of\nsuperluminous supernovae (SLSN), starburst (SB) and active galactic nuclei\n(AGN). We consider that continuum radiation (bremsstrahlung) is emitted from\nthe same clouds which emit the line spectrum in each object. Therefore, we have\nselected from the samples of the GRB host continuum observations those that\nwere previously modelled on the basis of the line spectra, because modelling\nthe continuum SED is less constraining. The bremsstrahlung is generally\nrecognised in the radio and in the UV-X-ray frequency ranges, while dust\nreradiation peaks in the IR. To reproduce the continuum SED of most of the GRB,\nSLSN, SB and AGN in the near-IR-optical range, the contribution of an old star\nbackground population is needed. This radiation can be reproduced by a black\nbody (bb) corresponding to temperatures T_{bb} =3000-8000 K. The best fit of a\nfew host SEDs includes also the direct contribution of the bb flux from the SB\ncorresponding to Ts about 5 10^4K. d/g calculated by modelling the SEDs of\nobscured GRB hosts roughly increases with z resembling the SFR trend."}, {"title": "Revisiting the Stellar Velocity Ellipsoid - Hubble type relation: observations versus simulations", "abstract": "The stellar velocity ellipsoid (SVE) in galaxies can provide important\ninformation on the processes that participate in the dynamical heating of their\ndisc components (e.g. giant molecular clouds, mergers, spiral density waves,\nbars). Earlier findings suggested a strong relation between the shape of the\ndisc SVE and Hubble type, with later-type galaxies displaying more anisotropic\nellipsoids and early-types being more isotropic. In this paper, we revisit the\nstrength of this relation using an exhaustive compilation of observational\nresults from the literature on this issue. We find no clear correlation between\nthe shape of the disc SVE and morphological type, and show that galaxies with\nthe same Hubble type display a wide range of vertical-to-radial velocity\ndispersion ratios. The points are distributed around a mean value and scatter\nof $\\sigma_z/\\sigma_R=0.7\\pm 0.2$. With the aid of numerical simulations, we\nargue that different mechanisms might influence the shape of the SVE in the\nsame manner and that the same process (e.g. mergers) does not have the same\nimpact in all the galaxies. The complexity of the observational picture is\nconfirmed by these simulations, which suggest that the vertical-to-radial axis\nratio of the SVE is not a good indicator of the main source of disc heating.\nOur analysis of those simulations also indicates that the observed shape of the\ndisc SVE may be affected by several processes simultaneously and that the\nsignatures of some of them (e.g. mergers) fade over time."}, {"title": "Photometric metallicity map of the Small Magellanic Cloud", "abstract": "We have created an estimated metallicity map of the Small Magellanic Cloud\n(SMC) using the Magellanic Cloud Photometric Survey (MCPS) and Optical\nGravitational Lensing Experiment (OGLE III) photometric data. This is a first\nof its kind map of metallicity up to a radius of $\\sim$ 2.5$^{\\circ}$. We\nidentify the RGB in the V, (V$-$I) colour magnitude diagrams of small\nsubregions of varying sizes in both data sets. We use the slope of the RGB as\nan indicator of the average metallicity of a subregion, and calibrate the RGB\nslope to metallicity using available spectroscopic data for selected\nsubregions. The average metallicity of the SMC is found to be [Fe/H] = $-$0.94\ndex ($\\sigma$[Fe/H] = 0.09) from OGLE III, and [Fe/H] = $-$0.95 dex\n($\\sigma$[Fe/H] = 0.08) from MCPS. We confirm a shallow but significant\nmetallicity gradient within the inner SMC up to a radius of 2.5$^{\\circ}$\n($-$0.045$\\pm$0.004 dex deg$^{-1}$ to $-$0.067$\\pm$0.006 dex deg$^{-1}$)."}, {"title": "Searching for Dual Active Galactic Nuclei", "abstract": "Binary or dual active galactic nuclei (DAGN) are expected from galaxy\nformation theories. However, confirmed DAGN are rare and finding these systems\nhas proved to be challenging. Recent systematic searches for DAGN using\ndouble-peaked emission lines have yielded several new detections. As have the\nstudies of samples of merging galaxies. In this paper, we present an updated\nlist of DAGN compiled from published data. We also present preliminary results\nfrom our ongoing Expanded Very Large Array (EVLA) radio study of eight double-\npeaked emission-line AGN (DPAGN). One of the sample galaxy shows an S-shaped\nradio jet. Using new and archival data, we have successfully fitted a\nprecessing jet model to this radio source. We find that the jet precession\ncould be due to a binary AGN with a super-massive black-hole (SMBH) separation\nof 0.02 pc or a single AGN with a tilted accretion disk. We have found that\nanother sample galaxy, which is undergoing a merger, has two radio cores with a\nprojected separation of 5.6 kpc. We discuss the preliminary results from our\nradio study."}, {"title": "Spatially-Offset AGN Candidates in the CLASS Survey", "abstract": "Prompted by a recent claim by Barrows et al. that X-ray AGN are often found\nsignificantly offset from the centres of their host galaxies, we have looked\nfor examples of compact radio sources which are offset from the optical\ncentroids of nearby (z < 0.2) galaxies. We have selected a sample of 345\ngalaxies from the Sloan Digital Sky Survey (SDSS) galaxy catalog which have\nnearby compact radio sources listed in the Cosmic-Lens All Sky Survey (CLASS)\ncatalog. We find only three matches (0.87 per cent of the sample) with offsets\ngreater than 600 milliarcsec (mas), which is considerably fewer than we would\nhave expected from the Barrows et al. X-ray survey. We fit our histogram of\noffsets with a Rayleigh distribution with {\\sigma} = 60.5 mas, but find that\nthere is an excess of objects with separations greater than approximately 150\nmas. Assuming that this excess represents AGN with real offsets, we place an\nupper limit of approximately 17 per cent on the fraction of offset AGN in our\nradio-selected sample. We select 38 objects with offsets greater than 150 mas,\nand find they have some diverse properties: some are well known, such as Mrk\n273 and Arp 220, and others have dust lanes which may have affected the optical\nastrometry, while a few are strong new candidates for offset AGN."}, {"title": "A search for Cyanopolyynes in L1157-B1", "abstract": "We present here a systematic search for cyanopolyynes in the shock region\nL1157-B1 and its associated protostar L1157-mm in the framework of the Large\nProgram \"Astrochemical Surveys At IRAM\" (ASAI), dedicated to chemical surveys\nof solar-type star forming regions with the IRAM 30m telescope. Observations of\nthe millimeter windows between 72 and 272 GHz permitted the detection of\nHC$_3$N and its $^{13}$C isotopologues, and HC$_5$N (for the first time in a\nprotostellar shock region). In the shock, analysis of the line profiles shows\nthat the emission arises from the outflow cavities associated with L1157-B1 and\nL1157-B2. Molecular abundances and excitation conditions were obtained from\nanalysis of the Spectral Line Energy Distributions under the assumption of\nLocal Thermodynamical Equilibrium or using a radiative transfer code in the\nLarge Velocity Gradient approximation. Towards L1157mm, the HC$_3$N emission\narises from the cold envelope ($T_{rot}=10$ K) and a higher-excitation region\n($T_{rot}$= $31$ K) of smaller extent around the protostar. We did not find any\nevidence of $^{13}$C or D fractionation enrichment towards L1157-B1. We obtain\na relative abundance ratio HC$_3$N/HC$_5$N of 3.3 in the shocked gas. We find\nan increase by a factor of 30 of the HC$_3$N abundance between the envelope of\nL1157-mm and the shock region itself. Altogether, these results are consistent\nwith a scenario in which the bulk of HC$_3$N was produced by means of gas phase\nreactions in the passage of the shock. This scenario is supported by the\npredictions of a parametric shock code coupled with the chemical model\nUCL_CHEM."}, {"title": "Chemical pre-processing of cluster galaxies over the past 10 billion years in the IllustrisTNG simulations", "abstract": "We use the IllustrisTNG simulations to investigate the evolution of the\nmass-metallicity relation (MZR) for star-forming cluster galaxies as a function\nof the formation history of their cluster host. The simulations predict an\nenhancement in the gas-phase metallicities of star-forming cluster galaxies\n(10^9< M_star<10^10 M_sun) at z<1.0 in comparisons to field galaxies. This is\nqualitatively consistent with observations. We find that the metallicity\nenhancement of cluster galaxies appears prior to their infall into the central\ncluster potential, indicating for the first time a systematic \"chemical\npre-processing\" signature for {\\it infalling} cluster galaxies. Namely,\ngalaxies which will fall into a cluster by z=0 show a ~0.05 dex enhancement in\nthe MZR compared to field galaxies at z<0.5. Based on the inflow rate of gas\ninto cluster galaxies and its metallicity, we identify that the accretion of\npre-enriched gas is the key driver of the chemical evolution of such galaxies,\nparticularly in the stellar mass range (10^9< M_star<10^10 M_sun). We see\nsignatures of an environmental dependence of the ambient/inflowing gas\nmetallicity which extends well outside the nominal virial radius of clusters.\nOur results motivate future observations looking for pre-enrichment signatures\nin dense environments."}, {"title": "Probing star formation and ISM properties using galaxy disk inclination I: Evolution in disk opacity since $z\\sim0.7$", "abstract": "Disk galaxies at intermediate redshift ($z\\sim0.7$) have been found in\nprevious work to display more optically thick behaviour than their local\ncounterparts in the rest-frame B-band surface brightness, suggesting an\nevolution in dust properties over the past $\\sim$6 Gyr. We compare the measured\nluminosities of face-on and edge-on star-forming galaxies at different\nwavelengths (Ultraviolet (UV), mid-infrared (MIR), far-infrared (FIR), and\nradio) for two well-matched samples of disk-dominated galaxies: a local Sloan\nDigital Sky Survey (SDSS)-selected sample at $z\\sim0.07$ and a sample of disks\nat $z\\sim0.7$ drawn from Cosmic Evolution Survey (COSMOS). We have derived\ncorrection factors to account for the inclination dependence of the parameters\nused for sample selection. We find that typical galaxies are transparent at MIR\nwavelengths at both redshifts and that the FIR and radio emission is also\ntransparent as expected. However, reduced sensitivity at these wavelengths\nlimits our analysis; we cannot rule out opacity in the FIR or radio.\nUltra-violet attenuation has increased between $z\\sim0$ and $z\\sim0.7$, with\nthe $z\\sim0.7$ sample being a factor of $\\sim$3.4 more attenuated. The larger\nUV attenuation at $z\\sim0.7$ can be explained by more clumpy dust around\nnascent star-forming regions. There is good agreement between the fitted\nevolution of the normalisation of the SFR$_{\\text{UV}}$ versus 1-cos(i) trend\n(interpreted as the clumpiness fraction) and the molecular gas fraction/dust\nfraction evolution of galaxies found out to $z<1$."}, {"title": "ALMA observations of AGN fuelling: the case of PKS B1718-649", "abstract": "We present ALMA observations of the $^{12}$CO (2--1) line of the newly born\n($t_\\mathrm{radio}\\sim10^2$ years) active galactic nucleus (AGN), PKS\nB1718-649. These observations reveal that the carbon monoxide in the innermost\n15 kpc of the galaxy is distributed in a complex warped disk. In the outer\nparts of this disk, the CO gas follows the rotation of the dust lane and of the\nstellar body of the galaxy hosting the radio source. In the innermost\nkiloparsec, the gas abruptly changes orientation and forms a circumnuclear disk\n($r\\lesssim700$ pc) with its major axis perpendicular to that of the outer\ndisk. Against the compact radio emission of PKS B1718-649 ($r\\sim 2$ pc), we\ndetect an absorption line at red-shifted velocities with respect to the\nsystemic velocity ($\\Delta v = +365\\pm22$\\kms). This absorbing CO gas could\ntrace molecular clouds falling onto the central super-massive black hole. A\ncomparison with the near-infra red H$_{\\,2}$ 1-0 S(1) observations shows that\nthe clouds must be close to the black hole ($r\\lesssim 75$ pc). The physical\nconditions of these clouds are different from the gas at larger radii, and are\nin good agreement with the predictions for the conditions of the gas when cold\nchaotic accretion triggers an active galactic nucleus. These observations on\nthe centre of PKS B1718-649 provide one of the best indications that a\npopulation of cold clouds is falling towards a radio AGN, likely fuelling its\nactivity."}, {"title": "Formation of Globular Cluster Systems: From Dwarf Galaxies to Giants", "abstract": "Globular cluster (GC) systems around galaxies of a vast mass range show\nremarkably simple scaling relations. The combined mass of all GCs is a constant\nfraction of the total galaxy mass and the mean metallicity and metallicity\ndispersion of the GC system scale up weakly with galaxy mass. The metallicity\nof massive, metal-poor (\"blue\") clusters increases with cluster mass, while\nthat of metal-rich (\"red\") clusters does not. A significant age-metallicity\nrelation emerges from analysis of resolved stellar populations in Galactic GCs\nand unresolved populations in nearby galaxies. Remarkably, all these trends can\nbe explained by a simple merger-based model developed in previous work and\nupdated here using recent observations of galaxy scaling relations at high\nredshift. We show that the increasing dispersion of GC metallicity\ndistributions with galaxy mass is a robust prediction of the model. It arises\nfrom more massive galaxies having more mergers that combine satellite GC\nsystems. The average metallicity also increases by 0.6~dex over 3~dex in halo\nmass. The models show a non-linear trend between the GC system mass and host\ngalaxy mass which is consistent with the data. The model does not consider GC\nself-enrichment, yet predicts a correlation between cluster mass and\nmetallicity for massive blue clusters. The age-metallicity relation is another\nrobust prediction of the model. Half of all clusters are predicted to form\nwithin the redshift range $5<z<2.3$, corresponding to ages of $10.8-12.5$~Gyr,\nin halos of masses $10^{11}-10^{12.5} M_{\\odot}$."}, {"title": "First Results from the $Herschel$ and ALMA Spectroscopic Surveys of the SMC: The Relationship Between [CII]-bright Gas and CO-bright Gas at Low Metallicity", "abstract": "The Small Magellanic Cloud (SMC) provides the only laboratory to study the\nstructure of molecular gas at high resolution and low metallicity. We present\nresults from the Herschel Spectroscopic Survey of the SMC (HS$^{3}$), which\nmapped the key far-IR cooling lines [CII], [OI], [NII], and [OIII] in five\nstar-forming regions, and new ALMA 7m-array maps of $^{12}$CO and $^{13}$CO\n$(2-1)$ with coverage overlapping four of the five HS$^{3}$ regions. We detect\n[CII] and [OI] throughout all of the regions mapped. The data allow us to\ncompare the structure of the molecular clouds and surrounding photodissociation\nregions using $^{13}$CO, CO, [CII], and [OI] emission at $<10$\" ($<3$ pc)\nscales. We estimate Av using far-IR thermal continuum emission from dust and\nfind the CO/[CII] ratios reach the Milky Way value at high A$_{V}$ in the\ncenters of the clouds and fall to $\\sim{1/5-1/10}\\times$ the Milky Way value in\nthe outskirts, indicating the presence of translucent molecular gas not traced\nby bright CO emission. We estimate the amount of molecular gas traced by bright\n[CII] emission at low A$_{V}$ and bright CO emission at high A$_{V}$. We find\nthat most of the molecular gas is at low A$_{V}$ and traced by bright [CII]\nemission, but that faint CO emission appears to extend to where we estimate the\nH$_{2}$-to-HI transition occurs. By converting our H$_{2}$ gas estimates to a\nCO-to-H$_{2}$ conversion factor ($X_{CO}$), we show that $X_{CO}$ is primarily\na function of A$_{V}$, consistent with simulations and models of low\nmetallicity molecular clouds."}, {"title": "Radio-emitting narrow-line Seyfert 1 galaxies in the JVLA perspective", "abstract": "We report the first results of a survey on 74 narrow-line Seyfert 1 galaxies\n(NLS1s) carried out in 2015 with the Karl G. Jansky Very Large Array (JVLA) at\n5 GHz in A-configuration. So far, this is the largest survey aimed to image the\nradio continuum of NLS1s. We produced radio maps in order to compare the\ngeneral properties of three different samples of objects: radio-quiet NLS1s\n(RQNLS1s), steep-spectrum radio-loud NLS1s (S-NLS1s), and flat-spectrum\nradio-loud NLS1s (F-NLS1s). We find that the three classes correspond to\ndifferent radio morphologies, with F-NLS1s being more compact, and RQNLS1s\noften showing diffuse emission on kpc scales. We also find that F-NLS1s might\nbe low-luminosity and possibly young blazars, and that S-NLS1s are part of the\nparent population of F-NLS1s. Dedicated studies to RQNLS1s are needed in order\nto fully understand their role in the unification pictures."}, {"title": "Galactic Reddening in 3D from Stellar Photometry - An Improved Map", "abstract": "We present a new 3D map of interstellar dust reddening, covering three\nquarters of the sky (declinations greater than -30 degrees) out to a distance\nof several kiloparsecs. The map is based on high-quality stellar photometry of\n800 million stars from Pan-STARRS 1 and 2MASS. We divide the sky into\nsightlines containing a few hundred stars each, and then infer stellar\ndistances and types, along with the line-of-sight dust distribution. Our new\nmap incorporates a more accurate average extinction law and an additional 1.5\nyears of Pan-STARRS 1 data, tracing dust to greater extinctions and at higher\nangular resolutions than our previous map. Out of the plane of the Galaxy, our\nmap agrees well with 2D reddening maps derived from far-infrared dust emission.\nAfter accounting for a 15% difference in scale, we find a mean scatter of 10%\nbetween our map and the Planck far-infrared emission-based dust map, out to a\ndepth of 0.8 mag in E(r-z), with the level of agreement varying over the sky.\nOur map can be downloaded at http://argonaut.skymaps.info, or by its DOI:\n10.7910/DVN/LCYHJG."}, {"title": "The origin of diverse $α$-element abundances in galaxy discs", "abstract": "Spectroscopic surveys of the Galaxy reveal that its disc stars exhibit a\nspread in $\\mathrm{[\\alpha/Fe]}$ at fixed $\\mathrm{[Fe/H]}$, manifest at some\nlocations as a bimodality. The origin of these diverse, and possibly distinct,\nstellar populations in the Galactic disc is not well understood. We examine the\nFe and $\\alpha$-element evolution of 133 Milky Way-like galaxies from the EAGLE\nsimulation, to investigate the origin and diversity of their\n$\\mathrm{[\\alpha/Fe]}$-$\\mathrm{[Fe/H]}$ distributions. We find that bimodal\n$\\mathrm{[\\alpha/Fe]}$ distributions arise in galaxies whose gas accretion\nhistories exhibit episodes of significant infall at both early and late times,\nwith the former fostering more intense star formation than the latter. The\nshorter characteristic consumption timescale of gas accreted in the earlier\nepisode suppresses its enrichment with iron synthesised by Type Ia SNe,\nresulting in the formation of a high-$\\mathrm{[\\alpha/Fe]}$ sequence. We find\nthat bimodality in $\\mathrm{[\\alpha/Fe]}$ similar to that seen in the Galaxy is\nrare, appearing in approximately 5 percent of galaxies in our sample. We posit\nthat this is a consequence of an early gas accretion episode requiring the mass\naccretion history of a galaxy's dark matter halo to exhibit a phase of\natypically-rapid growth at early epochs. The scarcity of EAGLE galaxies\nexhibiting distinct sequences in the $\\mathrm{[\\alpha/Fe]}$-$\\mathrm{[Fe/H]}$\nplane may therefore indicate that the Milky Way's elemental abundance patterns,\nand its accretion history, are not representative of the broader population of\n$\\sim L^\\star$ disc galaxies."}, {"title": "Abell 1367: a high fraction of late-type galaxies displaying HI morphological and kinematic perturbations", "abstract": "To investigate the effects the cluster environment has on Late-Type Galaxies\n(LTGs) we studied HI perturbation signatures for all Abell 1367 LTGs with HI\ndetections. We used new VLA HI observations combined with AGES single dish\nblind survey data. Our study indicates that the asymmetry between the high-and\nlow-velocity wings of the characteristic double-horn integrated HI spectrum as\nmeasured by the asymmetry parameter, Aflux, can be a useful diagnostic for\nongoing and/or recent HI stripping. 26% of A1367 LTGs have an Aflux ratio, more\nasymmetrical than 3 times the 1{\\sigma} spread in the Aflux ratio distribution\nof an undisturbed sample of isolated galaxies (2%) and samples from other\ndenser environments (10% to 20%). Over half of the A 1367 LTGs, which are\nmembers of groups or pairs, have an Aflux ratio larger than twice the 1\n{\\sigma} spread found in the isolated sample. This suggests inter-group/pair\ninteractions could be making a significant contribution to the LTGs displaying\nsuch Aflux ratios. The study also demonstrates that the definition of the HI\noffset from the optical centre of LTGs is resolution dependent, suggesting that\nunresolved AGES HI offsets that are significantly larger than the pointing\nuncertainties (> 2 {\\sigma}) reflect interactions which have asymmetrically\ndisplaced significant masses of lower density HI, while having minimal impact\non the location of the highest density HI in resolved maps. The distribution of\nAflux from a comparable sample of Virgo galaxies provides a clear indication\nthat the frequency of HI profile perturbations is lower than in A 1367."}, {"title": "Disc-Halo Interactions in ΛCDM", "abstract": "We present a new method for embedding a stellar disc in a cosmological dark\nmatter halo and provide a worked example from a {\\Lambda}CDM zoom-in\nsimulation. The disc is inserted into the halo at a redshift z = 3 as a\nzero-mass rigid body. Its mass and size are then increased adiabatically while\nits position, velocity, and orientation are determined from rigid-body\ndynamics. At z = 1, the rigid disc is replaced by an N-body disc whose\nparticles sample a three-integral distribution function (DF). The simulation\nthen proceeds to z = 0 with live disc and halo particles. By comparison, other\nmethods assume one or more of the following: the centre of the rigid disc\nduring the growth phase is pinned to the minimum of the halo potential, the\norientation of the rigid disc is fixed, or the live N-body disc is constructed\nfrom a two rather than three-integral DF. In general, the presence of a disc\nmakes the halo rounder, more centrally concentrated, and smoother, especially\nin the innermost regions. We find that methods in which the disc is pinned to\nthe minimum of the halo potential tend to overestimate the amount of adiabatic\ncontraction. Additionally, the effect of the disc on the subhalo distribution\nappears to be rather insensitive to the disc insertion method. The live disc in\nour simulation develops a bar that is consistent with the bars seen in\nlate-type spiral galaxies. In addition, particles from the disc are launched or\n\"kicked up\" to high galactic latitudes."}, {"title": "The nuclear activity and central structure of the elliptical galaxy NGC 5322", "abstract": "We have analysed a new high-resolution e-MERLIN 1.5 GHz radio continuum map\ntogether with $HST$ and SDSS imaging of NGC 5322, an elliptical galaxy hosting\nradio jets, aiming to understand the galaxy's central structure and its\nconnection to the nuclear activity. We decomposed the composite $HST$ + SDSS\nsurface brightness profile of the galaxy into an inner stellar disc, a\nspheroid, and an outer stellar halo. Past works showed that this embedded disc\ncounter-rotates rapidly with respect to the spheroid. The $HST$ images reveal\nan edge-on nuclear dust disc across the centre, aligned along the major-axis of\nthe galaxy and nearly perpendicular to the radio jets. After careful masking of\nthis dust disc, we find a central stellar mass deficit $M_{\\rm def}$ in the\nspheroid, scoured by SMBH binaries with final mass $M_{\\rm BH}$ such that\n$M_{\\rm def}/M_{\\rm BH} \\sim 1.3 - 3.4$. We propose a three-phase formation\nscenario for NGC 5322 where a few ($2-7$) \"dry\" major mergers involving SMBHs\nbuilt the spheroid with a depleted core. The cannibalism of a gas-rich\nsatellite subsequently creates the faint counter-rotating disc and funnels\ngaseous material directly onto the AGN, powering the radio core with a\nbrightness temperature of $T_{\\rm B,core} \\sim 4.5 \\times 10^{7}$ K and the\nlow-power radio jets ($P_{\\rm jets}\\sim 7.04 \\times 10^{20}$ W Hz$^{-1}$) which\nextend $\\sim 1.6$ kpc. The outer halo can later grow via minor mergers and the\naccretion of tidal debris. The low-luminosity AGN/jet-driven feedback may have\nquenched the late-time nuclear star formation promptly, which could otherwise\nhave replenished the depleted core."}, {"title": "Galaxy And Mass Assembly (GAMA): The effect of galaxy group environment on active galactic nuclei", "abstract": "In galaxy clusters, efficiently accreting active galactic nuclei (AGN) are\npreferentially located in the infall regions of the cluster projected\nphase-space, and are rarely found in the cluster core. This has been attributed\nto both an increase in triggering opportunities for infalling galaxies, and a\nreduction of those mechanisms in the hot, virialised, cluster core. Exploiting\nthe depth and completeness ($98\\,$per cent at $r<19.8\\,$mag) of the Galaxy And\nMass Assembly survey (GAMA), we probe down the group halo mass function to\nassess whether AGN are found in the same regions in groups as they are in\nclusters. We select 451 optical AGN from 7498 galaxies with\n$\\log_{10}(M_*/\\text{M}_\\odot) > 9.9$ in 695 groups with $11.53\\leq\n\\log_{10}(M_{200}/\\text{M}_\\odot) \\leq 14.56$ at $z<0.15$. By analysing the\nprojected phase-space positions of these galaxies we demonstrate that when\nsplit both radially, and into physically derived infalling and core\npopulations, AGN position within group projected phase-space is dependent on\nhalo mass. For groups with $\\log_{10}(M_{200}/\\text{M}_\\odot)>13.5$, AGN are\npreferentially found in the infalling galaxy population with $3.6\\sigma$\nconfidence. At lower halo masses we observe no difference in AGN fraction\nbetween core and infalling galaxies. These observations support a model where a\nreduced number of low-speed interactions, ram pressure stripping and\nintra-group/cluster medium temperature, the dominance of which increase with\nhalo mass, work to inhibit AGN in the cores of groups and clusters with\n$\\log_{10}(M_{200}/\\text{M}_\\odot)>13.5$, but do not significantly affect\nnuclear activity in cores of less massive structures."}, {"title": "ALMA Dust Polarization Observations of Two Young Edge-on Protostellar Disks", "abstract": "Polarized emission is detected in two young nearly edge-on protostellar disks\nin 343 GHz continuum at ~ 50 au (~ 0.12\") resolution with Atacama Large\nMillimeter/submillimeter Array. One disk is in HH 212 (Class 0) and the other\nin HH 111 (early Class I) protostellar system. Polarization fraction is ~ 1%.\nThe disk in HH 212 has a radius of ~ 60 au. The emission is mainly detected\nfrom the nearside of the disk. The polarization orientations are almost\nperpendicular to the disk major axis, consistent with either self-scattering or\nemission by grains aligned with a poloidal field around the outer edge of the\ndisk because of optical depth effect and temperature gradient; the presence of\na poloidal field would facilitate the launching of a disk wind, for which there\nis already tentative evidence in the same source. The disk of HH 111 VLA 1 has\na larger radius of ~ 220 au and is thus more resolved. The polarization\norientations are almost perpendicular to the disk major axis in the nearside,\nbut more along the major axis in the farside, forming roughly half of an\nelliptical pattern there. It appears that toroidal and poloidal magnetic field\nmay explain the polarization on the near and far side of the disk,\nrespectively. However, it is also possible that the polarization is due to\nself-scattering. In addition, alignment of dust grains by radiation flux may\nplay a role in the farside. Our observations reveal a diversity of disk\npolarization patterns that should be taken into account in future modeling\nefforts."}, {"title": "A method for determining the radius of an open cluster from stellar proper motions", "abstract": "We propose a method for calculating the radius of an open cluster in an\nobjective way from an astrometric catalogue containing, at least, positions and\nproper motions. It uses the minimum spanning tree (hereinafter MST) in the\nproper motion space to discriminate cluster stars from field stars and it\nquantifies the strength of the cluster-field separation by means of a\nstatistical parameter defined for the first time in this paper. This is done\nfor a range of different sampling radii from where the cluster radius is\nobtained as the size at which the best cluster-field separation is achieved.\nThe novelty of this strategy is that the cluster radius is obtained\nindependently of how its stars are spatially distributed. We test the\nreliability and robustness of the method with both simulated and real data from\na well-studied open cluster (NGC 188), and apply it to UCAC4 data for five\nother open clusters with different catalogued radius values. NGC 188, NGC 1647,\nNGC 6603 and Ruprecht 155 yielded unambiguous radius values of 15.2+/-1.8,\n29.4+/-3.4, 4.2+/-1.7 and 7.0+/-0.3 arcmin, respectively. ASCC 19 and Collinder\n471 showed more than one possible solution but it is not possible to know\nwhether this is due to the involved uncertainties or to the presence of complex\npatterns in their proper motion distributions, something that could be inherent\nto the physical object or due to the way in which the catalogue was sampled."}, {"title": "Reverberation Mapping of High-z, High-luminosity Quasars", "abstract": "We present Reverberation Mapping results after monitoring a sample of 17\nhigh-z, high-luminosity quasars for more than 10 years using photometric and\nspectroscopic capabilities. Continuum and line emission flux variability is\nobserved in all quasars. Using cross-correlation analysis we successfully\ndetermine lags between the variations in the continuum and broad emission lines\nfor several sources. Here we present a highlight of our results and the\ndetermined radius--luminosity relations for Ly_alpha and CIV."}, {"title": "Semi-Analytic Galaxies - I. Synthesis of environmental and star-forming regulation mechanisms", "abstract": "We present results from the semi-analytic model of galaxy formation SAG\napplied on the MultiDark simulation MDPL2. SAG features an updated supernova\n(SN) feedback scheme and a robust modelling of the environmental effects on\nsatellite galaxies. This incorporates a gradual starvation of the hot gas halo\ndriven by the action of ram pressure stripping (RPS), that can affect the cold\ngas disc, and tidal stripping (TS), which can act on all baryonic components.\nGalaxy orbits of orphan satellites are integrated providing adequate positions\nand velocities for the estimation of RPS and TS. The star formation history and\nstellar mass assembly of galaxies are sensitive to the redshift dependence\nimplemented in the SN feedback model. We discuss a variant of our model that\nallows to reconcile the predicted star formation rate density at $z \\gtrsim 3$\nwith the observed one, at the expense of an excess in the faint end of the\nstellar mass function at $z=2$. The fractions of passive galaxies as a function\nof stellar mass, halo mass and the halo-centric distances are consistent with\nobservational measurements. The model also reproduces the evolution of the main\nsequence of star forming central and satellite galaxies. The similarity between\nthem is a result of the gradual starvation of the hot gas halo suffered by\nsatellites, in which RPS plays a dominant role. RPS of the cold gas does not\naffect the fraction of quenched satellites but it contributes to reach the\nright atomic hydrogen gas content for more massive satellites\n($M_{\\star}\\gtrsim 10^{10}\\,{\\rm M}_{\\odot}$)."}, {"title": "Semi-Analytic Galaxies - II. Revealing the role of environmental and mass quenching in galaxy formation", "abstract": "We use the semi-analytic model of galaxy formation SAG to study the relevance\nof mass and environmental quenching on satellite galaxies. We find that\nenvironmental processes dominate the star formation (SF) quenching of low-mass\nsatellites ($M_{\\star} \\lesssim 10^{10.5}\\, {\\rm M}_{\\odot}$), whereas\nhigh-mass galaxies typically quench as centrals. High-mass galaxies that remain\nactively forming stars while being accreted are found to be mainly affected by\nmass quenching after their first infall. For a given stellar mass, our model\npredicts SF quenching to be less efficient in low-mass haloes both before and\nafter infall, in contradiction with common interpretations of observational\ndata. Our model supports a two-stage scenario to explain the SF quenching.\nInitially, the SF of satellites resembles that of centrals until the gas\ncooling rate is reduced to approximately half its value at infall. Then, the SF\nfades through secular processes that exhaust the cold gas reservoir. This\nreservoir is not replenished efficiently due to the action of either\nram-pressure stripping (RPS) of the hot gas in low-mass satellites, or feedback\nfrom the active galactic nucleus (AGN) in high-mass satellites. The delay times\nfor the onset of SF quenching are found to range from $\\approx 3\\,{\\rm Gyr}$ to\n$\\approx 1\\,{\\rm Gyr}$ for low-mass ($M_{\\star} \\approx 10^{10}\\, {\\rm\nM}_{\\odot}$) and high-mass ($M_{\\star} \\approx 10^{11}\\, {\\rm M}_{\\odot}$)\nsatellites, respectively. SF fades in $\\approx 1.5\\,{\\rm Gyr}$, largely\nindependent of stellar mass. We find that the SF quenching of low-mass\nsatellites supports the so-called delay-then-rapid quenching scenario. However,\nthe SF history of $z=0$ passive satellites of any stellar mass is better\ndescribed by a delay-then-fade quenching scenario."}, {"title": "Structure of photodissociation fronts in star-forming regions revealed by observations of high-J CO emission lines with Herschel", "abstract": "In bright photodissociation regions (PDRs) associated to massive star\nformation, the presence of dense \"clumps\" that are immersed in a less dense\ninterclump medium is often proposed to explain the difficulty of models to\naccount for the observed gas emission in high-excitation lines. We aim at\npresenting a comprehensive view of the modeling of the CO rotational ladder in\nPDRs, including the high-J lines that trace warm molecular gas at PDR\ninterfaces. We observed the 12CO and 13CO ladders in two prototypical PDRs, the\nOrion Bar and NGC 7023 NW using the instruments onboard Herschel. We also\nconsidered line emission from key species in the gas cooling of PDRs (C+, O,\nH2) and other tracers of PDR edges such as OH and CH+. All the intensities are\ncollected from Herschel observations, the literature and the Spitzer archive\nand are analyzed using the Meudon PDR code. A grid of models was run to explore\nthe parameter space of only two parameters: thermal gas pressure and a global\nscaling factor that corrects for approximations in the assumed geometry. We\nconclude that the emission in the high-J CO lines, which were observed up to\nJup=23 in the Orion Bar (Jup=19 in NGC7023), can only originate from small\nstructures of typical thickness of a few 1e-3 pc and at high thermal pressures\n(Pth~1e8 K cm-3). Compiling data from the literature, we found that the gas\nthermal pressure increases with the intensity of the UV radiation field given\nby G0, following a trend in line with recent simulations of the\nphotoevaporation of illuminated edges of molecular clouds. This relation can\nhelp rationalising the analysis of high-J CO emission in massive star formation\nand provides an observational constraint for models that study stellar feedback\non molecular clouds."}, {"title": "Gas Kinematics in FIRE Simulated Galaxies Compared to Spatially Unresolved HI Observations", "abstract": "The shape of a galaxy's spatially unresolved, globally integrated 21-cm\nemission line depends on its internal gas kinematics: galaxies with\nrotation-supported gas disks produce double-horned profiles with steep wings,\nwhile galaxies with dispersion-supported gas produce Gaussian-like profiles\nwith sloped wings. Using mock observations of simulated galaxies from the FIRE\nproject, we show that one can therefore constrain a galaxy's gas kinematics\nfrom its unresolved 21-cm line profile. In particular, we find that the\nkurtosis of the 21-cm line increases with decreasing $V/\\sigma$, and that this\ntrend is robust across a wide range of masses, signal-to-noise ratios, and\ninclinations. We then quantify the shapes of 21-cm line profiles from a\nmorphologically unbiased sample of $\\sim$2000 low-redshift, HI-detected\ngalaxies with $M_{\\rm star} = 10^{7-11} M_{\\odot}$ and compare to the simulated\ngalaxies. At $M_{\\rm star} \\gtrsim 10^{10} M_{\\odot}$, both the observed and\nsimulated galaxies produce double-horned profiles with low kurtosis and steep\nwings, consistent with rotation-supported disks. Both the observed and\nsimulated line profiles become more Gaussian-like (higher kurtosis and\nless-steep wings) at lower masses, indicating increased dispersion support.\nHowever, the simulated galaxies transition from rotation to dispersion support\nmore strongly: at $M_{\\rm star} = 10^{8-10}M_{\\odot}$, most of the simulations\nproduce more Gaussian-like profiles than typical observed galaxies with similar\nmass, indicating that gas in the low-mass simulated galaxies is, on average,\noverly dispersion-supported. Most of the lower-mass simulated galaxies also\nhave somewhat lower gas fractions than the median of the observed population.\nThe simulations nevertheless reproduce the observed line-width baryonic\nTully-Fisher relation, which is insensitive to rotation vs. dispersion support."}, {"title": "On the claimed X-shaped structure in the Milky Way bulge", "abstract": "A number of recent studies have claimed that the double red clump (RC)\nobserved in the Milky Way bulge is a consequence of a giant X-shaped structure.\nIn particular, Ness & Lang (2016) reported a direct detection of a faint\nX-shaped structure from the residual map of the Wide-Field Infrared Survey\nExplorer (WISE) bulge image. Here we show, however, that their result is\naffected substantially by whether the conventional dust extinction correction\nis applied or not and partly by a bulge model subtracted from the original\nimage. We find that the residuals obtained by subtracting either ellipsoidal or\nboxy bulge models from the dereddened images show no obvious X-shaped\nstructure. We further show that, even if it is real, the stellar density in the\nclaimed X-shaped structure is way too low to be observed as a strong double RC\nat l=0."}, {"title": "Average [O II] nebular emission associated with Mg II absorbers: Dependence on Fe II absorption", "abstract": "We investigate the effect of Fe II equivalent width ($W_{2600}$) and fibre\nsize on the average luminosity of [O II]$\\lambda\\lambda$3727,3729 nebular\nemission associated with Mg II absorbers (at $0.55 \\le z \\le 1.3$) in the\ncomposite spectra of quasars obtained with 3 and 2 arcsec fibres in the Sloan\nDigital Sky Survey. We confirm the presence of strong correlations between [O\nII] luminosity (L$_{[\\rm O~II]}$) and equivalent width ($W_{2796}$) and\nredshift of Mg II absorbers. However, we show L$_{[\\rm O~II]}$ and average\nluminosity surface density suffers from fibre size effects. More importantly,\nfor a given fibre size the average L$_{[\\rm O~II]}$ strongly depends on the\nequivalent width of Fe II absorption lines and found to be higher for Mg II\nabsorbers with $R \\equiv$ $W_{\\rm 2600}/W_{\\rm 2796}$ $\\ge 0.5$. In fact, we\nshow the observed strong correlations of L$_{[\\rm O~II]}$ with $W_{2796}$ and\n$z$ of Mg II absorbers are mainly driven by such systems. Direct [O II]\ndetections also confirm the link between L$_{[\\rm O~II]}$ and $R$. Therefore,\none has to pay attention to the fibre losses and dependence of redshift\nevolution of Mg II absorbers on $W_{2600}$ before using them as a luminosity\nunbiased probe of global star formation rate density. We show that the [O II]\nnebular emission detected in the stacked spectrum is not dominated by few\ndirect detections (i.e., detections $\\ge 3 \\sigma$ significant level). On an\naverage the systems with $R$ $\\ge 0.5$ and $W_{2796}$ $\\ge 2$ \\AA\\ are more\nreddened, showing colour excess E($B-V$) $\\sim$ 0.02, with respect to the\nsystems with $R$ $< 0.5$ and most likely traces the high H I column density\nsystems."}, {"title": "On the relevance of chaos for halo stars in the solar neighbourhood II", "abstract": "In a previous paper based on dark matter only simulations we show that, in\nthe approximation of an analytic and static potential describing the strongly\ntriaxial and cuspy shape of Milky Way-sized haloes, diffusion due to chaotic\nmixing in the neighbourhood of the Sun does not efficiently erase phase space\nsignatures of past accretion events. In this second paper we further explore\nthe effect of chaotic mixing using multicomponent Galactic potential models and\nsolar neighbourhood-like volumes extracted from fully cosmological hydrodynamic\nsimulations, thus naturally accounting for the gravitational potential\nassociated with baryonic components, such as the bulge and disc. Despite the\nstrong change in the global Galactic potentials with respect to those obtained\nin dark matter only simulations, our results confirm that a large fraction of\nhalo particles evolving on chaotic orbits exhibit their chaotic behaviour after\nperiods of time significantly larger than a Hubble time. In addition,\nsignificant diffusion in phase space is not observed on those particles that do\nexhibit chaotic behaviour within a Hubble time."}, {"title": "Numerical Simulations of Multiphase Winds and Fountains from Star-Forming Galactic Disks: I. Solar Neighborhood TIGRESS Model", "abstract": "Gas blown away from galactic disks by supernova (SN) feedback plays a key\nrole in galaxy evolution. We investigate outflows utilizing the solar\nneighborhood model of our high-resolution, local galactic disk simulation\nsuite, TIGRESS. In our numerical implementation, star formation and SN feedback\nare self-consistently treated and well resolved in the multiphase, turbulent,\nmagnetized interstellar medium. Bursts of star formation produce spatially and\ntemporally correlated SNe that drive strong outflows, consisting of hot\n(T>5x10^5K) winds and warm (5050K < T < 2x10^4K) fountains. The hot gas at\ndistance d>1kpc from the midplane has mass and energy fluxes nearly constant\nwith d. The hot flow escapes our local Cartesian box barely affected by gravity\nand is expected to accelerate up to the terminal velocity of\nv_wind~350-500km/s. The mean mass and energy loading factors of the hot wind\nare 0.1 and 0.02, respectively. For warm gas, the mean outward mass flux\nthrough d=1kpc is comparable to the mean star formation rate, but only a small\nfraction of this gas is at velocity >50km/s. Thus, the warm outflows eventually\nfall back as inflows. The warm fountain flows are created by expanding hot\nsuperbubbles at d< 1kpc; at larger d neither ram pressure acceleration nor\ncooling transfers significant momentum or energy flux from the hot wind to the\nwarm outflow. The velocity distribution at launching near d~1kpc better\nrepresents warm outflows than a single mass loading factor, potentially\nenabling development of subgrid models for warm galactic winds in arbitrary\nlarge-scale galactic potentials."}, {"title": "Dynamics and Shocks from H$α$ Emission of Nearby Galaxy Mergers", "abstract": "We examine the dynamical properties of interacting galaxies and the\nproperties of shocked gas produced as a result of the interaction. We observed\n22 galaxy mergers using the SparsePak IFU at Kitt Peak National Observatory\n(KPNO). The goal of the observations was to obtain the \\ha\\ velocity maps over\nthe entire luminous parts of the galaxies including the faint tidal tails and\nto find extended shocks and outflows. Our sample consists of major and minor\ngalaxy mergers with mass ratios $1<\\mu<8$. We fit multiple kinematic components\nto the \\ha\\ and \\nii\\ emission lines, develop an MCMC code to robustly estimate\nthe error of fit parameters, and use the F-test to determine the best number of\nkinematic components for each fiber. We use \\nii/\\ha\\ and velocity dispersion\nof components to separate star-forming (HII) regions from shocks. We use the\nkinematics of the \\ha\\ emission from HII regions and an automated modeling\nmethod to put the first-ever constraints on the encounter parameters of one of\nthe observed systems. Besides, we roughly estimate the fraction of shocked \\ha\\\nemission , $\\text{f}_\\text{shocked}$, without taking extinction into account\nand examine the spatial distribution of shocks. We find that close galaxy pairs\nhave, on average, a higher shock fraction than wide pairs, and coalesced\nmergers have the highest average $\\text{f}_\\text{shocked}$. In addition, galaxy\npairs with more equal mass ratio tend to have a higher\n$\\text{f}_\\text{shocked}$. Combining the dynamical models from the literature\nand this work, we inspect trends between $\\text{f}_\\text{shocked}$ and\ndynamical encounter parameters. Our findings are generally consistent with\nshocks being produced either by the direct collision of the ISM or by the chain\nof events provoked by the tidal impulse during the first passage."}, {"title": "Gaia 1 cannot be a Thick Disk Galactic cluster", "abstract": "In this note I show how the recently suggested membership of the open cluster\nGaia 1 to the Galactic thick disk is based on incorrect assumptions about the\nstructure of the disk itself, and neglect well-known observational evidences on\nthe disk warp and flare."}, {"title": "Molecular Gas Properties in M83 from CO PDFs", "abstract": "We have obtained 12CO(1--0) data of the nearby barred spiral galaxy M83 from\nAtacama Large Millimeter/submillimeter Array and Nobeyama 45m observations. By\ncombining these two data sets, the total CO flux has been recovered, and a high\nangular resolution (2\" corresponding to ~40 pc at the distance of M83) has been\nachieved. The field of view is 3' corresponding to ~3.4 kpc and covers the\ngalactic center, bar, and spiral arm regions. In order to investigate how these\ngalactic structures affect gas properties, we have created a probability\ndistribution function (PDF) of the CO integrated intensity (I_CO), peak\ntemperature, and velocity dispersion for a region with each structure. We find\nthat the I_CO PDF for the bar shows a bright-end tail while that for the arm\ndoes not. Since the star formation efficiency is lower in the bar, this\ndifference in PDF shape is contrary to the trend in Milky Way studies where the\nbright-end tail is found for star-forming molecular clouds. While the peak\ntemperature PDFs are similar for bar and arm regions, velocity dispersion in\nbar is systematically larger than in arm. This large velocity dispersion is\nlikely a major cause of the bright-end tail and of suppressed star formation.\nWe also investigate an effect of stellar feedback to PDF profiles and find that\nthe different I_CO PDFs between bar and arm regions cannot be explained by the\nfeedback effect, at least at the current spatial scale."}, {"title": "Spectroscopic Observations of the Outflowing Wind in the Lensed Quasar SDSS J1001+5027", "abstract": "We performed spectroscopic observations of the small-separation lensed quasar\nSDSS J1001+5027, whose images have an angular separation $\\theta \\sim\n2.^{\\!\\!\\prime\\prime}86$, and placed constraints on the physical properties of\ngas clouds in the vicinity of the quasar (i.e., in the outflowing wind launched\nfrom the accretion disk). The two cylinders of sight to the two lensed images\ngo through the same region of the outflowing wind and they become fully\nseparated with no overlap at a very large distance from the source ($\\sim 330$\npc). We discovered a clear difference in the profile of the CIV broad\nabsorption line (BAL) detected in the two lensed images in two observing\nepochs. Because the kinematic components in the BAL profile do not vary in\nconcert, the observed variations cannot be reproduced by a simple change of\nionization state. If the variability is due to gas motion around the background\nsource (i.e., the continuum source), the corresponding rotational velocity is\n$v_{rot}\\geq 18,000$ km/s, and their distance from the source is $r\\leq 0.06$\npc assuming Keplerian motion. Among three MgII and three CIV NAL systems that\nwe detected in the spectra, only the MgII system at $z_{abs} = 0.8716$ shows a\nhint of variability in its MgI profile on a rest-frame time scale of $\\Delta\nt_{rest}$ $\\leq 191$ days and an obvious velocity shear between the sightlines\nwhose physical separation is $\\sim 7$ kpc. We interpret this as the result of\nmotion of a cosmologically intervening absorber, perhaps located in a\nforeground galaxy."}, {"title": "Properties and rotation of molecular clouds in M 33", "abstract": "The sample of 566 molecular clouds identified in the CO(2--1) IRAM survey\ncovering the disk of M~33 is explored in detail.The clouds were found using\nCPROPS and were subsequently catalogued in terms of their star-forming\nproperties as non-star-forming (A), with embedded star formation (B), or with\nexposed star formation C.We find that the size-linewidth relation among the\nM~33 clouds is quite weak but, when comparing with clouds in other nearby\ngalaxies, the linewidth scales with average metallicity.The linewidth and\nparticularly the line brightness decrease with galactocentric distance.The\nlarge number of clouds makes it possible to calculate well-sampled cloud mass\nspectra and mass spectra of subsamples.As noted earlier, but considerably\nbetter defined here, the mass spectrum steepens (i.e. higher fraction of small\nclouds) with galactocentric distance.A new finding is that the mass spectrum of\nA clouds is much steeper than that of the star-forming clouds.Further dividing\nthe sample, this difference is strong at both large and small galactocentric\ndistances and the A vs C difference is a stronger effect than the inner/outer\ndisk difference in mass spectra.Velocity gradients are identified in the clouds\nusing standard techniques.The gradients are weak and are dominated by prograde\nrotation; the effect is stronger for the high signal-to-noise clouds.A\ndiscussion of the uncertainties is presented.The angular momenta are low but\ncompatible with at least some simulations.The cloud and galactic gradients are\nsimilar; the cloud rotation periods are much longer than cloud lifetimes and\ncomparable to the galactic rotation period.The rotational kinetic energy is\n1-2\\% of the gravitational potential energy and the cloud edge velocity is well\nbelow the escape velocity, such that cloud-scale rotation probably has little\ninfluence on the evolution of molecular clouds."}, {"title": "Detection of the Aromatic Molecule Benzonitrile ($c$-C$_6$H$_5$CN) in the Interstellar Medium", "abstract": "Polycyclic aromatic hydrocarbons and polycyclic aromatic nitrogen\nheterocycles are thought to be widespread throughout the Universe, because\nthese classes of molecules are probably responsible for the unidentified\ninfrared bands, a set of emission features seen in numerous Galactic and\nextragalactic sources. Despite their expected ubiquity, astronomical\nidentification of specific aromatic molecules has proven elusive. We present\nthe discovery of benzonitrile ($c$-C$_6$H$_5$CN), one of the simplest\nnitrogen-bearing aromatic molecules, in the interstellar medium. We observed\nhyperfine-resolved transitions of benzonitrile in emission from the molecular\ncloud TMC-1. Simple aromatic molecules such as benzonitrile may be precursors\nfor polycyclic aromatic hydrocarbon formation, providing a chemical link to the\ncarriers of the unidentified infrared bands."}, {"title": "Feedback from Reorienting AGN Jets", "abstract": "[abridged] Aims: We test the effects of re-orienting jets from an active\ngalactic nucleus (AGN) on the intracluster medium in a galaxy cluster\nenvironment with short central cooling time. We investigate appearance and\nproperties of the resulting cavities, and the efficiency of jets in providing\nnear-isotropic heating to the cooling cluster core.\n  Methods: We use numerical simulations to explore four models of jets over\nseveral active/inactive cycles. We keep the jet power and duration fixed,\nvarying only the jet angle prescription. We track the total energy of the\nintracluster medium (ICM) in the cluster core over time, and the fraction of\nthe jet energy transferred to the ICM, paying attention to where the energy is\ndeposited. We also compare synthetic X-ray images of the simulated cluster to\nactual observations.\n  Results: Jets whose re-orientation is minimal ($\\lesssim 20^{\\circ}$)\ntypically produce conical structures of interconnected cavities, with the\nopening angle of the cones being $\\sim 15-20^{\\circ}$, extending to $\\sim 300$\nkpc from the cluster centre. Such jets transfer about $60\\%$ of their energy to\nthe ICM, yet they are not very efficient at heating the cluster core, as the\njet energy is deposited further out. Jets that re-orient by $\\gtrsim\n20^{\\circ}$ generally produce multiple pairs of detached cavities. Although\nsmaller, these cavities are inflated within the central 50~kpc and are more\nisotropically distributed, resulting in more effective heating of the core.\nSuch jets, over few hundreds Myr, can deposit up to $80\\%$ of their energy\nwhere it is required. Consequently, these models come the closest to an\nheating/cooling balance and to mitigating runaway cooling of the core, even\nthough all models have identical power/duration profiles. Additionally, the\ncorresponding synthetic X-ray images exhibit structures closely resembling\nthose seen in real cool-core clusters."}, {"title": "Efficient Switching of 3-Terminal Magnetic Tunnel Junctions by the Giant Spin Hall Effect of $\\rm{Pt}_{85}\\rm{Hf}_{15}$ Alloy", "abstract": "Recent research has indicated that introducing impurities that increase the\nresistivity of Pt can enhance the efficiency of the spin Hall torque it\ngenerates. Here we directly demonstrate the usefulness of this strategy by\nfabricating prototype 3-terminal in-plane-magnetized magnetic tunnel junctions\nthat utilize the spin Hall torque from a $\\rm{Pt}_{85}\\rm{Hf}_{15}$ alloy, and\nmeasuring the critical currents for switching. We find that\n$\\rm{Pt}_{85}\\rm{Hf}_{15}$ reduces the switching current densities compared to\npure Pt by approximately a factor of 2 for both quasi-static ramped current\nbiases and nanosecond-scale current pulses, thereby proving the feasibility of\nthis approach to assist in the development of efficient embedded magnetic\nmemory technologies."}, {"title": "Phase diagram of the B-BN system at pressures up to 24 GPa: Experimental study and thermodynamic analysis", "abstract": "Phase relations in the B-BN system have been studied ex situ and in situ at\npressures 2-20 GPa and temperatures up to 2800 K. The evolution of topology of\nthe B-BN phase diagram has been investigated up to 24 GPa using models of\nphenomenological thermodynamics with interaction parameters derived from our\nexperimental data on phase equilibria at high pressures and high temperatures.\nThere are two thermodynamically stable boron subnitrides in the system i.e.\nB13N2 and B50N2. Above 16.5 GPa the B50N2 = L + B13N2 peritectic reaction\ntransforms to the solid-phase reaction of B50N2 decomposition into tetragonal\nboron (t'-B52) and B13N2, while the incongruent type of B13N2 melting changes\nto the congruent type only above 23.5 GPa. The constructed phase diagram\nprovides fundamentals for directed high-pressure synthesis of superhard phases\nin the B-BN system."}, {"title": "Structural Disorder and Electronic Structure in Alloyed SrTiO3/SrFeO2.5 Compounds: A Theoretical Study", "abstract": "Many mixed ionic/electronic conductors (MIECs) applied in fuel cell\nelectrodes can be considered as alloys between perovskite oxides and ordered\noxygen vacancy compounds. For example, in the model MIEC (STF), low oxygen\ndiffusion barrier exist in SrTiO3 lattice, when it has been mixed with SrFeO2.5\nwith intrinsic oxygen deficiency, the ionic conductivity can be greatly\nimproved. Meanwhile, the electronic conductivity can be optimized by\ncontrolling the defect chemistry of the alloy. However, the configurational\nspace is too large in such alloys so that it is difficult for direct atomic\nmodeling, which hinders in-depth understanding and predictive modeling. In this\nwork, we present a cluster expansion model to describe the energetics of the\ndisordered SrTiO3/SrFeO2.5 alloy within the full solid solution composition\nspace Sr(Ti1-x,Fex)O3-0.5x (0<x<1). Cluster expansion Monte Carlo simulations\nhave been performed to search the lowest energy atomic configurations and\ninvestigate the origin of lattice disorder. With representations of realistic\nconfigurations, the electronic structures of such alloys at different\nstoichiometry have also been examined. We find that the band gap evolution with\ncomposition calculated using our atomic model is consistent with experiment\nmeasurement. Meanwhile, the band edge analysis elucidate that electronic\nconductivity within such alloy can be facilitated by the Fe/Ti cation disorder.\nTaking SrTiO3/SrFeO2.5 alloy as an example, the generalized computational\nframework applied here can be extended to other relevant MIEC material systems."}, {"title": "Spectroscopic study of native defects in the semiconductor to metal phase transition in V2O5 nanostructure", "abstract": "Vanadium is a transition metal with multiple oxidation states and V2O5 is the\nmost stable form among them. Besides catalysis, chemical sensing and\nphoto-chromatic applications, V2O5 is also reported to exhibit a semiconductor\nto metal transition (SMT) at a temperature range of 530-560K. Even though,\nthere are debates in using the term 'SMT' for V2O5, the metallic behavior above\ntransition temperature and its origin are of great interests in the scientific\ncommunity. In this study, V2O5 nanostructures were deposited on SiO2/Si\nsubstrate by vapour transport method using Au as catalyst. Temperature\ndependent electrical measurement confirms the SMT in V2O5 without any\nstructural change. Temperature dependent photoluminescence analysis proves the\nappearance of oxygen vacancy related peaks due to reduction of V2O5 above the\ntransition temperature, as also inferred from temperature dependent Raman\nspectroscopic studies. The newly evolved defect levels in the V2O5 electronic\nstructure with increasing temperature is also understood from the downward\nshift of the bottom most split-off conduction bands due to breakdown of pd{\\pi}\nbonds leading to metallic behavior in V2O5 above the transition temperature."}, {"title": "The graphene/n-Ge(110) interface: structure, doping, and electronic properties", "abstract": "The implementation of graphene in semiconducting technology requires the\nprecise knowledge about the graphene-semiconductor interface. In our work the\nstructure and electronic properties of the graphene/$n$-Ge(110) interface are\ninvestigated on the local (nm) and macro (from $\\mu\\mathrm{m}$ to mm) scales\nvia a combination of different microscopic and spectroscopic surface science\ntechniques accompanied by density functional theory calculations. The\nelectronic structure of freestanding graphene remains almost completely intact\nin this system, with only a moderate $n$-doping indicating weak interaction\nbetween graphene and the Ge substrate. With regard to the optimization of\ngraphene growth it is found that the substrate temperature is a crucial factor,\nwhich determines the graphene layer alignment on the Ge(110) substrate during\nits growth from the atomic carbon source. Moreover, our results demonstrate\nthat the preparation routine for graphene on the doped semiconducting material\n($n$-Ge) leads to the effective segregation of dopants at the interface between\ngraphene and Ge(110). Furthermore, it is shown that these dopant atoms might\nform regular structures at the graphene/Ge interface and induce the doping of\ngraphene. Our findings help to understand the interface properties of the\ngraphene-semiconductor interfaces and the effect of dopants on the electronic\nstructure of graphene in such systems."}, {"title": "Scaling behavior of dynamic hysteresis in Na0.5Bi4.5Ti4O15 bulk ceramics", "abstract": "The ferroelectric hysteresis loops of sodium bismuth titanate\nNa0.5Bi4.5Ti4O15 bulk ceramics were measured under periodical electric field in\nrange of frequency from 0.01Hz to 100Hz and field from 10kV/cm to 150kV/cm. The\nthree-stage scaling behavior of dynamic hysteresis was investigated in\nNa0.5Bi4.5Ti4O15 bulk ceramics. The scaling behavior at low amplitude of\nelectric field is described as <A> is proportional to f^(-0.122)E0^3.30 for low\nfrequency and <A> is proportional to f^(-0.122)E0^3.15 for high frequency. <A>,\nf and E0 represent the area of hysteresis loop, frequency and amplitude of\nperiodic electric field, respectively. At E0 around coercive field, scaling\nbehavior takes the form of <A> is proportional to f^(-0.11)E0^4.28 for low\nfrequency and <A> is proportional to f^(-0.11)E0^4.17 for high frequency. At\nhigh E0, we obtained <A> is proportional to f^(-0.04)E0^2.90 for low frequency\nand <A> is proportional to f^(-0.06)E0^2.75 for high frequency. The\ncontribution to scaling relation mainly results from reversible of\nferroelectric domain switching at low E0, the velocity of domain wall motion at\nE0 around coercive field and simultaneously reversible and irreversible domain\nswitching at high E0."}, {"title": "Strain Tunable Phononic Topological Bandgaps in Two-Dimensional Hexagonal Boron Nitride", "abstract": "The field of topological mechanics has recently emerged due to the interest\nin robustly transporting various types of energy in a flaw and\ndefect-insensitive fashion. While there have been a significant number of\nstudies based on discovering and proposing topological materials and\nstructures, very few have focused on tuning the resulting topological bandgaps,\nwhich is critical because the bandgap frequency is fixed once the structure has\nbeen fabricated. Here, we perform both lattice dynamical calculations and\nmolecular dynamical simulations to investigate strain effects on the phononic\ntopological bandgaps in two-dimensional monolayer hexagonal boron nitride. Our\nstudies demonstrate that while the topologically protected phononic bandgaps\nare not closed even for severely deformed hexagonal boron nitride, and are\nrelatively insensitive to uniaxial tension and shear strains, the position of\nthe frequency gap can be efficiently tuned in a wide range through the\napplication of biaxial strains. Overall, this work thus demonstrates that\ntopological phonons are robust against the effects of mechanical strain\nengineering, and sheds light on the tunability of the topological bandgaps in\nnanomaterials."}, {"title": "Phonon driven Floquet matter", "abstract": "A resonantly excited coherent phonon leads to a periodic oscillation of the\natomic lattice in a crystal structure bringing the material into a\nnon-equilibrium electronic configuration. Periodically oscillating quantum\nsystems can be understood in terms of Floquet theory and we show these concepts\ncan be applied to coherent lattice vibrations reflecting the underlying\ncoupling mechanism between electrons and bosonic modes. This coupling leads to\ndressed quasi-particles imprinting specific signatures in the spectrum of the\nelectronic structure. Taking graphene as a paradigmatic material we show how\nthe phonon-dressed states display an intricate sideband structure revealing\nelectron-phonon coupling and topological ordering. This work establishes that\nthe recently demonstrated concept of light-induced non-equilibrium Floquet\nphases can also be applied when using coherent phonon modes for the dynamical\ncontrol of material properties. The present results are generic for bosonic\ntime-dependent perturbations and similar phenomena can be observed for plasmon,\nmagnon or exciton driven materials."}, {"title": "Multiple Quintets via Singlet Fission in Ordered Films at Room Temperature", "abstract": "The growing interest in harnessing singlet fission for photovoltaic\napplications stems from the possibility of generating two excitons from a\nsingle photon. Quantum efficiencies above unity have been reported, yet the\ncorrelation between singlet fission and intermolecular geometry is poorly\nunderstood. To address this, we investigated ordered solid solutions of\npentacene in p-terphenyl grown by organic molecular beam deposition. Two\nclasses of dimers are expected from the crystal structure - parallel and\nherringbone - with intrinsically distinctive electronic coupling. Using\nelectron paramagnetic resonance spectroscopy, we provide compelling evidence\nfor the formation of distinct quintet excitons at room temperature. These are\nassigned to specific pentacene pairs according to their angular dependence.\nThis work highlights the importance of controlling the intermolecular geometry\nand the need to develop adequate theoretical models to account for the\nrelationship between structure and electronic interactions in strongly-coupled,\nhigh-spin molecular systems."}, {"title": "Redox agent enhanced chemical mechanical polishing of thin film diamond", "abstract": "The chemical nature of the chemical mechanical polishing of diamond has been\nexamined by adding various redox agents to the alkaline SF1 polishing slurry.\nThree oxidizing agents namely, hydrogen peroxide, potassium permanganate and\nferric nitrate, and two reducing agents, oxalic acid and sodium thiosulfate,\nwere added to the SF1 slurry. Oxalic acid produced the fastest polishing rate\nwhile hydrogen peroxide had very little effect on polishing, probably due to\nits volatile nature. X-ray photoelectron spectroscopy (XPS) reveals little\ndifference in the surface oxygen content on the polished samples using various\nslurries. This suggests that the addition of redox agents do not increase the\ndensity of oxygen containing species on the surface but accelerates the process\nof attachment and removal of Si or O atoms within the slurry particles to the\ndiamond surface."}, {"title": "Ultrastrong adhesion in the contact with thin elastic layers on a rigid foundation", "abstract": "In the present short note, we generalize simple approximate\nJohnson-Jaffar-Barber solutions for the indentation by a rigid punch of a thin\nelastic layer on a rigid foundation to the case of adhesion. This could be an\ninteresting geometry for an adhesive system, a limit case of the more general\nclass of layered systems, or FGMs (Functionally Graded Materials). We show that\nultrastrong adhesion (up to theoretical strength) can be reached both in line\ncontact or in axisymmetric contact for thin layers (typically of nanoscale\nsize), which suggests a new possible strategy for \"optimal adhesion\". In\nparticular, in line contact adhesion enhancement occurs as an increase of the\nactual pull-off force, while in axisymmetric case the latter is apparently very\nclose to the classical JKR case. However, it appears in closer examination that\nalso for axisymmetric case, the enhancement occurs by reducing the size of\ncontact needed to sustain the pull-off force. These effects are further\nenhanced by Poisson's ratio effects in the case of nearly incompressible layer."}, {"title": "The role of lattice mismatch on the emergence of surface states in 2D hybrid perovskite quantum wells", "abstract": "Surface states are ubiquitous to semiconductors and significantly impact the\nphysical properties and consequently the performance of optoelectronic devices.\nMoreover, surface effects are strongly amplified in lower dimensional systems\nsuch as quantum wells and nanostructures. Layered halide perovskites (LHPs) are\n2D solution-processed natural quantum wells, where optoelectronic properties\ncan be tuned by varying the perovskite layer thickness. They are efficient\nsemiconductors with technologically relevant stability. Here, a generic elastic\nmodel and electronic structure modelling are applied to LHPs heterostructures\nwith various layer thickness. We show that the relaxation of the interface\nstrain is triggered by perovskite layers above a critical thickness. This leads\nto the release of the mechanical energy arising from the lattice mismatch,\nwhich nucleates the surface reorganization and consequently the formation of\nlower energy edge states. These states, which are absent in 3D perovskites,\ndominate the optoelectronic properties of LHPs and are anticipated to play a\ncrucial role in the design of LHPs for optoelectronics devices."}, {"title": "Some further validations and comparison of the Bearing Area Model (BAM) for adhesion of rough surfaces", "abstract": "In the present short note, we attempt further validations and comparisons of\na recent simple model for the estimate for adhesion between elastic (hard)\nrough solids with Gaussian multiple scales of roughness, BAM (Bearing Area\nModel) belonging to a DMT class of models. In one case, we use the GJP\n(Generalized Johnson Parameter) model, which is an empirical fit validated on\nthe same (and so far most extensive) set of data on which BAM was validated,\nnamely that of Pastewka and Robbins. In the second case, we compare with\nanother approximate DMT theory, that of Persson and Scaraggi, which turns out\nextremely close to the BAM model, despite much more complex: GJP however can\nlead to significant discrepancies."}, {"title": "EXAFS and electrical studies of new narrow-gap semiconductors: InTe$_{1-x}$Se$_x$ and In$_{1-x}$Ga$_x$Te", "abstract": "The local environment of Ga, Se, and Tl atoms in InTe-based solid solutions\nwas studied by EXAFS technique. It was shown that all investigated atoms are\nsubstitutional impurities, which enter the In(1), Te, and In(2) positions in\nthe InTe structure, respectively. The electrical measurements revealed that\nIn$_{1-x}$Ga$_x$Te and InTe$_{1-x}$Se$_x$ solid solutions become semiconductors\nat $x>0.24$ and $x>0.15$, respectively."}, {"title": "Edge-Grafted Molecular Junctions between Graphene Nanoplatelets: Applied Chemistry to Enhance Heat Transfer in Nanomaterials", "abstract": "The edge-functionalization of graphene nanoplatelets (GnP) was carried out\nexploiting diazonium chemistry, aiming at the synthesis of edge decorated\nnanoparticles to be used as building blocks in the preparation of engineered\nnanostructured materials for enhanced heat transfer. Indeed, both phenol\nfunctionalized and dianiline-bridged GnP (GnP-OH and E-GnP, respectively) were\nassembled in nanopapers exploiting the formation of non-covalent and covalent\nmolecular junctions, respectively. Molecular dynamics allowed to estimate the\nthermal conductance for the two different types of molecular junction,\nsuggesting a factor 6 between conductance of covalent vs. non-covalent\njunctions. Furthermore, the chemical functionalization was observed to drive\nthe self-organization of the nanoflakes into the nanopapers, leading to a 20%\nenhancement of the thermal conductivity for GnP-OH and E-GnP while the cross\nplane thermal conductivity was boosted by 150% in the case of E-GnP. The\napplication of chemical functionalization to the engineering of contact\nresistance in nanoparticles network was therefore validated as a fascinating\nroute for the enhancement of heat exchange efficiency on nanoparticle networks,\nwith great potential impact in low-temperature heat exchange and recovery\napplications"}, {"title": "Origin and Control of Polyacrylonitrile Alignments on Carbon Nanotube and Graphene Nanoribbon", "abstract": "While one of the most promising applications of carbon nanotubes (CNTs) is to\nenhance polymer orientation and crystallization to achieve advanced carbon\nfibers, the successful realization of this goal has been hindered by the\ninsufficient atomistic understanding of polymer-CNT interfaces. We herein\ntheoretically study polyacrylonitrile (PAN)-CNT hybrid structures as a\nrepresentative example of polymer-CNT composites. Based on density-functional\ntheory calculations, we first find that the relative orientation of polar PAN\nnitrile groups with respect to the CNT surface is the key factor that\ndetermines the PAN-CNT interface energetics and the lying-down PAN\nconfigurations are much more preferable than their standing-up counterparts.\nThe CNT curvature is identified as another important factor, giving the largest\nbinding energy in the zero-curvature graphene limit. Charge transfer analysis\nexplains the unique tendency of linear PAN alignments on the CNT surface and\nthe possibility of ordered PAN-PAN assembly. Next, performing large-scale\nmolecular dynamics simulations, we show that the desirable linear PAN-CNT\nalignment can be achieved even for relatively large initial misorientations and\nfurther demonstrate that graphene nanoribbons are a promising carbon\nnano-reinforcement candidate. The microscopic understanding accumulated in this\nstudy will provide design guidelines for the development of next-generation\ncarbon nanofibers."}, {"title": "Opposing effects of stacking faults and antisite domain boundaries on the conduction band edge in kesterite quaternary semiconductors", "abstract": "We investigated stability and the electronic structure of extended defects\nincluding anti-site domain boundaries and stacking faults in the\nkesterite-structured semiconductors, Cu$_2$ZnSnS$_4$ (CZTS) and\nCu$_2$ZnSnSe$_4$ (CZTSe). Our hybrid density functional theory calculations\nshow that stacking faults in CZTS and CZTSe induce a higher conduction band\nedge than the bulk counterparts, and thus the stacking faults act as electron\nbarriers. Antisite domain boundaries, however, accumulate electrons as the\nconduction band edge is reduced in energy, having an opposite role. An Ising\nmodel was constructed to account for the stability of stacking faults, which\nshows the nearest neighbour interaction is stronger in the case of the\nselenide."}, {"title": "Real space pairwise electrostatic summation in a uniform neutralising background", "abstract": "Evaluating the total energy of an extended distribution of point charges,\nwhich interact through the Coulomb potential, is central to the study of\ncondensed matter. With near ubiquity, the summation required is carried out\nusing Ewald's method, which splits the problem into two separately convergent\nsums; one in real space and the other in reciprocal space. Density functional\nbased electronic structure methods require the evaluation of the ion-ion\nrepulsive energy, neutralised by a uniform background charge. Here a purely\nreal-space approach is described. It is straightforward to implement,\ncomputationally efficient and offers linear scaling. When applied to the\nevaluation of the electrostatic energy of neutral ionic crystals, it is shown\nto be closely related to Wolf's method."}, {"title": "Dimensionality-driven orthorhombic MoTe2 at room temperature", "abstract": "We use a combination of Raman spectroscopy and transport measurements to\nstudy thin flakes of the type-II Weyl semimetal candidate MoTe2 protected from\noxidation. In contrast to bulk crystals, which undergo a phase transition from\nmonoclinic to the inversion symmetry breaking, orthorhombic phase below ~250 K,\nwe find that in moderately thin samples below ~12 nm, a single orthorhombic\nphase exists up to and beyond room temperature. This could be due to the effect\nof c-axis confinement, which lowers the energy of an out-of-plane hole band and\nstabilizes the orthorhombic structure. Our results suggest that Weyl nodes,\npredicated upon inversion symmetry breaking, may be observed in thin MoTe2 at\nroom temperature."}, {"title": "Quantum oscillations and nontrivial transport in (Bi0.92In0.08)2Se3", "abstract": "Quantum phase transition in topological insulators has drawn heightened\nattention in condensed matter physics and future device applications. Here we\nreport the magnetotransport properties of single crystalline\n(Bi0.92In0.08)2Se3. The average mobility of about 1000 cm2/Vs is obtained from\nthe Lorentz law at the low field up to 50 K. The quantum oscillations rise at a\nfield of about 5 T, revealing a high mobility of 1.4*10^4 cm2/Vs at 2 K. The\ntopological Dirac fermions are evident by the nontrivial Berry phase in the\nLandau Fan diagram. The properties make the (Bi0.92In0.08)2Se3 a promising\nplatform for the investigation of quantum phase transition in topological\ninsulators."}, {"title": "Tunable magnetic and transport properties of Mn3Ga thin films on Ta/Ru seedlayer", "abstract": "Hexagonal D019-type Mn3Z alloys that possess large anomalous and\ntopological-like Hall effects have attracted much attention due to their great\npotential in the antiferromagnetic spintronic devices. Here, we report the\npreparation of Mn3Ga film in both tetragonal and hexagonal phases with a tuned\nTa/Ru seed layer on the thermally oxidized Si substrate. A large coercivity\ntogether with a large anomalous Hall resistivity is found in the Ta-only sample\nwith mixed tetragonal phase. By increasing the thickness of Ru layer, the\ntetragonal phase gradually disappears and a relatively pure hexagonal phase is\nobtained in the Ta(5)/Ru(30) buffered sample. Further magnetic and transport\nmeasurements revealed that the anomalous Hall conductivity nearly vanishes in\nthe pure hexagonal sample, while an abnormal asymmetric hump structure emerges\nin the low field region. The extracted additional Hall term is robust in a\nlarge temperature range and presents a sign reversal above 200K. The abnormal\nHall properties are proposed to be closely related with the frustrated spin\nstructure of D019 Mn3Ga."}, {"title": "Phonon transport of Janus monolayer MoSSe: a first-principles study", "abstract": "Transition Metal Dichalcogenide (TMD) monolayers have most widely studied due\nto their unique physical properties. Recently, Janus TMD Monolayer MoSSe with\nsandwiched S-Mo-Se structure has been synthesized by replacing the top S atomic\nlayer in $\\mathrm{MoS_2}$ with Se atoms. In this work, we systematically\ninvestigate the phonon transport and lattice thermal conductivity ($\\kappa_L$)\nof MoSSe monolayer by first-principles calculations and linearized phonon\nBoltzmann equation within the single-mode relaxation time approximation (RTA).\nCalculated results show that the $\\kappa_L$ of MoSSe monolayer is very lower\nthan that of $\\mathrm{MoS_2}$ monolayer, and higher than that of\n$\\mathrm{MoSe_2}$ monolayer. The corresponding sheet thermal conductance of\nMoSSe monolayer is 342.50 $\\mathrm{W K^{-1}}$ at room temperature. These can be\nunderstood by phonon group velocities and lifetimes. Compared with\n$\\mathrm{MoS_2}$ monolayer, the smaller group velocities and shorter phonon\nlifetimes of MoSSe monolayer give rise to lower $\\kappa_L$. The larger group\nvelocities for MoSSe than $\\mathrm{MoSe_2}$ monolayer is main reason of higher\n$\\kappa_L$. The elastic properties of $\\mathrm{MoS_2}$, MoSSe and\n$\\mathrm{MoSe_2}$ monolayers are also calculated, and the order of Young's\nmodulus is identical with that of $\\kappa_L$. Calculated results show that\nisotope scattering leads to 5.8\\% reduce of $\\kappa_L$. The size effects on the\n$\\kappa_L$ are also considered, which is usually used in the device\nimplementation. When the characteristic length of MoSSe monolayer is about 110\nnm, the $\\kappa_L$ reduces to half. These results may offer perspectives on\nthermal management of MoSSe monolayer for applications of thermoelectrics,\nthermal circuits and nanoelectronics, and motivate further theoretical or\nexperimental efforts to investigate thermal transports of Janus TMD monolayers."}, {"title": "Reconstruction-Stabilized Epitaxy of LaCoO3/SrTiO3(111) Heterostructures by Pulsed Laser Deposition", "abstract": "Unlike widely explored complex oxide heterostructures grown along [001], the\nstudy of [111]-oriented heterointerfaces are very limited thus far. One of the\nmain challenges is to overcome the polar discontinuity that hinders the epitaxy\nof atomically sharp interfaces. Here, by taking the LaCoO3/SrTiO3(111) as a\nprototype, we show that the reconstruction, which effectively compensates the\nsurface polarity, can stabilize the epitaxy of the heterostructure with polar\ndiscontinuity. Reconstructed substrate surface is prepared, while the growth is\ncontrolled to form reconstruction on the film surface. To suppress the chemical\ndiffusion across the interface, the growth is interrupted between each unit\ncell layer to allow the lattice relaxation at a lowered temperature. In this\nway, high quality two-dimensional growth is realized and the heterointerfaces\nexhibit sharpness at the atomic scale. Our work provides a path to precisely\ncontrol the growth of complex oxide heterostructures along polar orientations\nthat exhibit emergent quantum phenomena."}, {"title": "Photocorrosion-limited maximum efficiency of solar photoelectrochemical water splitting", "abstract": "Photoelectrochemical (PEC) water splitting to generate hydrogen is one of the\nmost studied methods for converting solar energy into clean fuel because of its\nsimplicity and potentially low cost. Despite over 40 years of intensive\nresearch, PEC water splitting remains in its early stages with stable\nefficiencies far less than 10%, a benchmark for commercial applications. Here,\nwe revealed that the desired photocorrosion stability sets a limit of 2.48 eV\n(relative to the normal hydrogen electrode (NHE)) for the highest possible\npotential of the valence band (VB) edge of a photocorrosion-resistant\nsemiconducting photocatalyst. We further demonstrated that such limitation has\na deep root in underlying physics after deducing the relation between energy\nposition of the valence band edge and free-energy for a semiconductor. The\ndisparity between the stability-limited VB potential at 2.48 V and the oxygen\nevolution reaction (OER) potential at 1.23 V vs NHE reduces the maximum STH\nconversion efficiency to approximately 8% for long-term stable single-bandgap\nPEC water splitting cells. Based on this understanding, we suggest that the\nmost promising strategy to overcome this 8% efficiency limit is to decouple the\nrequirements of efficient light harvesting and chemical stability by protecting\nthe active semiconductor photocatalyst surface with a photocorrosion-resistant\noxide coating layer."}, {"title": "Intercalation of hydrogen in the SiC/epitaxial graphene interface", "abstract": "We have measured optical absorption in mid-infrared spectral range on\nhydrogen intercalated epitaxial graphene grown on silicon face of SiC. We have\nused attenuated total reflection geometry to enhance absorption related to the\nsurface and SiC/graphene interface. The samples of epitaxial graphene have been\nintercalated in the temperature range of 790 to 1250$^\\circ$C and compared to\nthe reference samples of hydrogen etched SiC. We have found that although the\nSi-H bonds form at as low temperatures as 790$^\\circ$C, the well developed bond\norder has been reached only for epitaxial graphene intercalated at temperatures\nexceeding 1000$^\\circ$C. We also show that the hydrogen intercalation\ndegradates on a time scale of few days when samples are stored in ambient air.\nThe optical spectroscopy shows on a formation of vinyl and silyl functional\ngroups on the SiC/graphene interface due to the residual atomic hydrogen left\nfrom the intercalation process."}, {"title": "Dislocations and cracks in generalized continua", "abstract": "Dislocations play a key role in the understanding of many phenomena in solid\nstate physics, materials science, crystallography and engineering. Dislocations\nare line defects producing distortions and self-stresses in an otherwise\nperfect crystal lattice. In particular, dislocations are the primary carrier of\ncrystal plasticity and in dislocation based fracture mechanics."}, {"title": "Effect of in-situ electric field assisted growth on anti-phase boundaries in epitaxial Fe3O4 thin films on MgO", "abstract": "Anti-phase boundaries (APBs) normally form as a consequence of the initial\ngrowth conditions in all spinel ferrite thin films. The presence of APBs in\nepitaxial films of the inverse spinel Fe3O4 alters their electronic and\nmagnetic properties due to strong antiferromagnetic (AF) interactions across\nthese boundaries. The effect of using in-situ electric field assisted growth on\nthe migration of APBs in hetero epitaxial Fe3O4(100)/MgO(100) thin films have\nbeen explored in the present work. The electric field assisted growth is found\nto reduce the AF interactions across APBs and as a consequence APBs free thin\nfilm like properties are obtained, which have been probed by electronic,\nmagnetic and structural characterization. An increase in energy associated with\nthe nucleation and/or early stage of the growth and, therefore, a corresponding\nincrease in surface mobility of the ad-atoms play a critical role in\ncontrolling the density of APBs. This innovative technique can be employed to\ngrow epitaxial spinel thin films with controlled AF interactions across APBs."}, {"title": "Diverse Electronic and Magnetic Properties of Chlorination-Related Graphene Nanoribbons", "abstract": "The dramatic changes in electronic and magnetic properties are investigated\nusing the first-principles calculations for (Cl, Br, I, At)-adsorbed graphene\nnanoribbons. The rich and unique features are clearly revealed in the\nadatom-dominated band structures, p-type doping, spin arrangement/magnetic\nmoment, spatial charge distribution, and orbital- and spin-projected density of\nstates. Halogen adsorptions can create the non-magnetic, ferromagnetic or\nanti-ferromagnetic metals, being mainly determined by concentrations and edge\nstructures. The number of holes per unit cell increases with the adatom\nconcentrations. Furthermore, magnetism becomes nonmagnetic when the adatom\nconcentration is beyond 60 % adsorption. There are many low-lying\nspin-dependent van Hove singularities. The diversified properties are\nattributed to the significant X-C bonds, the strong X-X bonds, and the adatom-\nand edge-carbon-induced spin states."}, {"title": "Electrical tuning of spin splitting in Bi-doped ZnO nanowires", "abstract": "The effect of applying an external electric field on doping-induced\nspin-orbit splitting of the lowest conduction-band states in a bismuth-doped\nzinc oxide nanowire is studied by performing electronic structure calculations\nwithin the framework of density functional theory. It is demonstrated that spin\nsplitting in Bi-doped ZnO nanowires could be tuned and enhanced electrically\nvia control of the strength and direction of the applied electric field, thanks\nto the nonuniform and anisotropic response of the ZnO:Bi nanowire to external\nelectric fields. The results reported here indicate that a single ZnO nanowire\ndoped with a low concentration of Bi could function as a spintronic device,\noperation of which is controlled by applied lateral electric fields."}, {"title": "Pyrolytic Graphite Sheet, a New Adsorption Substrate for Superfluid Thin Films", "abstract": "We have measured surface morphology and gas adsorption characteristics of\nuncompressed pyrolytic graphite sheet (uPGS) which is a candidate substrate for\nAC and DC superflow experiments on monolayers of 4He below T = 1 K. The PGS is\na mass-produced thin graphite sheet with various thicknesses between 10 and 100\n{\\mu}m. We employed a variety of measuring techniques such as imagings with\noptical microscope, SEM and STM, Raman spectroscopy, and adsorption isotherm.\nPGS has smooth and atomically-flat external surfaces with high crystallinity.\nAlthough the specific surface area (<0.1 m$^2$/g) is rather small, by making\nuse of its smooth external surface, the thinnest uPGS of 10 {\\mu}m thick is\nfound to be suitable for the superflow experiments on the strictly\ntwo-dimensional helium systems."}, {"title": "Density and Microstructure of Amorphous Carbon Thin Films", "abstract": "In this work, we studied amorphous carbon ($a$-C) thin films deposited using\ndirect current (dc) and high power impulse magnetron sputtering (HiPIMS)\ntechniques. The microstructure and electronic properties reveal subtle\ndifferences in $a$-C thin films deposited by two techniques. While, films\ndeposited with dcMS have a smooth texture typically found in $a$-C thin films,\nthose deposited with HiPIMS consist of dense hillocks surrounded by a porous\nmicrostructure. The density of $a$-C thin films is a decisive parameter to\njudge their quality. Often, x-ray reflectivity (XRR) has been used to measure\nthe density of carbon thin films. From the present work, we find that\ndetermination of density of carbon thin films, specially those with a thickness\nof few tens of nm, may not be accurate with XRR due to a poor scattering\ncontrast between the film and substrate. By utilizing neutron reflectivity (NR)\nin the time of flight mode, a technique not commonly used for carbon thin\nfilms, we could accurately measure differences in the densities of $a$-C thin\nfilms deposited using dcMS and HiPIMS."}, {"title": "Temperature Dependent Layer Breathing Modes in Two Dimensional Materials", "abstract": "Relative out of plane displacements of the constituent layers of two\ndimensional materials gives rise to unique low frequency breathing modes. By\ncomputing the height-height correlation functions in momentum space, we show\nthat, the layer breathing modes (LBMs) can be mapped consistently to vibrations\nof a simple linear chain model. Our calculated thickness dependence of LBM\nfrequencies for few layer (FL) graphene and molybdenum disulphide (MoS$_{2}$)\nare in excellent agreement with available experiments. Our results show a\nredshift of LBM frequency with increase in temperature, which is a direct\nconsequence of anharmonicities present in the interlayer interaction. We also\npredict the thickness and temperature dependence of LBM frequencies for FL\nhexagonal boron nitride (hBN). Our study provides a simple and efficient way to\nprobe the interlayer interaction for layered materials and their\nheterostructures, with the inclusion of anharmonic effects."}, {"title": "Evolution of Glassy Carbon Microstructure: In Situ Transmission Electron Microscopy of the Pyrolysis Process", "abstract": "Glassy carbon is a graphene-rich form of elemental carbon obtained from\npyrolysis of polymers, which is composed of three-dimensionally arranged,\ncurved graphene fragments alongside fractions of disordered carbon and voids.\nPyrolysis encompasses gradual heating of polymers above 900 degree C under\ninert atmosphere, followed by cooling to room temperature. Here we report on an\nexperimental method to perform in situ high-resolution transmission electron\nmicroscopy (HR-TEM) for the direct visualization of microstructural evolution\nin a pyrolyzing polymer in the 500-1200 degree C temperature range. The results\nare compared with the existing microstructural models of glassy carbon.\nReported experiments are performed at 80 kV acceleration voltage using\nMEMS-based heating chips as sample substrates to minimize any undesired\nbeam-damage or sample preparation induced transformations. The outcome suggests\nthat the geometry, expansion and atomic arrangement within the resulting\ngraphene fragments constantly change, and that the intermediate structures\nprovide important cues on the evolution of glassy carbon. A complete\nunderstanding of the pyrolysis process will allow for a general process tuning\nspecific to the precursor polymer for obtaining glassy carbon with pre-defined\nproperties."}, {"title": "Band dependence of charge density wave in quasi-one-dimensional Ta2NiSe7 probed by orbital magnetoresistance", "abstract": "Ta2NiSe7 is a quasi-one-dimensional (quasi-1D) transition-metal chalcogenide\nwith Ta and Ni chain structure. An incommensurate charge-density wave (CDW) in\nthis quasi-1D structure was well studied previously using tunnelling spectrum,\nX-ray and electron diffraction, whereas its transport property and the relation\nto the underlying electronic states remain to be explored. Here we report our\nresults of magnetoresistance (MR) on Ta2NiSe7. A breakdown of the Kohler's rule\nis found upon entering the CDW state. Concomitantly, a clear change of\ncurvature in the field dependence of MR is observed. We show that the curvature\nchange is well described by two-band orbital MR, with the hole density being\nstrongly suppressed in the CDW state, indicating that the $p$ orbitals from Se\natoms dominate the change in transport through the CDW transition."}, {"title": "Platelet-zone in an age-hardening Mg-Zn-Gd alloy", "abstract": "The structure of a unique platelet zone with a three close-packed layer\nthickness, which occurred in a Mg-1at.%Zn-2at.%Gd alloy annealed at low\ntemperatures (<~500K), has been determined based on scanning transmission\nelectron microscopy and first principles calculations."}, {"title": "Mesoscopic 2D Charge Transport in Commonplace PEDOT:PSS Films", "abstract": "The correlation between the transport properties and structural degrees of\nfreedom of conducting polymers is a central concern in both practical\napplications and scientific research. In this study, we demonstrated the\nexistence of mesoscopic two-dimensional (2D) coherent charge transport in\npoly(3,4-ethylenedioxythiophene):poly(styrenesulfonate) (PEDOT:PSS) film by\nperforming structural investigations and high-field magnetoconductance (MC)\nmeasurements in magnetic fields of up to 15 T. We succeeded in observing marked\npositive MCs reflecting 2D electronic states in a conventional drop-cast film.\nThis low-dimensional feature is surprising, since PEDOT:PSS-a mixture of two\ndifferent polymers-seems to be significantly different from crystalline 2D\nmaterials in the viewpoint of the structural inhomogeneity, especially in\npopular drop-cast thick films. The results of the structural experiments\nsuggest that such 2D transport originates from the nanometer-scale\nself-assembled laminated structure, which is composed of PEDOT nanocrystals\nwrapped by insulating sheets consisting of amorphous PSSs. These results\nindicate that charge transport in the PEDOT:PSS film can be divided into two\nregimes: mesoscopic 2D coherent tunneling and macroscopic three-dimensional\nhopping among 2D states. Our findings elucidate the hieratical nature of charge\ntransport in the PEDOT:PSS film, which could provide new insight into a recent\nengineering concern, i.e., the anisotropic conductance."}, {"title": "Stochastic Density Functional Theory at Finite Temperatures", "abstract": "Simulations in the warm dense matter regime using finite temperature\nKohn-Sham density functional theory (FT-KS-DFT), while frequently used, are\ncomputationally expensive due to the partial occupation of a very large number\nof high-energy KS eigenstates which are obtained from subspace diagonalization.\nWe have developed a stochastic method for applying FT-KS-DFT, that overcomes\nthe bottleneck of calculating the occupied KS orbitals by directly obtaining\nthe density from the KS Hamiltonian. The proposed algorithm, scales as\n$O\\left(NT^{-1}\\right)$ and is compared with the high-temperature limit scaling\n$O\\left(N^{3}T^{3}\\right)$ of the deterministic approach, where $N$ is the\nsystem size (number of electrons, volume etc.) and $T$ is the temperature. The\nmethod has been implemented in a plane-waves code within the local density\napproximation (LDA); we demonstrate its efficiency, statistical errors and bias\nin the estimation of the free energy per electron for a diamond structure\nsilicon. The bias is small compared to the fluctuations, and is independent of\nsystem size. In addition to calculating the free energy itself, one can also\nuse the method to calculate its derivatives and obtain the equations of state."}, {"title": "Impact of Small Phonon Energies on the Charge-Carrier Lifetimes in Metal-Halide Perovskites", "abstract": "Solar cells based on metal-halide perovskite absorber layers have resulted in\noutstanding photovoltaic devices with long non-radiative lifetimes as a crucial\nfeature enabling high efficiencies. Long non-radiative lifetimes occur if the\ntransfer of the energy of the electron-hole pair into vibrational energy is\nslow, due to, e.g., a low density of defects, weak electron phonon coupling or\nthe release of a large number of phonons needed for a single transition. Here,\nwe discuss the implications of the known material properties of metal-halide\nperovskites (such as permittivities, phonon energies and effective masses) and\ncombine those with basic models for electron-phonon coupling and\nmultiphonon-transition rates in polar semiconductors. We find that the low\nphonon energies of MAPbI$_3$ lead to a strong dependence of recombination rates\non trap position, which can be readily deduced from the underlying physical\neffects determining non-radiative transitions. Here, we show that this is\nimportant for the non-radiative recombination dynamics of metal-halide\nperovskites, as it implies that these systems are rather insensitive to defects\nthat are not at midgap energy. This can lead to long lifetimes, which indicates\nthat the low phonon energies are likely an important factor for the high\nperformance of optoelectronic devices with metal halide perovskites."}, {"title": "Comment on \"Optical Imaging of Light-Induced Thermopower in Semiconductor\" [Phys. Rev. Applied 5, 024005 (2016)]", "abstract": "In a recent article [Phys. Rev. Applied 5, 024005 (2016)], Gibelli and\ncoworkers proposed a method to determine the thermopower, i.e. the Seebeck\ncoefficient, using photoluminescence measurements. The photoluminescence\nspectra are used to obtain the local gradients of both the electrochemical\npotential difference between electron and holes and the temperature of the\nelectron-hole plasma. However, the definition of the thermopower given in that\narticle seems erroneous due to a confusion between the different physical\nquantities needed to derive this parameter."}, {"title": "Elastic, electronic, optical and thermoelectric properties of K2Cu2GeS4: a new chalcogenide material", "abstract": "We report the first principles study of structural, elastic, electronic,\noptical and thermoelectric properties of newly synthesized K2Cu2GeS4. The\nstructural parameters are found to be in good agreement with experimental\nresults. The single crystal elastic constants (Cij) are calculated and\nK2Cu2GeS4 is found to be mechanical stable. The analysis of polycrystalline\nelastic constants reveals that the compound is expected to be soft in nature.\nThe values of Pugh and Poisson ratios suggested that the compound lies in the\nborder line of ductile/brittle behavior. The chemical bonding is primarily\nionic, the inter-atomic forces are central in nature and the compound is\nmechanically anisotropic. The computed electronic band profile shows\nsemiconducting characteristics and the estimated band gap is strongly dependent\non the functional used representing the exchange correlations. The nature of\nchemical bonding is explained using electronic charge density mapping.\nImportant optical constants such as dielectric constants, refractive index,\nabsorption coefficient, photoconductivity, reflectivity and loss function are\ncalculated and discussed in detail. Optical conductivity is found to be in good\nqualitative agreement with the results of band structure calculations. The\nSeebeck coefficients are positive for the entire temperature range used in this\nstudy, suggesting the presence of p-type charge carriers. We have obtained\nlarge Seebeck coefficent, 681 V/K at 100 K and 286 V/K at 300 K. At room\ntemperature, the electrical conductivity and electronic thermal conductivity\nare 1.8 31018 ms)-1 and 0.5 1014 W/mK.s, respectively. The dimensionless figure\nof merit of K2Cu2GeS4 is evaluated as ~1.0 at 300 K. This suggests that\nK2Cu2GeS4 is a potential candidate for thermoelectric applications."}, {"title": "Accelerated Carrier Recombination by Grain Boundary/Edge Defects in MBE Grown Transition Metal Dichalcogenides", "abstract": "Defect-carrier interaction in transition metal dichalcogenides (TMDs) play\nimportant roles in carrier relaxation dynamics and carrier transport, which\ndetermines the performance of electronic devices. With femtosecond laser\ntime-resolved spectroscopy, we investigated the effect of grain boundary/edge\ndefects on the ultrafast dynamics of photoexcited carrier in MBE grown MoTe2\nand MoSe2. We found that, comparing with exfoliated samples, carrier\nrecombination rate in MBE grown samples accelerates by about 50 times. We\nattribute this striking difference to the existence of abundant grain\nboundary/edge defects in MBE grown samples, which can serve as effective\nrecombination centers for the photoexcited carriers. We also observed coherent\nacoustic phonons in both exfoliated and MBE grown MoTe2, indicating strong\nelectron-phonon coupling in this materials. Our measured sound velocity agrees\nwell with previously reported result of theoretical calculation. Our findings\nprovide useful reference for the fundamental parameters: carrier lifetime and\nsound velocity, reveal the undiscovered carrier recombination effect of grain\nboundary/edge defects, both of which will facilitate the defect engineering in\nTMD materials for high speed opto-electronics."}, {"title": "Aluminum and Gallium Distribution in the Lu3(Al5-xGax)O12:Ce Multicomponent Garnet Scintillators Investigated by the Solid-State NMR and DFT calculations", "abstract": "Distribution of aluminum and gallium atoms over the tetrahedral and\noctahedral sites in the garnet structure was studied in the mixed\nLu3Al5-xGaxO12 crystals using the 27Al and 71Ga MAS NMR together with the\nsingle crystal 71Ga NMR. The experimental study was accompanied by theoretical\ncalculations based on the density functional theory in order to predict the\ntendency in substitutions of Al by Ga in the mixed garnets. Both experimental\nand theoretic results show a non-uniform distribution of Al and Ga over the\ntetrahedral and octahedral sites in the garnet structure, with strong\npreferences for Ga, having larger ionic radius than Al, to occupy the\ntetrahedral site with smaller volume in the garnet structure. The quadrupole\ncoupling constants and chemical shift parameters for Al and Ga nuclei have been\ndetermined for all the studied compounds as well as electric field gradients at\nAl and Ga nuclei were calculated in the framework of the density functional\ntheory."}, {"title": "A review of thermal transport and electronic properties of borophene", "abstract": "In recent years, two-dimensional boron sheets (borophene) have been\nexperimentally synthesized and theoretically observed as promising conductor or\ntransistor with novel thermal and electronic properties. We first give a\ngeneral survey of some notable electronic properties of borophene, including\nthe superconductivity and topological characters. We then mainly review the\nbasic approaches, thermal transport, as well as the mechanical properties of\nborophene with different configurations. This review gives a general\nunderstanding of some of the crucial thermal transport and electronic\nproperties of borophene, and also calls for further experimental investigations\nand applications on certain scientific community."}, {"title": "Morphology dependent surface properties of nanostructured GaN films grown by molecular beam epitaxy", "abstract": "The effect of film morphology on its surface chemistry and band structure has\nbeen analyzed for gallium nitride epitaxial films grown by molecular beam\nepitaxy. The film morphology has been studied using scanning electron\nmicroscopy and atomic force microscopy, and the bandstructure, defect and\nemission properties have been studied by X ray photoelectron spectroscopy and\ncathodoluminescence spectroscopy. It was found that the highly porous GaN\nnanowall network shows the highest relative conductivity and does not have\ndefect related luminescence. The flatter films were more resistive and showed\nyellow luminescence, due to Ga vacancies. GaN nanowall network exhibited a\nFermi level pinning at (1.8 $\\pm$ 0.2) eV above valence band maximum,\nsuggesting the presence of a Ga adlayer on the surface of GaN nanowall network.\nAr ion sputtering was found to preferentially sputter N atoms leading to\nsurface metallization."}, {"title": "Dynamic magnetic-transformation-induced exchange bias in (Fe2O3)0.1-(FeTiO3)0.9", "abstract": "Up to now, for the conventional exchange bias (EB) systems there has been one\npinning phase and one pinned phase, and the pinning and pinned phases are\ninherent to the material and do not mutually transform into each other.\nInterestingly, we show here that EB is observed in a special system\n(Fe2O3)0.1(FeTiO3)0.9 (HI9) different from the conventional EB system. Neutron\npowder diffraction and magnetic measurement confirm that for HI9: i) two types\nof short range antiferromagnetic ordering coexist, ii) there are two pinning\nphases and one pinned phase, iii) the pinned phase is not intrinsic to the\nstructure but can be dynamically produced from the pinning phase with the help\nof an external magnetic field. Consequently, two anomalous EB behaviors are\nobserved: i) both the coercivity (HC) and the exchange bias field (HE)\nsimultaneously decrease to zero at 30 K, ii) for a high cooling field (Hcool)\nHE decreases logarithmically with increasing Hcool. Using Arrott plots it is\nconfirmed that the first order magnetic phase transformation (FOMPT) from the\nAFM Fe2+ to ferromagnetic (FM) Fe2+ and the second order magnetic phase\ntransformation (SOMPT) for the process whereby the FM Fe2+ aligns with the\nexternal field direction coexist in HI-9. The Morin transition and FOMPT cause\nthe anomalous EB behaviors. This work may provide fresh ideas for research into\nEB behavior."}, {"title": "Molecular beam epitaxy and defect structure of Ge (111)/epi-Gd2O3 (111) /Si (111) heterostructures", "abstract": "Molecular beam epitaxy of Ge (111) thin films on epitaxial-Gd2O3/Si(111)\nsubstrates is reported, along with a systematic investigation of the evolution\nof Ge growth, and structural defects in the grown epilayer. While Ge growth\nbegins in the Volmer-Weber growth mode, the resultant islands coalesce within\nthe first 10 nm of growth, beyond which a smooth two-dimensional surface\nevolves. Coalescence of the initially formed islands results in formation of\nrotation and reflection microtwins, which constitute a volume fraction of less\nthan 1 %. It is also observed that while the stacking sequence of the (111)\nplanes in the Ge epilayer is similar to that of the Si substrate, the (111)\nplanes of the Gd2O3 epilayer are rotated by 180 degree about the [111]\ndirection. In metal-semiconductor-metal schottky photodiodes fabricated with\nthese all-epitaxial Ge-on-insulator (GeOI) samples, significant suppression of\ndark current is observed due to the presence of the Gd2O3 epilayer. These\nresults are promising for application of these GeOI structures as virtual\nsubstrates, or for realization of high-speed group-IV photonic components."}, {"title": "Large Spin Hall Effect in an Amorphous Binary Alloy", "abstract": "We investigate the spin Hall effect of W-Hf thin films, which exhibit a phase\ntransition from a segregated phase mixture to an amorphous alloy below 70% W.\nThe spin Hall angle was determined with a planar harmonic Hall voltage\ntechnique. Due to the accompanying jump in resistivity, the spin Hall angle\nshows a pronounced maximum at the composition of the phase transition. The spin\nHall conductivity does, however, reduce from W to Hf with a weak discontinouity\nacross the phase transition. The maximum spin Hall angle of $\\theta_\\mathrm{SH}\n= -0.25$ is obtained for amorphous W$_{0.7}$Hf$_{0.3}$. A detailed comparison\nwith spin Hall conductivities calculated from first principles for hcp, fcc,\nand bcc solid solutions provides valuable insight into the alloying physics of\nthis binary system."}, {"title": "Observation of magnetic vortex pairs at room temperature in a planar α-Fe2O3/Co heterostructure", "abstract": "Vortices are among the simplest topological structures, and occur whenever a\nflow field `whirls' around a one-dimensional core. They are ubiquitous to many\nbranches of physics, from fluid dynamics to superconductivity and\nsuperfluidity, and are even predicted by some unified theories of particle\ninteractions, where they might explain some of the largest-scale structures\nseen in today's Universe. In the crystalline state, vortex formation is rare,\nsince it is generally hampered by long-range interactions: in ferroic materials\n(ferromagnetic and ferroelectric), vortices are only observed when the effects\nof the dipole-dipole interaction is modified by confinement at the nanoscale,\nor when the parameter associated with the vorticity does not couple directly\nwith strain. Here, we present the discovery of a novel form of vortices in\nantiferromagnetic (AFM) hematite ($\\alpha$-Fe$_2$O$_3$) epitaxial films, in\nwhich the primary whirling parameter is the staggered magnetisation.\nRemarkably, ferromagnetic (FM) topological objects with the same vorticity and\nwinding number of the $\\alpha$-Fe$_2$O$_3$ vortices are imprinted onto an\nultra-thin Co ferromagnetic over-layer by interfacial exchange. Our data\nsuggest that the ferromagnetic vortices may be merons (half-skyrmions, carrying\nan out-of-plane core magnetisation), and indicate that the vortex/meron pairs\ncan be manipulated by the application of an in-plane magnetic field,\nH$_{\\parallel}$, giving rise to large-scale vortex-antivortex annihilation."}, {"title": "Quantification of lubrication and particle size distribution effects on tensile strength and stiffness of tablets", "abstract": "We adopt a Quality by Design (QbD) paradigm to better control the mechanical\nproperties of tablets. To this end, the effect of particle size distribution,\nlubricant concentration, and mixing time on the tensile strength and elastic\nmodulus of tablets is studied. Two grades of lactose, monohydrate and\nspray-dried, are selected. Tablets are compressed to different relative\ndensities ranging from $0.8$ to $0.94$ using an instrumented compaction\nsimulator. We propose a general model, which predicts the elastic modulus and\ntensile strength envelope that a specific powder can obtain based on its\nlubrication sensitivity for different particle size distributions. This is\npossible by introducing a new dimensionless parameter in the existing tensile\nstrength and elastic modulus relationships with relative density. A wide range\nof lubrication conditions is explored and a predictable model is callibrated.\nThe mechanical properties of lactose monohydrate tablets are noticeably\ndependent on particle size, unlike spray-dried lactose where little to almost\nno sensitivity to particle size is observed. The model is designed in a general\nfashion that can capture mechanical quality attributes in response to different\nlubrication conditions and particle size, and it can be extended to powders\nthan undergo different deformation mechanisms, complex mixtures, and doubly\nconvex tablets. Therefore, the model can be used to map the achievable design\nspace of any given formulation."}, {"title": "Negative spin Hall magnetoresistance in antiferromagnetic Cr2O3/Ta bilayer at low temperature region", "abstract": "We investigate the observation of negative spin Hall magnetoresistance (SMR)\nin antiferromagnetic Cr2O3/Ta bilayers at low temperature. The sign of the SMR\nsignals is changed from positive to negative monotonously from 300 K to 50 K.\nThe change of the signs for SMR is related with the competitions between the\nsurface ferromagnetism and bulky antiferromagnetic of Cr2O3. The surface\nmagnetizations of Cr2O3 (0001) is considered to be dominated at higher\ntemperature, while the bulky antiferromagnetics gets to be robust with\ndecreasing of temperature. The slopes of the abnormal Hall curves coincide with\nthe signs of SMR, confirming variational interface magnetism of Cr2O3 at\ndifferent temperature. From the observed SMR ratio under 3 T, the spin mixing\nconductance at Cr2O3/Ta interface is estimated to be 1.12*10^14 (ohm^-1*m^-2),\nwhich is comparable to that of YIG/Pt structures and our early results of\nCr2O3/W. (Appl. Phys. Lett. 110, 262401 (2017))"}, {"title": "Epitaxial Heusler Superlattice Co2MnAl/Fe2MnAl with Perpendicular Magnetic Anisotropy and Termination-Dependent Half-Metallicity", "abstract": "Single-crystal Heusler atomic-scale superlattices that have been predicted to\nexhibit perpendicular magnetic anisotropy and half-metallicity have been\nsuccessfully grown by molecular beam epitaxy. Superlattices consisting of\nfull-Heusler Co$_2$MnAl and Fe$_2$MnAl with one to three unit cell periodicity\nwere grown on GaAs (001), MgO (001), and Cr (001)/MgO (001). Electron energy\nloss spectroscopy maps confirmed clearly segregated epitaxial Heusler layers\nwith high cobalt or high iron concentrations for samples grown near room\ntemperature on GaAs (001). Superlattice structures grown with an excess of\naluminum had significantly lower thin film shape anisotropy and resulted in an\nout-of-plane spin reorientation transition at temperatures below 200 K for\nsamples grown on GaAs (001). Synchrotron-based spin resolved photoemission\nspectroscopy found that the superlattice structure improves the Fermi level\nspin polarization near the X point in the bulk Brillouin zone. Stoichiometric\nCo$_2$MnAl terminated superlattice grown on MgO (001) had a spin polarization\nof 95%, while a pure Co$_2$MnAl film had a spin polarization of only 65%."}, {"title": "Stochastic multi-step polarization switching in ferroelectrics", "abstract": "Consecutive stochastic 90{\\deg} polarization switching events, clearly\nresolved in recent experiments, are described by a new nucleation and growth\nmulti-step model. It extends the classical Kolmogorov-Avrami-Ishibashi approach\nand includes possible consecutive 90{\\deg}- and parallel 180{\\deg}-switching\nevents. The model predicts the results of simultaneous time-resolved\nmacroscopic measurements of polarization and strain, performed on a tetragonal\nPb(Zr,Ti)O3 ceramic in a wide range of electric fields over a time domain of\nfive orders of the magnitude. It allows the determination of the fractions of\nindividual switching processes, their characteristic switching times,\nactivation fields, and respective Avrami indices."}, {"title": "Evidence for a pressure-induced spin transition in olivine-type LiFePO$_{4}$ triphylite", "abstract": "We present a combination of first-principles and experimental results\nregarding the structural and magnetic properties of olivine-type LiFePO$_4$\nunder pressure. Our investigations indicate that the starting $Pbnm$ phase of\nLiFePO$_4$ persists up to 70 GPa. Further compression leads to an isostructural\ntransition in the pressure range of ~70-75 GPa, inconsistent with a former\ntheoretical study. Considering our first-principles prediction for a high-spin\nto low-spin transition of Fe$^{2+}$ close to 72 GPa, we attribute the\nexperimentally observed isostructural transition to a change on the spin state\nof Fe$^{2+}$ in LiFePO$_4$. Compared to relevant Fe-bearing minerals,\nLiFePO$_4$ exhibits the largest onset pressure for a pressure-induced spin\nstate transition."}, {"title": "Minimizing Residues and Strain in 2D Materials Transferred from PDMS", "abstract": "Integrating layered two-dimensional (2D) materials into 3D heterostructures\noffers opportunities for novel material functionalities and applications in\nelectronics and photonics. In order to build the highest quality\nheterostructures, it is crucial to preserve the cleanliness and morphology of\n2D material surfaces that come in contact with polymers such as PDMS during\ntransfer. Here we report that substantial residues and up to ~0.22% compressive\nstrain can be present in monolayer MoS$_{2}$ flakes transferred using PDMS. We\nshow that a UV-ozone pre-cleaning of the PDMS surface before exfoliation\nsignificantly reduces organic residues on transferred MoS$_{2}$ flakes. An\nadditional 200$^{\\circ}$C vacuum anneal after transfer efficiently removes\ninterfacial bubbles and wrinkles as well as accumulated strain, thereby\nrestoring the surface morphology of transferred flakes to their native state.\nOur recipe is important for building clean heterostructures of 2D materials and\nincreasing the reproducibility and reliability of devices based on them."}, {"title": "Magneto-electric effect in doped magnetic ferroelectrics", "abstract": "We propose a model of magneto-electric effect in doped magnetic\nferroelectrics. This magneto-electric effect does not involve the spin-orbit\ncoupling and is based purely on the Coulomb interaction. We calculate magnetic\nphase diagram of doped magnetic ferroelectrics. We show that magneto-electric\ncoupling is pronounced only for ferroelectrics with low dielectric constant. We\nfind that magneto-electric coupling leads to modification of magnetization\ntemperature dependence in the vicinity of ferroelectric phase transition. A\npeak of magnetization appears. We find that magnetization of doped magnetic\nferroelectrics strongly depends on applied electric field."}, {"title": "Microstructure and mechanical properties of Al-Cu alloy with 0.6%Fe produced with ultrasonic vibration and applied pressure", "abstract": "The combined effect of ultrasonic vibration (UT) and applied pressure (P) on\nmicrostructure and mechanical properties of Al-5.0Cu-0.6Mn-0.6Fe alloy were\ninvestigated. The best tensile properties produced by P+UT processing are UTS:\n268MPa, YS: 192MPa, E.L.: 17.1%, respectively, which increasing by 64%, 59% and\n307%, respectively, compare to the Non-treated alloy."}, {"title": "Prediction of a magnetic Weyl semimetal without spin-orbit coupling and strong anomalous Hall effect in the Heusler compensated ferrimagnet Ti2MnAl", "abstract": "We predict a magnetic Weyl semimetal in the inverse Heusler Ti2MnAl, a\ncompensated ferrimagnet with a vanishing net magnetic moment and a Curie\ntemperature of over 650 K. Despite the vanishing net magnetic moment, we\ncalculate a large intrinsic anomalous Hall effect (AHE) of about 300 S/cm. It\nderives from the Berry curvature distribution of the Weyl points, which are\nonly 14 meV away from the Fermi level and isolated from trivial bands.\nDifferent from antiferromagnets Mn3X (X= Ge, Sn, Ga, Ir, Rh, and Pt), where the\nAHE originates from the non-collinear magnetic structure, the AHE in Ti2MnAl\nstems directly from the Weyl points and is topologically protected. The large\nanomalous Hall conductivity (AHC) together with a low charge carrier\nconcentration should give rise to a large anomalous Hall angle. In contrast to\nthe Co-based ferromagnetic Heusler compounds, the Weyl nodes in Ti2MnAl do not\nderive from nodal lines due to the lack of mirror symmetries in the inverse\nHeusler structure. Since the magnetic structure breaks spin-rotation symmetry,\nthe Weyl nodes are stable without SOC. Moreover, because of the large\nseparation between Weyl points of opposite topological charge, the Fermi arcs\nextent up to 75% of the reciprocal lattice vectors in length. This makes\nTi2MnAl an excellent candidate for the comprehensive study of magnetic Weyl\nsemimetals. It is the first example of a material with Weyl points, large\nanomalous Hall effect and angle despite a vanishing net magnetic moment."}, {"title": "Piezoelectric field, exciton lifetime, and cathodoluminescence intensity at threading dislocations in GaN{0001}", "abstract": "The strain field of a dislocation emerging at a free surface is partially\nrelaxed to ensure stress free boundary conditions. We show that this relaxation\nstrain at the outcrop of edge threading dislocations in GaN{0001} gives rise to\na piezoelectric volume charge. The electric field produced by this charge\ndistribution is strong enough to dissociate free excitons at distances over 100\nnm from the dislocation line. We evaluate the impact of this effect on\ncathodoluminescence images of dislocations."}, {"title": "Length-scale dependent average structures, piezoelectricity enhancement and depolarization mechanisms in a non-MPB high-performance piezoelectric alloy system PbTiO3-Bi(Zr1/2Ni1/2)O3", "abstract": "There is a general perception that large piezoelectric response in\nferroelectric alloys requires tuning the system towards a morphotropic phase\nboundary (MPB), i.e., a composition driven inter-ferroelectric instability.\nHere we show that high piezoelectric response can be realized even in non-MPB\nalloy systems. This is demonstrated on (1-x)PbTiO3-(x)Bi(Zr0.5Ni0.5)O3 (PT-BNZ)\nby a comprehensive study involving electric-field and temperature dependent\nXRD, Raman spectroscopy, dielectric, piezoelectric and high field electrostrain\nmeasurements. We found that poling-field irreversibly suppresses the cubic-like\nphase at room temperature. Based on our results, we argue that that which\nappears as MPB, comprising of tetragonal and cubic-like phases on the global\nscale, is not so actually. The large piezoresponse is due to coexistence of\ntetragonal regions of long and short-range coherence. The PT-BNZ system is\ntherefore qualitatively different from the conventional MPB systems such as\nPZT, PMN-PT, and PbTiO3-BiScO3, etc., which exhibits coexisting tetragonal and\nrhombohedral/monoclinic phases in thermodynamic equilibrium. In the absence of\ninter-ferroelectric instability as a phenomenon, field induced\npolarization-rotation and inter-ferroelectric transformation are no longer\nplausible mechanisms to explain the large piezoelectric response in PT-BNZ. The\nlarge piezoelectricity is primarily due to enhanced mobility of the tetragonal\ndomain walls enabled by domain miniaturization. Our study proves that\nattainment of large piezoelectricity does not require inter-ferroelectric\ninstability as a necessary criterion."}, {"title": "Electronic band structure engineering in InAs/InSbAs and InSb/InSbAs superlattice heterostructures", "abstract": "We report a detailed ab initio study of two superlattice heterostructures,\none component of which is a unit cell of CuPt ordered InSb_(0.5)As_(0.5). This\nalloy part of the heterostructures is a topological semimetal. The other\ncomponent of each system is a semiconductor, zincblende-InSb, and\nwurtzite-InAs. Both heterostructures are semiconductors. Our theoretical\nanalysis predicts that the variation in the thickness of the InSb layer in\nInSb/InSb_(0.5)As_(0.5) heterostructure renders altered band gaps with\ndifferent characteristics (i.e. direct or indirect). The study holds promise\nfor fabricating heterostructures, in which the modulation of the thickness of\nthe layers changes the number of carrier pockets in these systems."}, {"title": "Coexisting ferroelectric and antiferroelectric phases in dipole ordered substances. Material properties and possible applications", "abstract": "Comprehensive review of investigations of substances with a small difference\nin the energies of the ferroelectric and antiferroelectric types of dipole\nordering is presented. Detailed analysis of the stability of homogeneous phases\nand conditions for existence of the inhomogeneous state of the substance\ncontaining domains of the coexisting ferroelectric and antiferroelectric phases\nis presented. It is shown that the interaction of locally separated domains of\nthe ferroelectric and antiferroelectric phases stabilizes an inhomogeneous\nstate of these materials. The analysis of physical phenomena caused by the\npresence of domains of the coexisting ferroelectric and antiferroelectric\nphases in different dipole ordered substances is presented. Peculiarities of\nbehavior of the systems with the inhomogeneous state of coexisting phases under\nthe action of changing external thermodynamic parameters (temperature,\npressure, electric field, and chemical composition) are analyzed and their\nimportance for applications of these materials is discussed."}, {"title": "The Effect of Ionic Composition on Acoustic Phonon Speeds in Hybrid Perovskites from Brillouin Spectroscopy and Density Functional Theory", "abstract": "Hybrid organic-inorganic perovskites (HOIPs) have recently emerged as highly\npromising solution-processable materials for photovoltaic (PV) and other\noptoelectronic devices. HOIPs represent a broad family of materials with\nproperties highly tuneable by the ions that make up the perovskite structure as\nwell as their multiple combinations. Interestingly, recent high-efficiency PV\ndevices using HOIPs with substantially improved long-term stability have used\ncombinations of different ionic compositions. The structural dynamics of these\nsystems are unique for semiconducting materials and are currently argued to be\ncentral to HOIPs stability and charge-transport properties. Here, we studied\nthe impact of ionic composition on phonon speeds of HOIPs from Brillouin\nspectroscopy experiments and density functional theory calculations for\nFAPbBr$_3$, MAPbBr$_3$, MAPbCl$_3$, and the mixed halide\nMAPbBr$_{1.25}$Cl$_{1.75}$. Our results show that the acoustic phonon speeds\ncan be strongly modified by ionic composition, which we explain by analysing\nthe lead-halide sublattice in detail. The vibrational properties of HOIPs are\ntherefore tuneable by using targeted ionic compositions in the perovskite\nstructure. This tuning can be rationalized with non-trivial effects, for\nexample, considering the influence of the shape and dipole moment of organic\ncations. This has an important implication to further improvements in the\nstability and charge-transport properties of these systems."}, {"title": "The hidden magnetization in ferromagnetic material: Miamagnetism", "abstract": "This paper presents the hidden magnetization features of ferromagnetic\nmaterials: called miamagnetism. As we know, we have several forms of\nmagnetization: the diamagnetism, the paramagnetism, the ferromagnetism etc. The\nmain character of the diamagnetism is that its magnetic susceptibility is\nnegative (from -1e-9 for gas and -1e-6 for liquid and solid to -1 for\nsuperconducting materials of type I) and it is not less than -1 unless for\nspecial materials like metamaterials at high frequencies. The miamagnetism has\nthe character that the magnetic susceptibility can reach at low frequencies a\nnegative value of -155 of magnitude leading to a negative permeability. We can\nnot see it because it is hidden by the ferromagnetic character which has a high\npositive magnetic susceptibility. We use the discrete Fourier transform to\nillustrate this hidden character and the hysteresis model can be represented\nonly by harmonics of (2n+1)f0 of magnitude. This magnetization follows a\nBoltzmann distribution for the modulus of theses harmonics."}, {"title": "Labyrinthine domains in ferroelectric nanoparticles: a manifestation of gradient-driven morphological transition", "abstract": "In the framework of the Landau-Ginzburg-Devonshire (LGD) approach we studied\nfinite size effects of the phase diagram and domain structure evolution in\nspherical nanoparticles of uniaxial ferroelectric. The particle surface is\ncovered by a layer of screening charge characterized by finite screening\nlength. The phase diagram, calculated in coordinates \"particle radius vs.\nscreening length\" has a wide region of versatile poly-domain structures\nseparating single-domain ferroelectric and nonpolar paraelectric phases.\nUnexpectedly, we revealed a region of stable irregular labyrinthine domains in\nthe nanoparticles of uniaxial ferroelectric CuInP2S6 with the first order\nparaelectric-ferroelectric phase transition. We established that the origin of\nlabyrinthine domains is the mutual balance of LGD, polarization gradient and\nelectrostatic energies. The branching of the domain walls appears and increases\nrapidly when the polarization gradient energy decreases below the critical\nvalue. Allowing for the generality of LGD approach, we expect that the\ngradient-induced morphological transition can be the source of labyrinthine\ndomains appearance in many spatially-confined ferroics with long-range order\nparameter, including relaxors, ferromagnetics, antiferrodistortive materials\nand materials with incommensurate ferroic phases."}, {"title": "Ultra-low lattice thermal conductivity in Cs2BiAgX6 (X=Cl, Br): Potential thermoelectric materials", "abstract": "We have explored electronic and thermoelectric properties of bismuth-based\ndouble-perovskite halides Cs2BiAgX6 by using first principles calculations. The\ncalculated indirect bandgaps 2.85 eV and 1.99 eV for Cs2BiAgCl6 and Cs2BiAgBr6,\nrespectively well agree with the measured value (2.77 eV of Cs2BiAgCl6 and 2.19\neV of Cs2BiAgBr6). We have calculated the relaxation time and lattice thermal\nconductivity by using relaxation time approximation (RTA) within supercell\napproach. The lattice thermal conductivities for both compounds are remarkably\nlow and the obtained values at 300K for Cs2BiAgCl6 and Cs2BiAgBr6 are 0.078 and\n0.065 Wm-1K-1, respectively. Such quite low lattice thermal conductivity arises\ndue to low phonon group velocity in the large weighted phase space and large\nphonon scattering. The large Seebeck coefficient obtained for both halides at\n400K. We have obtained the maximum power factors at 700K and the corresponding\nthermoelectric figure of merit for Cs2BiAgCl6 and Cs2BiAgBr6 are 0.775 and\n0.774, respectively. The calculated results reveal that both halides are\npotential thermoelectric materials."}, {"title": "Structural, elastic, electronic, magnetic and thermoelectric properties of new quaternary Heusler compounds CoZrMnX (X=Al, Ga, Ge, In)", "abstract": "We have performed a comprehensive set of first principles calculations to\nstudy the structural, elastic, electronic, magnetic and transport properties of\nnew quaternary Heusler compounds CoZrMnX (X =Al, Ga, Ge, In). The results\nshowed that all the quaternary Heusler compounds were stable in Type(I)\nstructure. CoZrMnX are elastically stable and relatively hard materials.\nCoZrMnAl, CoZrMnGa, and CoZrMnIn are found to be ductile and CoZrMnGe is\nbrittle in nature. The calculated Debye temperatures of all compounds are\nrelatively high. The electronic structure calculations reveal that CoZrMnAl is\nnearly half metallic, CoZrMnGa and CoZrMnIn are metallic, and CoZrMnGe is a\nnarrow indirect bandgap semiconductor. The calculated magnetic properties\nimplies that CoZrMnAl, CoZrMnGa, and CoZrMnIn are ferromagnetic while CoZrMnGe\nis non-magnetic material. The CoZrMnAl is highly spin-polarized (96%) and\nCoZrMnGe is non-spin-polarized. Seebeck coefficent (S) in CoZrMnGe is\nrelatively high (-106 {\\mu}V/K at 650K) due to its semiconducting nature. The\ncalculated thermoelectric figure of merit CoZrMnGe is 0.1 at 600K and for\nCoZrMnIn it is also 0.1 at 900 K. We hope our interesting results will inspire\nexperimentalist to synthesis the new quaternary Heusler compounds CoZrMnX (X\n=Al, Ga, Ge, In)."}, {"title": "Fullerene Faraday Cage Keeps Magnetic Properties of Inner Cluster Pristine", "abstract": "Any single molecular magnets (SMM) perspective for application is as good as\nits magnetization stability in ambient conditions. Endohedral metallofullerenes\n(EMFs) provide a solid basis for promising SMMs. In this study, we investigated\nthe behavior of functionalized EMFs on a gold surface (EMF-L-Au). Having\nfollowed the systems molecular dynamics paths, we observed that the chemically\nlocked inner cluster inside fullerene cage will remain locked even at room\ntemperature due to the ligand-effect. We have located multiple possible minima\nwith different charge arrangements between EMF-L-Au fragments. Remarkably, the\ncharge state of the EMF inner cluster remained virtually constant and so\nmagnetic properties are expected to be untouched."}, {"title": "Benchmarking acid and base dopants with respect to enabling the ice V to XIII and ice VI to XV hydrogen-ordering phase transitions", "abstract": "Doping the hydrogen-disordered phases of ice V, VI and XII with hydrochloric\nacid (HCl) has led to the discovery of their hydrogen-ordered counterparts ices\nXIII, XV and XIV. Yet, the mechanistic details of the hydrogen-ordering phase\ntransitions are still not fully understood. This includes in particular the\nrole of the acid dopant and the defect dynamics that it creates within the\nices. Here we investigate the effects of several acid and base dopants on the\nhydrogen ordering of ices V and VI with calorimetry and X-ray diffraction. HCl\nis found to be most effective for both phases which is attributed to a\nfavourable combination of high solubility and strong acid properties which\ncreate mobile H3O+ defects that enable the hydrogen-ordering processes.\nHydrofluoric acid (HF) is the second most effective dopant highlighting that\nthe acid strengths of HCl and HF are much more similar in ice than they are in\nliquid water. Surprisingly, hydrobromic acid doping facilitates hydrogen\nordering in ice VI whereas only a very small effect is observed for ice V.\nConversely, lithium hydroxide (LiOH) doping achieves a performance comparable\nto HF-doping in ice V but it is ineffective in the case of ice VI. Sodium\nhydroxide, potassium hydroxide (as previously shown) and perchloric acid doping\nare ineffective for both phases. These findings highlight the need for future\ncomputational studies but also raise the question why LiOH-doping achieves\nhydrogen-ordering of ice V whereas potassium hydroxide doping is most effective\nfor the 'ordinary' ice Ih."}, {"title": "Opposites Attract, Muons as Direct Probes for Iodide Diffusion in Methyl Ammonium Lead Iodide", "abstract": "The volume of research into organo-lead hailde perovskites is increasing\nrapidly, with perovskite solar cell efficiencies reaching as high as 22\npercent. There is considerable evidence that mobile ions in the perovskite\nstrongly influence the properties of the solar cell, with the majority of\nstudies carried out on whole cells under bias. Here we use muon spin relaxation\nto directly probe iodide diffusion in methyl ammonium lead iodide (MAPI). This\nis the first time that has been used to detect iodide diffusion in any material\nand the results provide valuable insight into the movement of ions in lead\nhalide perovskites. The experiment was carried out in the dark with no external\nbiases applied and allowed us to calculate a diffusion coefficient of 1.6\nx10-14 cm2/s for iodide in MAPI at 300 K."}, {"title": "Mechanical and thermal properties of Graphene Random nanofoams via Molecular Dynamics simulations", "abstract": "Graphene foams have recently attracted a great deal of interest for their\npossible use in technological applications, such as electrochemical storage\ndevices, wearable electronics, and chemical sensing. In this work, we present\ncomputational investigations, performed by using molecular dynamics with\nreactive potentials, of the mechanical and thermal properties of graphene\nrandom nanofoams. In particular, we assess the mechanical and thermal\nperformances of four families of random foams characterized by increasing mass\ndensity and decreasing average pore size. We find that the foams' mechanical\nperformances under tension cannot be rationalized in terms of mass density,\nwhile they are principally related to their topology. Under compression,\nhigher-density foams show the typical slope change in the stress-strain curve\nat 5-10% strain, moving from linear elasticity to bending stress plateau. At\nvariance, lower density foams display a quasi-linear behaviour up to 35%\nstrain. Furthermore, we assess the thermal conductivity of these random foams\nusing the Green-Kubo approach. While foam thermal conductivity is affected by\nboth connectivity and defects, nevertheless we obtain similar values for all\nthe investigated families."}, {"title": "Phononic Weyl Nodal Straight Lines in High-Temperature Superconductor MgB$_2$", "abstract": "Based on first-principles calculations, we predict that the superconducting\nMgB$_2$ with a AlB$_2$-type centrosymmetric lattice host the so-called phononic\ntopological Weyl nodal lines (PTWNLs) on its bulk phonon spectrum. These PTWNLs\ncan be viewed as countless Weyl points (WPs) closely aligned along the straight\nlines in the $-$H-K-H direction within the three-dimensional Brillouin zone\n(BZ). Their topological non-trivial natures are confirmed by the calculated\nBerry curvature distributions on the planes perpendicular to these lines. These\nlines are highly unique, because they exactly locate at the high-symmetry\nboundary of the BZ protected by the mirror symmetry and, simultaneously,\nstraightly transverse the whole BZ, in different from known classifications\nincluding nodal rings, nodal chains or nets, and nodal loops. On the\n(10$\\bar{1}$0) crystal surface, the PTWNLs-induced drumhead-like non-trivial\nsurface states appear within the rectangular area confined by the projected\nlines of the PTWNLs with opposite chirality. Moreover, when the mirror symmetry\nis broken, the double-degenerate PTWNLs are further lifted to form a pair WPs\nwith opposite chirality. Our results pave the ways for future experimental\nstudy on topological phonons on MgB$_2$ and highlights similar results in a\nseries of isostructural AlB$_2$-type metallic diborides."}, {"title": "A novel concept of pseudo ternary diffusion couple for the estimation of diffusion coefficients in multicomponent systems", "abstract": "A pseudo ternary diffusion couple technique in a multicomponent system by\nsimplifying the mathematical complications of Onsager formalism is proposed for\nthe estimation of composition dependent values of the interdiffusion\ncoefficients. This is otherwise impossible following the conventional method in\na system with more than three components. Other alternative methods estimate\nthe average diffusion coefficients over a composition range of random choice\nand lack physical significance. This method can be followed in a multicomponent\nsystem with any number of components on the condition that only three\ncomponents develop diffusion profiles keeping others as constant."}, {"title": "Quantum Many-Body Effects in X-Ray Spectra Efficiently Computed using a Basic Graph Algorithm", "abstract": "The growing interest in using x-ray spectroscopy for refined materials\ncharacterization calls for accurate electronic-structure theory to interpret\nx-ray near-edge fine structure. In this work, we propose an efficient and\nunified framework to describe all the many-electron processes in a Fermi liquid\nafter a sudden perturbation (such as a core hole). This problem has been\nvisited by the Mahan-Nozi\\'eres-De Dominicis (MND) theory, but it is\nintractable to implement various Feynman diagrams within first-principles\ncalculations. Here, we adopt a non-diagrammatic approach and treat all the\nmany-electron processes in the MND theory on an equal footing. Starting from a\nrecently introduced determinant formalism [Phys. Rev. Lett. 118, 096402\n(2017)], we exploit the linear-dependence of determinants describing different\nfinal states involved in the spectral calculations. An elementary graph\nalgorithm, breadth-first search, can be used to quickly identify the important\ndeterminants for shaping the spectrum, which avoids the need to evaluate a\ngreat number of vanishingly small terms. This search algorithm is performed\nover the tree-structure of the many-body expansion, which mimics a path-finding\nprocess. We demonstrate that the determinantal approach is computationally\ninexpensive even for obtaining x-ray spectra of extended systems. Using\nKohn-Sham orbitals from two self-consistent fields (ground and core-excited\nstate) as input for constructing the determinants, the calculated x-ray spectra\nfor a number of transition metal oxides are in good agreement with experiments.\nMany-electron aspects beyond the Bethe-Salpeter equation, as captured by this\napproach, are also discussed, such as shakeup excitations and many-body wave\nfunction overlap considered in Anderson's orthogonality catastrophe."}, {"title": "On melt solutions for the growth of CaTiO$_3$ crystals", "abstract": "When calcium titanate crystals are grown from stoichiometric melts, they\ncrystallize in the cubic perovskite structure. Upon cooling to room temperature\nthey undergo subsequent phase transitions to tetragonal and orthorhombic\nmodifications. These phase transitions are disruptive and result in severely\ndamaged crystals. This paper presents differential thermal analysis data for\nseveral prospective solvents, with the aim to identify a system offering the\npossibility to perform crystal growth of undistorted CaTiO$_3$ crystals by\ncrystallizing them significantly below the melting point directly in the low\ntemperature modification. From mixtures CaF$_2$:TiO$_2$:CaTiO$_3$ = 3:1:1\n(molar ratio) the growth of undistorted, at least millimeter-sized CaTiO$_3$\ncrystals is possible."}, {"title": "Ab-initio study of the Coulomb interaction in NbxCo clusters: Strong on-site versus weak non-local screening", "abstract": "By means of ab-initio calculations in conjunction with the random-phase\napproximation (RPA) within the full-potential linearized augmented plane wave\nmethod we study the screening of the Coulomb interaction in NbxCo (1<=x<=9)\nclusters. In addition, these results are compared with pure bcc Nb bulk. We\nfind that for all clusters the onsite Coulomb interaction in RPA is strongly\nscreened whereas the inter-site non-local Coulomb interaction is weakly\nscreened and for some clusters it is unscreened or even anti-screened. This is\nin strong contrast with pure Nb bulk, where the inter-site Coulomb interaction\nis almost completely screened. Further, constrained RPA calculations reveal\nthat the contribution of the Co 3d ! 3d channel to the total screening of the\nCo 3d electrons is small. Moreover, we find that both the onsite and inter-site\nCoulomb interaction parameters decrease in a reasonable approximation linearly\nwith the cluster size and for clusters having more than 20 Nb atoms a\ntransition from 0D to 3D screening is expected to take place."}, {"title": "Biexciton fine structure in monolayer transition metal dichalcogenides", "abstract": "The optical properties of atomically thin transition metal dichalcogenide\n(TMDC) semiconductors are shaped by the emergence of correlated many-body\ncomplexes due to strong Coulomb interaction. Exceptional electron-hole exchange\npredestines TMDCs to study fundamental and applied properties of Coulomb\ncomplexes such as valley depolarization of excitons and fine-structure\nsplitting of trions. Biexcitons in these materials are less understood and it\nhas been established only recently that they are spectrally located between\nexciton and trion.\n  Here we show that biexcitons in monolayer TMDCs exhibit a distinct fine\nstructure on the order of meV due to electron-hole exchange. Ultrafast\npump-probe experiments on monolayer WSe$_2$ reveal decisive biexciton\nsignatures and a fine structure in excellent agreement with a microscopic\ntheory. We provide a pathway to access biexciton spectra with unprecedented\naccuracy, which is valuable beyond the class of TMDCs, and to understand even\nhigher Coulomb complexes under the influence of electron-hole exchange."}, {"title": "Semi-metals as potential thermoelectric materials: case of HgTe", "abstract": "The best thermoelectric materials are believed to be heavily doped\nsemiconductors. The presence of a bandgap is assumed to be essential to achieve\nlarge thermoelectric power factor and figure of merit. In this work, we study\nHgTe as an example semimetal with competitive thermoelectric properties. We\nemploy ab initio calculations with hybrid exchange-correlation functional to\naccurately describe the electronic band structure in conjunction with the\nBoltzmann Transport theory to investigate the electronic transport properties.\nWe show that intrinsic HgTe, a semimetal with large disparity in its electron\nand hole masses, has a high thermoelectric power factor that is comparable to\nthe best known thermoelectric materials. We also calculate the lattice thermal\nconductivity using first principles calculations and evaluate the overall\nfigure of merit. Finally, we prepare semi-metallic HgTe samples and we\ncharacterize their transport properties. We show that our theoretical\ncalculations agree well with the experimental data"}, {"title": "Hydrodynamic heat transport regime in bismuth: a theoretical viewpoint", "abstract": "Bismuth is one of the rare materials in which second sound has been\nexperimentally observed. Our exact calculations of thermal transport with the\nBoltzmann equation predict the occurrence of this Poiseuille phonon flow\nbetween $\\approx$ 1.5 K and $\\approx$ 3.5 K, in sample size of 3.86 mm and 9.06\nmm, in consistency with the experimental observations. Hydrodynamic heat flow\ncharacteristics are given for any temperature: heat wave propagation length,\ndrift velocity, Knudsen number. We discuss a Gedanken-experiment allowing to\nassess the presence of a hydrodynamic regime in any bulk material."}, {"title": "High thermal conductivity of bulk epoxy resin by bottom-up parallel-linking and strain: a molecular dynamics study", "abstract": "The ultra-low thermal conductivity (~0.3 Wm-1K-1) of amorphous epoxy resins\nsignificantly limits their applications in electronics. Conventional top-down\nmethods e.g. electrospinning usually result in aligned structure for linear\npolymers thus satisfactory enhancement on thermal conductivity, but they are\ndeficient for epoxy resin polymerized by monomers and curing agent due to\ncompletely different cross-linked network structure. Here, we proposed a\nbottom-up strategy, namely parallel-linking method, to increase the intrinsic\nthermal conductivity of bulk epoxy resin. Through equilibrium molecular\ndynamics simulations, we reported on a high thermal conductivity value of\nparallel-linked epoxy resin (PLER) as 0.80 Wm-1K-1, more than twofold higher\nthan that of amorphous structure. Furthermore, by applying uniaxial tensile\nstrains along the intra-chain direction, a further enhancement in thermal\nconductivity was obtained, reaching 6.45 Wm-1K-1. Interestingly, we also\nobserved that the inter-chain thermal conductivities decrease with increasing\nstrain. The single chain of epoxy resin was also investigated and,\nsurprisingly, its thermal conductivity was boosted by 30 times through tensile\nstrain, as high as 33.8 Wm-1K-1. Our study may provide a new insight on the\ndesign and fabrication of epoxy resins with high thermal conductivity."}, {"title": "Biaxial tensile strain tuned up-and-down behavior on lattice thermal conductivity in $β$-AsP monolayer", "abstract": "Various two-dimensional (2D) materials with graphene-like buckled structure\nemerge, and the $\\beta$-phase AsP monolayer has been recently proposed to be\nthermodynamically stable from first-principles calculations. The studies of\nthermal transport are very useful for these 2D materials-based nano-electronics\ndevices. Motivated by this, a comparative study of strain-dependent phonon\ntransport of AsP monolayer is performed by solving the linearized phonon\nBoltzmann equation within the single-mode relaxation time approximation (RTA).\nIt is found that the lattice thermal conductivity ($\\kappa_L$) of AsP monolayer\nis very close to one of As monolayer with similar buckled structure, which is\ndue to neutralization between the reduce of phonon lifetimes and group velocity\nenhancement from As to AsP monolayer. The corresponding room-temperature sheet\nthermal conductance of AsP monolayer is 152.5 $\\mathrm{W K^{-1}}$. It is noted\nthat the increasing tensile strain can harden long wavelength out-of-plane (ZA)\nacoustic mode, and soften the in-plane longitudinal acoustic (LA) and\ntransversal acoustic (TA) modes. Calculated results show that $\\kappa_L$ of AsP\nmonolayer presents a nonmonotonic up-and-down behavior with increased strain.\nThe unusual strain dependence is due to the competition among reduce of phonon\ngroup velocities, improved phonon lifetimes of ZA mode and nonmonotonic\nup-and-down phonon lifetimes of TA/LA mode. It is found that acoustic branches\ndominate the $\\kappa_L$ in considered strain range, and the contribution from\nZA branch increases with increased strain, while it is opposite for TA/LA\nbranch. By analyzing cumulative $\\kappa_L$ with respect to phonon mean free\npath (MFP), tensile strain can modulate effectively size effects on $\\kappa_L$\nin AsP monolayer."}, {"title": "Deep sub-Ångstrom imaging of 2D materials with a high dynamic range detector", "abstract": "Aberration-corrected optics have made electron microscopy at\natomic-resolution a widespread and often essential tool for\nnanocharacterization. Image resolution is dominated by beam energy and the\nnumerical aperture of the lens ({\\alpha}), with state-of-the-art reaching ~0.47\n{\\AA} at 300 keV. Two-dimensional materials are imaged at lower beam energies\nto avoid knock-on damage, limiting spatial resolution to ~1 {\\AA}. Here, by\ncombining a new electron microscope pixel array detector with the dynamic range\nto record the complete distribution of transmitted electrons and full-field\nptychography to recover phase information from the full phase space, we\nincreased the spatial resolution well beyond the traditional lens limitations.\nAt 80 keV beam energy, our ptychographic reconstructions significantly improved\nimage contrast of single-atom defects in MoS2, reaching an information limit\nclose to 5{\\alpha}, corresponding to a 0.39 {\\AA} Abbe resolution, at the same\ndose and imaging conditions where conventional imaging modes reach only 0.98\n{\\AA}."}, {"title": "Electronic Structure of the Dilute Magnetic Semiconductor $Ga_{1-x}Mn_xP$ from Hard X-ray Photoelectron Spectroscopy and Hard X-ray Angle-Resolved Photoemission", "abstract": "We have investigated the electronic structure of the dilute magnetic\nsemiconductor (DMS) $Ga_{0.98}Mn_{0.02}P$ and compared it to that of an undoped\n$GaP$ reference sample, using hard X-ray photoelectron spectroscopy (HXPS) and\nhard X-ray angle-resolved photoemission spectroscopy (HARPES) at energies of\nabout 3 keV. We present experimental data, as well as theoretical calculations,\nin order to understand the role of the Mn dopant in the emergence of\nferromagnetism in this material. Both core-level spectra and angle-resolved or\nangle-integrated valence spectra are discussed. In particular, the HARPES\nexperimental data are compared to free-electron final-state model calculations\nand to more accurate one-step photoemission theory. The experimental results\nshow differences between $Ga_{0.98}Mn_{0.02}P$ and $GaP$ in both angle-resolved\nand angle-integrated valence spectra. The $Ga_{0.98}Mn_{0.02}P$ bands are\nbroadened due to the presence of Mn impurities that disturb the long-range\ntranslational order of the host $GaP$ crystal. Mn-induced changes of the\nelectronic structure are observed over the entire valence band range, including\nthe presence of a distinct impurity band close to the valence-band maximum of\nthe DMS. These experimental results are in good agreement with the one-step\nphotoemission calculations, and a prior HARPES study of $Ga_{0.97}Mn_{0.03}As$\nand $GaAs$ (Gray et al. Nature Materials 11, 957 (2012)), demonstrating the\nstrong similarity between these two materials. The Mn 2p and 3s core-level\nspectra also reveal an essentially identical state in doping both $GaAs$ and\n$GaP$."}, {"title": "Role of Water Molecule in Enhancing the Proton Conductivity on Graphene Oxide at Humidity Condition", "abstract": "Recent experimental reports on in-plane proton conduction in reduced graphene\noxide (rGO) films open a new way for the design of proton exchange membrane\nessential in fuel cells and chemical filters. At high humidity condition, water\nmolecules attached on the rGO sheet are expected to play a critical role, but\ntheoretical works for such phenomena have been scarcely found in the\nliterature. In this study, we investigate the proton migration on\nwater-adsorbed monolayer and bilayer rGO sheets using first-principles\ncalculations in order to reveal the mechanism. We devise a series of models for\nthe water-adsorbed rGO films as systematically varying the reduction degree and\nwater content, and optimize their atomic structures in reasonable agreement\nwith the experiment, using a density functional that accounts for van der Waals\ncorrection. Upon suggesting two different transport mechanisms, epoxy-mediated\nand water-mediated hoppings, we determine the kinetic activation barriers for\nthese in-plane proton transports on the rGO sheets. Our calculations indicate\nthat the water-mediated transport is more likely to occur due to its much lower\nactivation energy than the epoxy-mediated one and reveal new prospects for\ndeveloping efficient solid proton conductors."}, {"title": "Weighting graphene with QCM to monitor interfacial mass changes", "abstract": "In this Letter, using quartz crystal microbalance (QCM), we experimentally\ndetermined the mass density of graphene grown by chemical vapor deposition\nmethod. We developed a transfer printing technique to integrate large area\nsingle-layer graphene on QCM. By monitoring the resonant frequency of an\noscillating quartz crystal loaded with graphene, we were able to measure the\nmass density of graphene as ~118 ng/cm2, which is significantly larger than the\nideal graphene (~76 ng/cm2) mainly due to the presence of wrinkles and\norganic/inorganic residues on graphene sheets. High sensitivity of quartz\ncrystal resonator allowed us to determine the number of graphene layers in\nsamples. (The technique is very sensitive and able to determine the number of\ngraphene layers in a particular sample.) Besides, we extended our technique to\nprobe interfacial mass variation during adsorption of biomolecules on graphene\nsurface, and plasma-assisted oxidation of graphene. (Besides, we extended our\ntechnique to probe interfacial mass variation during adsorption of biomolecules\non graphene surface, and plasma-assisted oxidation of graphene.)"}, {"title": "Photovoltaic effect and photopolarization in Pb[(Mg1/3Nb2/3)0.68Ti0.32]O3 crystal", "abstract": "Ferroelectric photovoltaic materials are an alternative to\nsemiconductor-based photovoltaics and offer the advantage of above bandgap\nphotovoltage generation. However, there are few known compounds, and\nphotovoltaic efficiencies remain low. Here, we report the discovery of a\nphotovoltaic effect in undoped lead magnesium niobate-lead titanate crystal and\na significant improvement in the photovoltaic response under suitable electric\nfields and temperatures. The photovoltaic effect is maximum near the\nelectric-field-driven ferroelectric dipole reorientation, and increases\nthreefold near the Curie temperature. Moreover, at ferroelectric saturation,\nthe photovoltaic response exhibits clear remanent and transient effects. The\ntransient-remanent combinations together with electric and thermal tuning\npossibilities indicate photoferroelectric crystals as emerging elements for\nphotovoltaics and optoelectronics, relevant to all-optical information storage\nand beyond."}, {"title": "Reversal magnetization, spin reorientation and exchange bias in YCrO$_3$ doped with praseodymium", "abstract": "Crystal structure, thermal and magnetic properties were systematically\nstudied in the Y$_{1-x}$Pr$_x$CrO$_3$ with $0 \\leq x \\leq 0.3$ compositions.\nMagnetic susceptibility and specific heat measurements show an increase of the\nantiferromagnetic transition temperature ($T_N$) as Pr is substituted in the Y\nsites and notable magnetic features are observed below $T_N$. Strong coupling\nbetween magnetic and crystalline parameters is observed in a small range of Pr\ncompositions. A small perturbation in the lattice parameters by Pr ion is\nsufficient to induce a spin reorientation transition followed by magnetization\nreversal, to finally induce exchange bias effect. The spin reorientation\ntemperature ($T_{SR}$) is increased from 35 K to 149 K for $0.025 \\leq x \\leq\n0.1$ compositions. It is found that the Cr spins sublattice rotates\ncontinuously from $T_{SR}$ to a new spin configuration a lower temperature. In\naddition, magnetization reversal is observed at $T^* \\sim 35$ K for x= 0.05 up\nto $T^* \\sim 63$ K for x = 0.20 composition. The $M-H$ curves show negative\nexchange bias effect induced by Pr ions, which are observed below of 100 K and\nbeing more intense at 5 K. At 10 K, the magnetic contribution of the specific\nheat, as well as the ZFC magnetization, show the rise of a peak with increasing\nPr content. The magnetic anomaly could be associated with the freezing of the\nPr magnetic moment randomly distributed at the 4c crystallographic site. A\nclear correspondence between spin reorientation, magnetization reversal and\nexchange bias anisotropy with the tilting and octahedral distortion is also\ndiscussed."}, {"title": "Constitutive modeling of some 2D crystals: graphene, hexagonal BN, MoS$_2$, WSe$_2$ and NbSe$_2$", "abstract": "We lay down a nonlinear elastic constitutive framework for the modeling of\nsome 2D crystals of current interest. The 2D crystals we treat are graphene,\nhexagonal boron nitride and some metal dichalcogenides: molybdenium disulfide\n(MoS$_2$), tungsten selenium (WSe$_2$), and niobium diselenide (NbSe$_2$). We\nfirst find their arithmetic symmetries by using the theory of monoatomic and\ndiatomic 2-nets. Then, by confinement to weak transformation neighbourhoods and\nby applying the Cauchy-Born rule we are able to use the symmetries continuum\nmechanics utilizes: geometric symmetries. We give the complete and irreducible\nrepresentation for energies depending on an in-plane measure, the curvature\ntensor and the shift vector. This is done for the symmetry hierarchies that\ndescribe how symmetry changes at the continuum level: $\\mathcal C_{6 \\nu}\n\\rightarrow \\mathcal C_{2 \\nu} \\rightarrow \\mathcal C_1$ for monoatomic 2-nets\nand $\\mathcal C_{6 \\nu} \\rightarrow \\mathcal C_{1 \\nu} \\rightarrow \\mathcal\nC_1$ for diatomic two nets. Having these energies at hand we are able to\nevaluate stresses and couple stresses for each symmetry regime."}, {"title": "Graphene as a hexagonal 2-lattice: evaluation of the in-plane material constants for the linear theory. A multiscale approach", "abstract": "Continuum modeling of free-standing graphene monolayer, viewed as a two\ndimensional 2-lattice, requires specification of the components of the shift\nvector that acts as an auxiliary variable. If only in-plane motions are\nconsidered the energy depends on an in-plane strain measure and the shift\nvector. The assumption of geometrical and material linearity leads to quadratic\nenergy terms with respect to the shift vector, the strain tensor, and their\ncombinations. Graphene's hexagonal symmetry reduces the number of independent\nmoduli then to four. We evaluate these four material parameters using molecular\ncalculations and the AIREBO potential and compare them with standard linear\nelastic constitutive modeling. The results of our calculations show that the\npredicted values are in reasonable agreement with those obtained solely from\nour molecular calculations as well as those from literature. To the best of our\nknowledge, this is the first attempt to measure mechanical properties when\ngraphene is modeled as a hexagonal 2-lattice. This work targets at the\ncontinuum scale when the insight measurements comes from finer scales using\natomistic simulations."}, {"title": "Deep analytics of atomically-resolved images: manifest and latent features", "abstract": "Recent advances in scanning transmission electron and scanning tunneling\nmicroscopies allow researchers to measure materials structural and electronic\nproperties, such as atomic displacements and charge density modulations, at an\nAngstrom scale in real space. At the same time, the ability to quickly acquire\nlarge, high-resolution datasets has created a challenge for rapid physics-based\nanalysis of images that typically contain several hundreds to several thousand\natomic units. Here we demonstrate a universal deep-learning based framework for\nlocating and characterizing atomic species in the lattice, which can be applied\nto different types of atomically resolved measurements on different materials.\nSpecifically, by inspecting and categorizing features in the output layer of a\nconvolutional neural network, we are able to detect structural and electronic\n'anomalies' associated with the presence of point defects in a tungsten\ndisulfide monolayer, non-uniformity of the charge density distribution around\nspecific lattice sites on the surface of strongly correlated oxides, and\ntransition between different structural states of buckybowl molecules. We\nfurther extended our method towards tracking, from one image frame to another,\nminute distortions in the geometric shape of individual Si dumbbells in a\n3-dimensional Si sample, which are associated with a motion of lattice defects\nand impurities. Due the applicability of our framework to both scanning\ntunneling microscopy and scanning transmission electron microscopy\nmeasurements, it can provide a fast and straightforward way towards creating a\nunified database of defect-property relationships from experimental data for\neach material."}, {"title": "An analysis of the influence of grain size on the strength of FCC polycrystals by means of computational homogenization", "abstract": "The effect of grain size on the flow stress of FCC polycrystals is analyzed\nby means of a multiscale strategy based on computational homogenization of the\npolycrystal aggregate. The mechanical behavior of each crystal is given by a\ndislocation-based crystal plasticity model in which the critical resolved shear\nstress follows the Taylor model. The generation and annihilation of\ndislocations in each slip system during deformation is given by the\nKocks-Mecking model, which was modified to account for the dislocation storage\nat the grain boundaries. Polycrystalline Cu is selected to validate the\nsimulation strategy and all the model parameters are obtained from dislocation\ndynamics simulations or experiments at lower length scales and the simulation\nresults were in good agreement with experimental data in the literature. The\nmodel is applied to explore the influence of different microstructural factors\n(initial dislocation density, width of the grain size distribution, texture) on\nthe grain size effect. It is found that the initial dislocation density,\n$\\rho_i$, plays a dominant role in the magnitude of the grain size effect and\nthat dependence of flow stress with an inverse power of grain size ($\\sigma_ y\n-\\sigma_\\infty \\propto d_g^{-x}$) breaks down for large initial dislocation\ndensities ($> 10^{14}$ m$^{-2}$) and grain sizes $d_g > $ 40 $\\mu$m in FCC\nmetals. However, it was found that the grain size contribution to the strength\nfollowed a power-law function of the dimensionless parameter $d_g\\sqrt{\\rho_i}$\nfor small values of the applied strain ($<$ 2 \\%), in agreement with previous\ntheoretical considerations for size effects in plasticity."}, {"title": "Curvature dependent surface energy for free standing monolayer graphene: geometrical and material linearization with closed form solutions", "abstract": "Continuum modeling of a free-standing graphene monolayer, viewed as a two\ndimensional 2-lattice, requires specifications of the components of the shift\nvector that act as an auxiliary variable. The field equations are then the\nequations ruling the shift vector, together with momentum and moment of\nmomentum equations. To introduce material linearity energy is assumed to have a\nquadratic dependence on the strain tensor, the curvature tensor, the shift\nvector, as well as to combinations of them. Hexagonal symmetry then reduces the\noverall number of independent material constants to nine. We present an\nanalysis of simple loading histories such as axial, biaxial tension/compression\nand simple shear for a range of problems of increasing difficulty for the\ngeometrically and materially linear case. We start with the problem of in-plane\nmotions only. By prescribing the displacement, the components of the shift\nvector are evaluated. This way the field equations are satisfied trivially.\nOut-of-plane motions are treated as well; we assume in-plane\ntension/compression that leads to buckling/wrinkling and solve for the\ncomponents of the shift vector as well as the function present in buckling's\nmodeling. The assumptions of linearity adopted here simplifies the analysis and\nfacilitates analytical results."}, {"title": "Non-Gaussian diffusion profiles caused by mobile impurity-vacancy pairs in the five frequency model of diffusion", "abstract": "Vacancy-mediated diffusion of impurities under strong impurity-vacancy (I-v)\nattraction has been studied in the framework of the five-frequency model (5FM)\nfor the FCC host. The system of impurities and tightly bound I-v pairs has been\ntreated in the framework of the rate-equations approach of Cowern et al., Phys.\nRev. Lett. 65, 2434 (1990), developed for the description of the non-Gaussian\ndiffusion profiles (NGDPs) observed in dopant diffusion in silicon. In the\npresent study this approach has been extended to derive a three-dimensional\n(3D) integro-differential equation describing the pair-mediated impurity\ndiffusion. The equation predicts the same 1D NGDPs as in Cowern et al. but can\nbe also used for the simulation of 3D profiles of arbitrary geometry in the\nsystems where the diffusion proceeds via a mobile state. The parameters of the\ntheory has been calculated within the 5FM on the basis of available literature\ndata. The database on impurities in aluminum host has been analyzed and\npromising impurity-host systems for the observation of NGDPs has been\nidentified. The diffusion profiles for an impurity where NGDPs are expected to\nbe easily detectable have been simulated. It has been argued that with the\ninput parameters calculated on the basis of experimental diffusion constants\nthe simulated NGDPs can be accurate enough to serve as a quantitative test of\nthe 5FM."}, {"title": "Iron single crystal growth from a lithium-rich melt", "abstract": "\\alpha-Fe single crystals of rhombic dodecahedral habit were grown from a\nmelt of Li$_{84}$N$_{12}$Fe$_{\\sim 3}$. Crystals of several millimeter along a\nside form at temperatures around $T \\approx 800^\\circ$C. Upon further cooling\nthe growth competes with the formation of Fe-doped Li$_3$N. The b.c.c.\nstructure and good sample quality of \\alpha-Fe single crystals were confirmed\nby X-ray and electron diffraction as well as magnetization measurements and\nchemical analysis. A nitrogen concentration of 90\\,ppm was detected by means of\ncarrier gas hot extraction. Scanning electron microscopy did not reveal any\nsign of iron nitride precipitates."}, {"title": "Consistent determination of geometrically necessary dislocation density from simulations and experiments", "abstract": "The use of Nye's dislocation tensor for calculating the density of\ngeometrically necessary dislocations (GND) is widely adopted in the study of\nplastically deformed materials. The curl operation involved in finding the Nye\ntensor, while conceptually straightforward has been marred with inconsistencies\nand several different definitions are in use. For the three most common\ndefinitions, we show that their consistent application leads to the same\nresult. To eliminate frequently encountered confusion, a summary of expressions\nfor Nye's tensor in terms of elastic and plastic deformation gradient, and for\nboth small and large deformations, is presented. A further question when\nestimating GND density concerns the optimization technique used to solve the\nunder-determined set of equations linking Nye's tensor and GND density. A\nsystematic comparison of the densities obtained by two widely used techniques,\nL1 and L2 minimisation, shows that both methods yield remarkably similar total\nGND densities. Thus the mathematically simpler, L2, may be preferred over L1\nexcept when information about the distribution of densities on specific slip\nsystems is required. To illustrate this, we compare experimentally measured\nlattice distortions beneath nano-indents in pure tungsten, probed using\n3D-resolved synchrotron X-ray micro-diffraction, with those predicted by 3D\nstrain-gradient crystal plasticity finite element calculations. The results are\nin good agreement and show that the volumetric component of the elastic strain\nfield has a surprisingly small effect on the determined Nye tensor. This is\nimportant for experimental techniques, such as micro-beam Laue measurements and\nHR-EBSD, where only the deviatoric strain component is measured."}, {"title": "Non-uniform plastic deformations of crystals undergoing anti-plane constrained shear", "abstract": "The present paper studies non-uniform plastic deformations of crystals\nundergoing anti-plane constrained shear. The asymptotically exact energy\ndensity of crystals containing a moderately large density of excess\ndislocations is found by the averaging procedure. This energy density is\nextrapolated to the cases of extremely small or large dislocation densities. By\nincorporating the configurational temperature and the density of redundant\ndislocations, we develop the thermodynamic dislocation theory for non-uniform\nplastic deformations and use it to predict the stress-strain curves and the\ndislocation densities."}, {"title": "Quasi-static and Dynamic Behavior of Additively Manufactured Metallic Lattice Cylinders", "abstract": "Lattice structures have tailorable mechanical properties which allows them to\nexhibit superior mechanical properties (per unit weight) beyond what is\nachievable through natural materials. In this paper, quasi-static and dynamic\nbehavior of additively manufactured stainless steel lattice cylinders is\nstudied. Cylindrical samples with internal lattice structure are fabricated by\na laser powder bed fusion system. Equivalent hollow cylindrical samples with\nthe same length, outer diameter, and mass (larger wall thickness) are also\nfabricated. Split Hopkinson bar is used to study the behavior of the specimens\nunder high strain rate loading. It is observed that lattice cylinders reduce\nthe transmitted wave amplitude up to about 21% compared to their equivalent\nhollow cylinders. However, the lower transmitted wave energy in lattice\ncylinders comes at the expense of a greater reduction in their stiffness, when\ncompared to their equivalent hollow cylinder. In addition, it is observed that\nincreasing the loading rate by five orders of magnitude leads to up to about\n36% increase in the peak force that the lattice cylinder can carry, which is\nattributed to strain rate hardening effect in the bulk stainless steel\nmaterial. Finite element simulations of the specimens under dynamic loads are\nperformed to study the effect of strain rate hardening, thermal softening, and\nthe failure mode on dynamic behavior of the specimens. Numerical results are\ncompared with experimental data and good qualitative agreement is observed."}, {"title": "Solution to the hole-doping problem and tunable quantum Hall effect in Bi$_{2}$Se$_{3}$ thin films", "abstract": "Bi$_{2}$Se$_{3}$, one of the most widely studied topological insulators\n(TIs), is naturally electron-doped due to n-type native defects. However, many\nyears of efforts to achieve p-type Bi$_{2}$Se$_{3}$ thin films have failed so\nfar. Here, we provide a solution to this long-standing problem, showing that\nthe main culprit has been the high density of interfacial defects. By\nsuppressing these defects through an interfacial engineering scheme, we have\nsuccessfully implemented p-type Bi$_{2}$Se$_{3}$ thin films down to the\nthinnest topological regime. On this platform, we present the first tunable\nquantum Hall effect (QHE) study in Bi$_{2}$Se$_{3}$ thin films, and reveal not\nonly significantly asymmetric QHE signatures across the Dirac point but also\nthe presence of competing anomalous states near the zeroth Landau level. The\navailability of doping tunable Bi$_{2}$Se$_{3}$ thin films will now make it\npossible to implement various topological quantum devices, previously\ninaccessible."}, {"title": "Characterization of free standing InAs quantum membranes by standing wave hard x-ray photoemission spectroscopy", "abstract": "Free-standing nanoribbons of InAs quantum membranes (QMs) transferred onto a\n(Si/Mo) multilayer mirror substrate are characterized by hard x-ray\nphotoemission spectroscopy (HXPS), and by standing-wave HXPS (SW-HXPS).\nInformation on the chemical composition and on the chemical states of the\nelements within the nanoribbons was obtained by HXPS and on the quantitative\ndepth profiles by SW-HXPS. By comparing the experimental SW-HXPS rocking curves\nto x-ray optical calculations, the chemical depth profile of the InAs(QM) and\nits interfaces were quantitatively derived with angstrom precision. We\ndetermined that: i) the exposure to air induced the formation of an InAsO$_4$\nlayer on top of the stoichiometric InAs(QM); ii) the top interface between the\nair-side InAsO$_4$ and the InAs(QM) is not sharp, indicating that\ninterdiffusion occurs between these two layers; iii) the bottom interface\nbetween the InAs(QM) and the native oxide SiO$_2$ on top of the (Si/Mo)\nsubstrate is abrupt. In addition, the valence band offset (VBO) between the\nInAs(QM) and the SiO$_2$/(Si/Mo) substrate was determined by HXPS. The value of\n$VBO = 0.2 \\pm 0.04$ eV is in good agreement with literature results obtained\nby electrical characterization, giving a clear indication of the formation of a\nwell-defined and abrupt InAs/SiO$_2$ heterojunction. We have demonstrated that\nHXPS and SW-HXPS are non-destructive, powerful methods for characterizing\ninterfaces and for providing chemical depth profiles of nanostructures, quantum\nmembranes, and 2D layered materials."}, {"title": "Rashba-Dresselhaus Effect in Inorganic/Organic Lead Iodide Perovskite Interfaces", "abstract": "Despite the imperative importance in solar-cell efficiency, the intriguing\nphenomena at the interface between perovskite solar-cell and adjacent carrier\ntransfer layers are hardly uncovered. Here we show that PbI$_2$/AI-terminated\nlead-iodide-perovskite (APbI$_3$; A=Cs$^+$/ methylammonium(MA)) interfaced with\nthe charge transport medium of graphene or TiO2 exhibits the sizable/robust\nRashba-Dresselhaus (RD) effect using density-functional-theory and ab initio\nmolecular dynamics (AIMD) simulations above cubic-phase temperature. At the\nPbI$_2$-terminated graphene/CsPbI3(001) interface, ferroelectric distortion\ntowards graphene facilitates an inversion breaking field. At the MAI-terminated\nTiO$_2$/MAPbI$_3$(001) interface, the enrooted alignment of MA$^+$ towards\nTiO$_2$ by short-strong hydrogen-bonding and the concomitant PbI$_3$ distortion\npreserve the RD interactions even above 330 K. The robust RD effect at the\ninterface even at high temperatures, unlike in bulk, changes the direct-type\nband to the indirect to suppress recombination of electron and hole, thereby\nletting these accumulated carriers overcome the potential barrier between\nperovskite and charge transfer materials, which promotes the solar-cell\nefficiency."}, {"title": "In-Situ Neutron Diffraction and Crystal Plasticity Finite Element Modeling to study the Kinematic Stability of Retained Austenite in Bearing Steels", "abstract": "This work integrates in-situ neutron diffraction and crystal plasticity\nfinite element modeling to study the kinematic stability of retained austenite\nin high carbon bearing steels. The presence of a kinematically metastable\nretained austenite in bearing steels can significantly affect the\nmacro-mechanical and micro-mechanical material response. Mechanical\ncharacterization of metastable austenite is a critical component in accurately\ncapturing the micro-mechanical behavior under typical application loads.\nTraditional mechanical characterization techniques are unable to discretely\nquantify the micro-mechanical response of the austenite, and as a result, the\ncomputational predictions rely heavily on trial and error or qualitative\ndescriptions of the austenite phase. In order to overcome this, in this present\nwork, we use in-situ neutron diffraction of a uniaxial tension test of an A485\nGrade 1 bearing steel specimen. The mechanical response determined from the\nneutron diffraction analysis was incorporated into a hybrid crystal plasticity\nfinite element model that accounts for the martensite's crystal plasticity and\nthe stress-assisted transformation from austenite to martensite in bearing\nsteels. The modeling response was used to estimate the single crystal elastic\nconstants of the austenite and martensite phases. The results show that using\nin-situ neutron diffraction coupled with a crystal plasticity model, can\nsuccessfully predict both the micro-mechanical and macro-mechanical responses\nof bearing steels while accounting for the martensitic transformation of the\nretained austenite."}, {"title": "Influence of topology in the mobility enhancement of pulse-coupled oscillator synchronization", "abstract": "In this work we revisit the nonmonotonic behavior (NMB) of synchronization\ntime with velocity reported for systems of mobile pulse-coupled oscillators\n(PCOs). We devise a control parameter that allows us to predict in which range\nof velocities NMB may occur, also uncovering the conditions allowing us to\nestablish the emergence of NMB based on specific features of the connectivity\nrule. Specifically, our results show that if the connectivity rule is such that\nthe interaction patterns are sparse and, more importantly, include a large\nfraction of non reciprocal interactions, then the system will display NMB. We\nfurthermore provide a microscopic explanation relating the presence of such\nfeatures of the connectivity patterns to the existence of local clusters unable\nto synchronize, termed frustrated clusters, for which we also give a precise\ndefinition in terms of simple graph concepts. We conclude that, if the\nprobability of finding a frustrated cluster in a system of moving PCOs is high\nenough, NMB occurs in a predictable range of velocities."}, {"title": "External and mutual synchronization of chimeras in a two layer network of nonlinear oscillators", "abstract": "We study numerically synchronization phenomena of spatiotemporal structures,\nincluding chimera states, in a two layer network of nonlocally coupled\nnonlinear chaotic discrete-time systems. Each of the interacting ensembles\nrepresents a one layer ring network of nonlocally coupled logistic maps in the\nchaotic regime. The coupled networks differ in their control parameters that\nenables one to observe distinct spatiotemporal dynamics in the networks when\nthere is no coupling between them. We explore in detail external and mutual\nsynchronization of chimera structures. The identity of synchronous structures\nand the estimation of synchronization regions are quantified by calculating the\ncross-correlation coefficient between relevant oscillators of the networks. We\nshow that for non-identical networks, unidirectional and symmetric couplings\nlead to external and mutual synchronization between the interacting ensembles,\nrespectively. This is confirmed by identical synchronous structures and by the\nexistence of finite regions of synchronization within which the\ncross-correlation coefficient is equal to 1. We also show that these findings\nare qualitatively equivalent to the results of the classical synchronization\ntheory of periodic self-sustained oscillations."}, {"title": "Global synchronization of partially forced Kuramoto oscillators on Networks", "abstract": "We study the synchronization of Kuramoto oscillators on networks where only a\nfraction of them is subjected to a periodic external force. When all\noscillators receive the external drive the system always synchronize with the\nperiodic force if its intensity is sufficiently large. Our goal is to\nunderstand the conditions for global synchronization as a function of the\nfraction of nodes being forced and how these conditions depend on network\ntopology, strength of internal couplings and intensity of external forcing.\nNumerical simulations show that the force required to synchronize the network\nwith the external drive increases as the inverse of the fraction of forced\nnodes. However, for a given coupling strength, synchronization does not occur\nbelow a critical fraction, no matter how large is the force. Network topology\nand properties of the forced nodes also affect the critical force for\nsynchronization. We develop analytical calculations for the critical force for\nsynchronization as a function of the fraction of forced oscillators and for the\ncritical fraction as a function of coupling strength. We also describe the\ntransition from synchronization with the external drive to spontaneous\nsynchronization."}, {"title": "Twisted states in low-dimensional hypercubic lattices", "abstract": "Twisted states with non-zero winding numbers composed of sinusoidally coupled\nidentical oscillators have been observed in a ring. The phase of each\noscillator in these states constantly shifts, following its preceding neighbor\nin a clockwise direction, and the summation of such phase shifts around the\nring over $2\\pi$ characterizes the winding number of each state. In this work,\nwe consider finite-sized $d$-dimensional hypercubic lattices, namely square\n($d=2$) and cubic ($d=3$) lattices with periodic boundary conditions. For\nidentical oscillators, we observe new states in which the oscillators belonging\nto each line (plane) for $d=2$ ($d=3$) are phase synchronized with non-zero\nwinding numbers along the perpendicular direction. These states can be reduced\ninto twisted states in a ring with the same winding number if we regard each\nsubset of phase-synchronized oscillators as one single oscillator. For\nnonidentical oscillators with heterogeneous natural frequencies, we observe\nsimilar patterns with slightly heterogeneous phases in each line $(d=2)$ and\nplane $(d=3)$. We show that these states generally appear for random\nconfigurations when the global coupling strength is larger than the critical\nvalues for the states."}, {"title": "Stable amplitude chimera states in a network of locally coupled Stuart-Landau oscillators", "abstract": "We investigate the occurrence of collective dynamical states such as\ntransient amplitude chimera, stable amplitude chimera and imperfect breathing\nchimera states in a \\textit{locally coupled} network of Stuart-Landau\noscillators. In an imperfect breathing chimera state, the synchronized group of\noscillators exhibits oscillations with large amplitudes while the\ndesynchronized group of oscillators oscillates with small amplitudes and this\nbehavior of coexistence of synchronized and desynchronized oscillations\nfluctuates with time. Then we analyze the stability of the amplitude chimera\nstates under various circumstances, including variations in system parameters\nand coupling strength, and perturbations in the initial states of the\noscillators. For an increase in the value of the system parameter, namely the\nnonisochronicity parameter, the transient chimera state becomes a stable\nchimera state for a sufficiently large value of coupling strength. In addition,\nwe also analyze the stability of these states by perturbing the initial states\nof the oscillators. We find that while a small perturbation allows one to\nperturb a large number of oscillators resulting in a stable amplitude chimera\nstate, a large perturbation allows one to perturb a small number of oscillators\nto get a stable amplitude chimera state. We also find the stability of the\ntransient and stable amplitude chimera states as well as traveling wave states\nfor appropriate number of oscillators using Floquet theory. In addition, we\nalso find the stability of the incoherent oscillation death states."}, {"title": "Distinct collective states due to the trade-off between attractive and repulsive couplings", "abstract": "We investigate the effect of repulsive coupling together with an attractive\ncoupling in a network of nonlocally coupled oscillators. To understand the\ncomplex interaction between these two couplings we introduce a control\nparameter in the repulsive coupling which plays a crucial role in inducing\ndistinct complex collective patterns. In particular, we show the emergence of\nvarious cluster chimera death states through a dynamically distinct transition\nroute, namely the oscillatory cluster state and coherent oscillation death\nstate as a function of the repulsive coupling in the presence of the attractive\ncoupling. In the oscillatory cluster state, the oscillators in the network are\ngrouped into two distinct dynamical states of homogeneous and inhomogeneous\noscillatory states. Further, the network of coupled oscillators follows the\nsame transition route in the entire coupling range. Depending upon distinct\ncoupling ranges the system displays a different number of clusters in the death\nstate and oscillatory state. We also observe that the number of coherent\ndomains in the oscillatory cluster state exponentially decreases with increase\nin coupling range and obeys a power law decay. Additionally, we show analytical\nstability for observed solitary state, synchronized state, and incoherent\noscillation death state."}, {"title": "Stochastic oscillations produce dragon king avalanches in self-organized quasi-critical systems", "abstract": "In the last decade, several models with network adaptive mechanisms (link\ndeletion-creation, dynamic synapses, dynamic gains) have been proposed as\nexamples of self-organized criticality (SOC) to explain neuronal avalanches.\nHowever, all these systems present stochastic oscillations hovering around the\ncritical region that are incompatible with standard SOC. This phenomenology has\nbeen called self-organized quasi-criticality (SOqC). Here we make a linear\nstability analysis of the mean field fixed points of two SOqC systems: a fully\nconnected network of discrete time stochastic spiking neurons with firing rate\nadaptation produced by dynamic neuronal gains and an excitable cellular\nautomata with depressing synapses. We find that the fixed point corresponds to\na stable focus that loses stability at criticality. We argue that when this\nfocus is close to become indifferent, demographic noise can elicit stochastic\noscillations that frequently fall into the absorbing state. This mechanism\ninterrupts the oscillations, producing both power law avalanches and dragon\nking events, which appear as bands of synchronized firings in raster plots. Our\napproach differs from standard SOC models in that it predicts the coexistence\nof these different types of neuronal activity."}, {"title": "The Dynamics of Interacting Swarms", "abstract": "Swarms are self-organized dynamical coupled agents which evolve from simple\nrules of communication. They are ubiquitous in nature, and be- coming more\nprominent in defense applications. Here we report on a preliminary study of\nswarm collisions for a swarm model in which each agent is self-propelling but\nglobally communicates with other agents. We generalize previous models by\ninvestigating the interacting dynamics when delay is introduced to the\ncommunicating agents. One of our major find- ings is that interacting swarms\nare far less likely to flock cohesively if they are coupled with delay. In\naddition, parameter ranges based on coupling strength, incidence angle of\ncollision, and delay change dramatically for other swarm interactions which\nresult in flocking, milling, and scattering."}, {"title": "Nonlocal coupling among oscillators mediated by a slowly diffusing substance", "abstract": "A general theory is presented for the coupling among nonlinear oscillators\nmediated by a diffusing chemical substance. We extend a model originally\ndeveloped by Kuramoto, who supposed that the diffusion characteristic time is\nmuch shorter than the oscillator main period, such that diffusion occurs very\nfast. We eliminate this constraint and consider diffusion to have an arbitrary\ncharacteristic time, by solving exactly the diffusion equation using suitable\nGreen functions. We present results in one, two and three dimension, with and\nwithout boundary conditions."}, {"title": "Synchronization Dynamics in the Presence of Coupling Delays and Phase Shifts", "abstract": "In systems of coupled oscillators, the effects of complex signaling can be\ncaptured by time delays and phase shifts. Here, we show how time delays and\nphase shifts lead to different oscillator dynamics and how synchronization\nrates can be regulated by substituting time delays by phase shifts at constant\ncollective frequency. For spatially extended systems with time delays, we show\nthat fastest synchronization can occur for intermediate wavelengths, giving\nrise to novel synchronization scenarios."}, {"title": "Stabilisation of dynamics of oscillatory systems by non-autonomous perturbation", "abstract": "Synchronisation and stability under periodic oscillatory driving are\nwell-understood, but little is known about the effects of aperiodic driving,\ndespite its abundance in nature. Here, we consider oscillators subject to\ndriving with slowly varying frequency, and investigate both short-term and\nlong-term stability properties. For a phase oscillator, we find that,\ncounter-intuitively, such variation is guaranteed to enlarge the Arnold tongue\nin parameter space. Using analytical and numerical methods that provide\ninformation on time-variable dynamical properties, we find that the growth of\nthe Arnold tongue is specifically due to the growth of a region of intermittent\nsynchronisation where trajectories alternate between short-term stability and\nshort-term neutral stability, giving rise to stability on average. We also\npresent examples of higher-dimensional nonlinear oscillators where a similar\nstabilisation phenomenon is numerically observed. Our findings help support the\ncase that in general, deterministic non-autonomous perturbation is a very good\ncandidate for stabilising complex dynamics."}, {"title": "Structure and dynamical behaviour of non-normal networks", "abstract": "We analyse a collection of empirical networks in a wide spectrum of\ndisciplines and show that strong non-normality is ubiquitous in network\nscience. Dynamical processes evolving on non-normal networks exhibit a peculiar\nbehaviour, as initial small disturbances may undergo a transient phase and be\nstrongly amplified in linearly stable systems. Additionally, eigenvalues may\nbecome extremely sensible to noise, and have a diminished physical meaning. We\nidentify structural properties of networks that are associated to non-normality\nand propose simple models to generate networks with a tuneable level of\nnon-normality. We also show the potential use of a variety of metrics capturing\ndifferent aspects of non-normality, and propose their potential use in the\ncontext of the stability of complex ecosystems."}, {"title": "Finite-time scaling in local bifurcations", "abstract": "Finite-size scaling is a key tool in statistical physics, used to infer\ncritical behavior in finite systems. Here we use the analogous concept of\nfinite-time scaling to describe the bifurcation diagram at finite times in\ndiscrete dynamical systems. We analytically derive finite-time scaling laws for\ntwo ubiquitous transitions given by the transcritical and the saddle-node\nbifurcation, obtaining exact expressions for the critical exponents and scaling\nfunctions. One of the scaling laws, corresponding to the distance of the\ndynamical variable to the attractor, turns out to be universal. Our work\nestablishes a new connection between thermodynamic phase transitions and\nbifurcations in low-dimensional dynamical systems, and opens new avenues to\nidentify the nature of dynamical shifts in systems for which only short time\nseries are available."}, {"title": "In-phase synchronization in complex oscillator networks by adaptive delayed feedback control", "abstract": "In-phase synchronization is a special case of synchronous behavior when\ncoupled oscillators have the same phases for any time moments. Such behavior\nappears naturally for nearly identical coupled limit-cycle oscillators when the\ncoupling strength is greatly above the synchronization threshold. We\ninvestigate the general class of nearly identical complex oscillators connected\ninto network in a context of a phase reduction approach. By treating each\noscillator as a black-box possessing a single-input single-output, we provide a\npractical and simply realizable control algorithm to attain the in-phase\nsynchrony of the network. For a general diffusive-type coupling law and any\nvalue of a coupling strength (even greatly below the synchronization threshold)\nthe delayed feedback control with a specially adjusted time-delays can provide\nin-phase synchronization. Such adjustment of the delay times performed in an\nautomatic fashion by the use of an adaptive version of the delayed feedback\nalgorithm when time-delays become time-dependent slowly varying control\nparameters. Analytical results show that there are many arrangements of the\ntime-delays for the in-phase synchronization, therefore we supplement the\nalgorithm by an additional requirement to choose appropriate set of the\ntime-delays, which minimize power of a control force. Performed numerical\nvalidations of the predictions highlights the usefulness of our approach."}, {"title": "Reducing the number of time delays in coupled dynamical systems", "abstract": "When several dynamical systems interact, the transmission of the information\nbetween them necessarily implies a time delay. When the time delay is not\nnegligible, the study of the dynamics of these interactions deserve a special\ntreatment. We will show here that under certain assumptions, it is possible to\nset to zero a significant amount of time-delayed connections without altering\nthe global dynamics. We will focus on graphs of interactions with identical\ntime delays and bidirectional connections. With these premises, it is possible\nto find a configuration where a number $n_z$ of time delays have been removed\nwith $n_v-1 \\leq n_z \\leq n_v^2/4$, where $n_v$ is the number of dynamical\nsystems on a connected graph."}, {"title": "Curing Braess' Paradox by Secondary Control in Power Grids", "abstract": "Robust operation of power transmission grids is essential for most of today's\ntechnical infrastructure and our daily life. Adding renewable generation to\npower grids requires grid extensions and sophisticated control actions on\ndifferent time scales to cope with short-term fluctuations and long-term power\nimbalance. Braess' paradox constitutes a counterintuitive collective phenomenon\nthat occurs if adding new transmission line capacity to a network increases\nloads on other lines, effectively reducing the system's performance and\npotentially even entirely removing its operating state. Combining simple\nanalytical considerations with numerical investigations on a small sample\nnetwork, we here study dynamical consequences of secondary control in AC power\ngrid models. We demonstrate that sufficiently strong control not only implies\ndynamical stability of the system but may also cure Braess' paradox. Our\nresults highlight the importance of demand control in conjunction with grid\ntopology for stable operation and reveal a new functional benefit of secondary\ncontrol."}, {"title": "Symmetry and symmetry breaking in coupled oscillator communities", "abstract": "With the recent development of analytical methods for studying the collective\ndynamics of coupled oscillator systems, the dynamics of communities of coupled\noscillators have received a great deal of attention in the nonlinear dynamics\ncommunity. However, the majority of these works treat systems with a number of\nsymmetries to simplify the analysis. In this work we study the role of symmetry\nand symmetry-breaking in the collective dynamics of coupled oscillator\ncommunities, allowing for a comparison between the macroscopic dynamics of\nsymmetric and asymmetric systems. We begin by treating the symmetric case,\nderiving the bifurcation diagram as a function of intra- and inter-community\ncoupling strengths. In particular we describe transitions between incoherence,\nstanding wave, and partially synchronized states and reveal bistability\nregions. When we turn our attention to the asymmetric case we find that the\nsymmetry-breaking complicates the bifurcation diagram. For instance, a\npitchfork bifurcation in the symmetric case is broken, giving rise to a Hopf\nbifurcation. Moreover, an additional partially synchronized state emerges, as\nwell as a new bistability region."}, {"title": "Power grid stability under perturbation of single nodes: Effects of heterogeneity and internal nodes", "abstract": "Non-linear equations describing the time evolution of frequencies and\nvoltages in power grids exhibit fixed points of stable grid operation. The\ndynamical behaviour after perturbations around these fixed points can be used\nto characterise the stability of the grid. We investigate both probabilities of\nreturn to a fixed point and times needed for this return after perturbation of\nsingle nodes. Our analysis is based on an IEEE test grid and the second-order\nswing equations for voltage phase angles $\\theta_j$ at nodes $j$ in the\nsynchronous machine model. The perturbations cover all possible changes\n$\\Delta\\theta$ of voltage angles and a wide range of frequency deviations in a\nrange $\\Delta f=\\pm1$~Hz around the common frequency $\\omega=2\\pi\nf=\\dot\\theta_j$ in a synchronous fixed point state. Extensive numerical\ncalculations are carried out to determine, for all node pairs $(j,k)$, the\nreturn times $t_{jk}(\\Delta\\theta,\\Delta \\omega)$ of node $k$ after a\nperturbation of node $j$. We find that for strong perturbations of some nodes,\nthe grid does not return to its synchronous state. If returning to the fixed\npoint, the times needed for the return are strongly different for different\ndisturbed nodes and can reach values up to 20 seconds and more. When\nhomogenising transmission line and node properties, the grid always returns to\na synchronous state for the considered perturbations, and the longest return\ntimes have a value of about 4 seconds for all nodes. The neglect of reactances\nbetween points of power generation (internal nodes) and injection (terminal\nnodes) leads to an underestimation of return probabilities."}, {"title": "Diverse Stochasticity Leads a Colony of Ants to Optimal Foraging", "abstract": "A mathematical model of garden ants (Laius japonicus) is introduced herein to\ninvestigate the relationship between the distribution of the degree of\nstochasticity in following pheromone trails and the group foraging efficiency.\nNumerical simulations of the model indicate that depending on the systematic\nchange of the feeding environment, the optimal distribution of stochasticity\nshifts from a mixture of almost deterministic and mildly stochastic ants to a\ncontrasted mixture of almost deterministic ants and highly stochastic ants. In\naddition, the interaction between the stochasticity and the pheromone path\nregulates the dynamics of the foraging efficiency optimization. Stochasticity\ncould strengthen the collective efficiency when stochasticity to the\nsensitivity of pheromone for ants is introduced in the model."}, {"title": "Coherence resonance in an excitable potential well", "abstract": "The excitable behaviour is considered as motion of a particle in a potential\nfield in the presence of dissipation. The dynamics of the oscillator proposed\nin the present paper corresponds to the excitable behaviour in a potential well\nunder condition of positive dissipation. Type-II excitability of the offered\nsystem results from intrinsic peculiarities of the potential well, whose shape\ndepends on a system state. Concept of an excitable potential well is\nintroduced. The effect of coherence resonance and self-oscillation excitation\nin a state-dependent potential well under condition of positive dissipation are\nexplored in numerical experiments."}, {"title": "Method of increasing the information capacity of associative memory of oscillator neural networks using high-order synchronization effect", "abstract": "Computational modelling of two- and three-oscillator schemes with thermally\ncoupled $VO_2$-switches is used to demonstrate a novel method of pattern\nstorage and recognition in an impulse oscillator neural network (ONN) based on\nthe high-order synchronization effect. The method ensures high information\ncapacity of associative memory, i.e. a large number of synchronous states\n$N_s$. Each state in the system is characterized by the synchronization order\ndetermined as the ratio of harmonics number at the common synchronization\nfrequency. The modelling demonstrates attainment of $N_s$ of several orders\nboth for a three-oscillator scheme $N_s$~650 and for a two-oscillator scheme\n$N_s$~260. A number of regularities are obtained, in particular, an optimal\nstrength of oscillator coupling is revealed when $N_s$ has a maximum. A general\ntendency toward information capacity decrease is shown when the coupling\nstrength and switch inner noise amplitude increase. An algorithm of pattern\nstorage and test vector recognition is suggested. It is also shown that the\ncoordinate number in each vector should be one less than the switch number to\nreduce recognition ambiguity. The demonstrated method of associative memory\nrealization is a general one and it may be applied in ONNs with various\nmechanisms and oscillator coupling topology."}, {"title": "Dynamics of Kuramoto oscillators with time-delayed positive and negative couplings", "abstract": "Many real-world examples of distributed oscillators involve not only time\ndelays but also attractive (positive) and repulsive (negative) influences in\ntheir network interactions. Here, considering such examples, we generalize the\nKuramoto model of globally coupled oscillators with time-delayed positive and\nnegative couplings to explore the effects of such couplings in collective phase\nsynchronization. We analytically derive the exact solutions for stable\nincoherent and coherent states in terms of the system parameters allowing us to\nprecisely understand the interplay of time delays and couplings in collective\nsynchronization. Dependent on these parameters, fully coherent, incoherent\nstates and mixed states are possible. Time-delays especially in the negative\ncoupling seem to facilitate collective synchronization. In case of a stronger\nnegative coupling than positive one, a stable synchronized state cannot be\nachieved without time delays. We discuss the implications of the model and the\nresults for natural systems, particularly neuronal network systems in the\nbrain."}, {"title": "Low Dimensional Dynamics of the Kuramoto Model with Rational Frequency Distributions", "abstract": "The Kuramoto model is a paradigmatic tool for studying the dynamics of\ncollective behavior in large ensembles of coupled dynamical systems. Over the\npast decade a great deal of progress has been made in analytical descriptions\nof the macroscopic dynamics of the Kuramoto mode, facilitated by the discovery\nof Ott and Antonsen's dimensionality reduction method. However, the vast\nmajority of these works relies on a critical assumption where the oscillators'\nnatural frequencies are drawn from a Cauchy, or Lorentzian, distribution, which\nallows for a convenient closure of the evolution equations from the\ndimensionality reduction. In this paper we investigate the low dimensional\ndynamics that emerge from a broader family of natural frequency distributions,\nin particular a family of rational distribution functions. We show that, as the\npolynomials that characterize the frequency distribution increase in order, the\nlow dimensional evolution equations become more complicated, but nonetheless\nthe system dynamics remain simple, displaying a transition from incoherence to\npartial synchronization at a critical coupling strength. Using the low\ndimensional equations we analytically calculate the critical coupling strength\ncorresponding to the onset of synchronization and investigate the scaling\nproperties of the order parameter near the onset of synchronization. These\nresults agree with calculations from Kuramoto's original self-consistency\nframework, but we emphasize that the low dimensional equations approach used\nhere allows for a true stability analysis categorizing the bifurcations."}, {"title": "Self-consistent Method and Steady States of Second-order Oscillators", "abstract": "The self-consistent method, first introduced by Kuramoto, is a powerful tool\nfor the analysis of the steady states of coupled oscillator networks. For\nsecond-order oscillator networks complications to the application of the\nself-consistent method arise because of the bistable behavior due to the\nco-existence of a stable fixed point and a stable limit cycle, and the\nresulting complicated boundary between the corresponding basins of attraction.\nIn this paper, we report on a self-consistent analysis of second-order\noscillators which is simpler compared to previous approaches while giving more\naccurate results in the small inertia regime and close to incoherence. We apply\nthe method to analyze the steady states of coupled second-order oscillators and\nwe introduce the concepts of margin region and scaled inertia. The improved\naccuracy of the self-consistent method close to incoherence leads to an\naccurate estimate of the critical coupling corresponding to transitions from\nincoherence."}, {"title": "Complexity Matching and Requisite Variety", "abstract": "Complexity matching characterizes the role of information in interactions\nbetween systems and can be traced back to the 1957 Introduction to Cybernetics\nby Ross Ashby. We argue that complexity can be expressed in terms of crucial\nevents, which are generated by the processes of spontaneous self-organization.\nComplex processes, ranging from biological to sociological, must satisfy the\nhomeodynamic condition and host crucial events that have recently been shown to\ndrive the information transport between complex systems. We adopt a\nphenomenological approach, based on the subordination to periodicity that makes\nit possible to combine homeodynamics and self-organization induced crucial\nevents. The complexity of crucial events is defined by the waiting-time\nprobability density function (PDF) of the intervals between consecutive crucial\nevents, which have an inverse power law (IPL) PDF $\\psi (\\tau )\\propto 1/(\\tau\n)^{\\mu }$ with $1<\\mu <3$. We show that the action of crucial events has an\neffect compatible with the shared notion of complexity-induced entropy\nreduction, while making the synchronization between systems sharing the same\ncomplexity different from chaos synchronization. We establish the coupling\nbetween two temporally complex systems using a phenomenological approach\ninspired by models of swarm cognition and prove that complexity matching,\nnamely sharing the same IPL index $\\mu $, facilitates the transport of\ninformation, generating perfect synchronization. This new form of complexity\nmatching is expected to contribute significantly to progress in understanding\nand improving biofeedback therapies."}, {"title": "Chimera states in nonlocally coupled bicomponent phase oscillators: From synchronous to asynchronous chimeras", "abstract": "Chimera states, a symmetry-breaking spatiotemporal pattern in nonlocally\ncoupled identical dynamical units, prevail in a variety of systems. Here, we\nconsider a population of nonlocally coupled bicomponent phase oscillators in\nwhich oscillators with natural frequency $\\omega_0$ (positive oscillators) and\n$-\\omega_0$ (negative oscillators) are randomly distributed along a ring. We\nshow the existence of chimera states no matter how large $\\omega_0$ is and the\nstates manifest themselves in the form that oscillators with positive/negative\nfrequency support their own chimera states. There are two types of chimera\nstates, synchronous chimera states at small $\\omega_0$ in which coherent\npositive and negative oscillators share a same mean phase velocity and\nasynchronous chimera states at large $\\omega_0$ in which coherent positive and\nnegative oscillators have different mean phase velocities. Increasing\n$\\omega_0$ induces a desynchronization transition between synchronous chimera\nstates and asynchronous chimera states."}, {"title": "An information-theoretic approach to self-organisation: Emergence of complex interdependencies in coupled dynamical systems", "abstract": "Self-organisation lies at the core of fundamental but still unresolved\nscientific questions, and holds the promise of de-centralised paradigms crucial\nfor future technological developments. While self-organising processes have\nbeen traditionally explained by the tendency of dynamical systems to evolve\ntowards specific configurations, or attractors, we see self-organisation as a\nconsequence of the interdependencies that those attractors induce. Building on\nthis intuition, in this work we develop a theoretical framework for\nunderstanding and quantifying self-organisation based on coupled dynamical\nsystems and multivariate information theory. We propose a metric of global\nstructural strength that identifies when self-organisation appears, and a\nmulti-layered decomposition that explains the emergent structure in terms of\nredundant and synergistic interdependencies. We illustrate our framework on\nelementary cellular automata, showing how it can detect and characterise the\nemergence of complex structures."}, {"title": "Node-Level Resilience Loss in Dynamic Complex Networks", "abstract": "In an increasingly connected world, the resilience of networked dynamical\nsystems is important in the fields of ecology, economics, critical\ninfrastructures, and organizational behaviour. Whilst we understand small-scale\nresilience well, our understanding of large-scale networked resilience is\nlimited. Recent research in predicting the effective network-level resilience\npattern has advanced our understanding of the coupling relationship between\ntopology and dynamics. However, a method to estimate the resilience of an\nindividual node within an arbitrarily large complex network governed by\nnon-linear dynamics is still lacking. Here, we develop a sequential mean-field\napproach and show that after 1-3 steps of estimation, the node-level resilience\nfunction can be represented with up to 98\\% accuracy. This new understanding\ncompresses the higher dimensional relationship into a one-dimensional dynamic\nfor tractable understanding, mapping the relationship between local dynamics\nand the statistical properties of network topology. By applying this framework\nto case studies in ecology and biology, we are able to not only understand the\ngeneral resilience pattern of the network, but also identify the nodes at the\ngreatest risk of failure and predict the impact of perturbations. These\nfindings not only shed new light on the causes of resilience loss from cascade\neffects in networked systems, but the identification capability could also be\nused to prioritize protection, quantify risk, and inform the design of new\nsystem architectures."}, {"title": "Delay-induced chimeras in neural networks with fractal topology", "abstract": "We study chimera states, which are partial synchronization patterns\nconsisting of spatially coexisting domains of coherent (synchronized) and\nincoherent (desynchronized) dynamics, in ring networks of FitzHugh-Nagumo\noscillators with fractal connectivities. In particular, we focus on the\ninterplay of time delay in the coupling term and the network topology. In the\nparameter plane of coupling strength and delay time we find tongue-like regions\nof existence of chimera states alternating with regions of coherent dynamics.\nWe show analytically and numerically that the period of the synchronized\ndynamics as a function of delay is characterized by a sequence of piecewise\nlinear branches. In between these branches various chimera states and other\npartial synchronization patterns are induced by the time delay. By varying the\ntime delay one can deliberately choose and stabilize desired spatio-temporal\npatterns."}, {"title": "Multi-clusters in networks of adaptively coupled phase oscillators networks", "abstract": "Dynamical systems on networks with adaptive couplings appear naturally in\nreal-world systems such as power grid networks, social networks as well as\nneuronal networks. We investigate a paradigmatic system of adaptively coupled\nphase oscillators inspired by neuronal networks with synaptic plasticity. One\nimportant behaviour of such systems reveals splitting of the network into\nclusters of oscillators with the same frequencies, where different clusters\ncorrespond to different frequencies. Starting from one-cluster solutions we\nprovide existence criteria for multi-cluster solutions and present their\nexplicit form. The phases of the oscillators within one cluster can be\norganized in different patterns: antipodal, double antipodal, and splay type.\nInterestingly, multi-clusters are shown to exist where different clusters\nexhibit different patterns. For instance, an antipodal cluster can coexist with\na splay cluster. We also provide stability conditions for one- and\nmulti-cluster solutions. These conditions, in particular, reveal a high level\nof multistability."}, {"title": "Revisiting asymptotic periodicity in networks of degrade-and-fire oscillators", "abstract": "Networks of degrade-and-fire oscillators are elementary models of populations\nof synthetic gene circuits with negative feedback, which show elaborate\nphenomenology while being amenable to mathematical analysis. In addition to\nthorough investigation in various examples of interaction graphs, previous\nstudies have obtained conditions on interaction topology and strength that\nensure that asymptotic behaviors are periodic (assuming that the so-called\nfiring sequence is itself periodic and involves all nodes). Here, we revisit\nand extend these conditions and we analyse the dynamics in a case of\nunidirectional periodic chain. This example shows in particular that the\nupdated conditions for asymptotic periodicity are optimal. Altogether, our\nresults provide a novel instance of direct impact of the topology of\ninteractions in the global dynamics of a collective system."}, {"title": "Bifurcation in the angular velocity of a circular disk propelled by symmetrically distributed camphor pills", "abstract": "We studied rotation of a disk propelled by a number of camphor pills\nsymmetrically distributed at its edge. The disk was put on a water surface so\nthat it could rotate around a vertical axis located at the disk center. In such\na system, the driving torque originates from surface tension difference\nresulting from inhomogeneous surface concentration of camphor molecules\nreleased from the pills. Here we investigated the dependence of the stationary\nangular velocity on the disk radius and on the number of pills. The work\nextends our previous study on a linear rotor propelled by two camphor pills\n[Phys. Rev. E, 96, 012609 (2017)]. It was observed that the angular velocity\ndropped to zero after a critical number of pills was exceeded. Such behavior\nwas confirmed by a numerical model of time evolution of the rotor. The model\npredicts that, for a fixed friction coefficient, the speed of pills can be\naccurately represented by a function of the linear number density of pills. We\nalso present bifurcation analysis of the conditions at which the transition\nbetween a standing and a rotating disk appears."}, {"title": "The Winfree model with heterogeneous phase-response curves: Analytical results", "abstract": "We study an extension of the Winfree model of coupled phase oscillators in\nwhich both natural frequencies and phase-response curves (PRCs) are\nheterogeneous. In the first part of the paper we resort to averaging and derive\nan approximate model, in which the oscillators are coupled through their phase\ndifferences. Remarkably, this simplified model is the 'Kuramoto model with\ndistributed shear' (2011 Phys. Rev. Lett. 106 254101). We find that above a\ncritical level of PRC heterogeneity the incoherent state is always stable. In\nthe second part of the paper we perform the analysis of the full model for\nLorentzian heterogeneities, resorting to the Ott-Antonsen ansatz. The critical\nlevel of PRC heterogeneity obtained within the averaging approximation has a\ndifferent manifestation in the full model depending on the sign of the center\nof the distribution of PRCs."}, {"title": "Phase resetting and intermittent control at critical edge of stability as major mechanisms of fractality in human gait cycle variability", "abstract": "The fractality of human gait, namely, the long-range correlation that\ncharacterizes scale-free fluctuations of gait descriptors, such as the stride\nintervals during steady-state walking, depends on the well-tuned organization\nof the sensorimotor controller. Gait fractality is apparent in healthy young\nadults but tends to disappear in elderly individuals and neurological patients.\nTherefore, its partial loss may be indicative of pathological conditions.\nDespite its potential to be used as a dynamical biomarker for fall risk\nassessment, the mechanistic origin of gait fractality has been investigated by\nonly a few studies, and even less attention has been devoted to the link\nbetween gait fractality and gait stability. Here, we propose a novel\ncomputational model of gait by addressing the flexibility-stability trade-off\nfirst, and then by showing that gait fractality is a natural consequence of the\ndeveloped control mechanisms, including phase resetting and intermittent\ncontrol, which supplement instability in a linear feedback controller operated\nat stability's edge. The results suggest that pathological gait, characterized\nby joint-rigidity and/or loss of fractality, may be caused by dysfunction in\nsome of these mechanisms."}, {"title": "From phase to amplitude oscillators", "abstract": "We analyze an intermediate collective regime where amplitude oscillators\ndistribute themselves along a closed, smooth, time-dependent curve\n$\\mathcal{C}$, thereby maintaining the typical ordering of (identical) phase\noscillators. This is achieved by developing a general formalism based on two\npartial differential equations, which describe the evolution of the probability\ndensity along $\\mathcal{C}$ and of the shape of $\\mathcal{C}$ itself. The\nformalism is specifically developed for Stuart-Landau oscillators, but it is\ngeneral enough to be applied to other classes of amplitude oscillators. The\nmain achievements consist in: (i) identification and characterization of a new\ntransition to self-consistent partial synchrony (SCPS), which confirms the\ncrucial role played by higher Fourier hamonics in the coupling function; (ii)\nan analytical treatment of SCPS, including a detailed stability analysis; (iii)\nthe discovery of a new form of collective chaos, which can be seen as a\ngeneralization of SCPS and characterized by a multifractal probability density.\nFinally, we are able to describe given dynamical regimes both at the\nmacroscopic as well as the microscopic level, thereby shedding further light on\nthe relationship between the two different levels of description."}, {"title": "First-order phase transitions in the Kuramoto model with compact bimodal frequency distributions", "abstract": "The Kuramoto model of a network of coupled phase oscillators exhibits a\nfirst-order phase transition when the distribution of natural frequencies has a\nfinite flat region at its maximum. First-order phase transitions including\nhysteresis and bistability are also present if the frequency distribution of a\nsingle network is bimodal. In this study we are interested in the interplay of\nthese two configurations and analyze the Kuramoto model with compact bimodal\nfrequency distributions in the continuum limit. As of yet, a rigorous analytic\ntreatment has been elusive. By combining Kuramoto's self-consistency approach,\nCrawford's symmetry considerations, and exploiting the Ott-Antonsen ansatz\napplied to a family of rational distribution functions that converge towards\nthe compact distribution, we derive a full bifurcation diagram for the system's\norder parameter dynamics. We show that the route to synchronization always\npasses through a standing wave regime when the bimodal distribution is\ncompounded by two unimodal distributions with compact support. This is in\ncontrast to a possible transition across a region of bistability when the two\ncompounding unimodal distributions have infinite support."}, {"title": "Self-organizing dynamical networks able to learn autonomously", "abstract": "We present a model for the time evolution of network architectures based on\ndynamical systems. We show that the evolution of the existence of a connection\nin a network can be described as a stochastic non-markovian telegraphic signal\n(NMTS). Such signal is formulated in two ways: as an algorithm and as the\nresult of a system of differential equations. The autonomous learning\nconjecture [Phys. Rev. E \\textbf{90},030901(R) (2014)] is implemented in the\nproposed dynamics. As a result, we construct self-organizing dynamical systems\n(networks) able to modify their structures in order to learn prescribed target\nfunctionalities. This theory is applied to two systems: the flow processing\nnetworks with time-programmed responses, and a system of first-order chemical\nreactions. In both cases, we show examples of the evolution and a statistical\nanalysis of the obtained functional networks with respect to the model\nparameters."}, {"title": "Big Bang Bifurcations in the Tantalus Oscillator under Biphasics Perturbations", "abstract": "The Tantalus Oscillator is a non linear hydrodynamic oscillator with an\nattractive limit cycle. In this study we pursue the construction of a\nbiparametric bifurcation diagram for the Tantalus Oscillator under biphasics\nperturbations. That is the first time that this kind of diagram is built for\nthis kind of oscillator under biphasics perturbations. Results show that\nbiphasic perturbations have no effect when the coupling time is chosen over a\nwide range of values. This modifies the bifurcation diagram obtain under\nmonophasics perturbations. Now we have the appearance of periodic increment Big\nBang Bifurcations. The theoretical results are in excellent agreement with\nexperimental observations."}, {"title": "Using reservoir computers to distinguish chaotic signals", "abstract": "Several recent papers have shown that reservoir computers are useful for\nanalyzing and predicting dynamical systems. Reservoir computers have also been\nshown to be useful for various classification problems. In this work, a\nreservoir computer is used to identify one out of the 19 different Sprott\nsystems. An advantage of reservoir computers for this problem is that no\nembedding is necessary. Some guidance on choosing the reservoir computer\nparameters is given. The dependance on number of points, number of reservoir\nnodes and noise in identifying the Sprott systems is explored."}, {"title": "Delay Regulated Explosive Synchronization in Multiplex Networks", "abstract": "It is known that explosive synchronization (ES) in an isolated network of\nKuramoto oscillators with inertia is significantly enhanced by the presence of\ntime delay. Here we show that time delay in one layer of the multiplex network\ngoverns the transition to synchronization and ES in the other layers. We found\nthat a single layer with time-delayed intra-layer coupling, depending on the\nvalues of time delay, experiences a different type of transition to\nsynchronization, e.g., ES or continuous, and the same type of transition is\nincorporated simultaneously in other layer(s) as well. Hence, a suitable choice\nof time-delay in only one layer can lead to a desired (either ES or continuous)\ntransition simultaneously in the delayed and other undelayed layers of\nmultiplexed network. These results offer a platform for a better understanding\nof the dynamics of those complex systems which are represented by multilayered\nframework and contain time delays in the communication processes."}, {"title": "Temporal inactivation enhances robustness in an evolving system", "abstract": "We study the robustness of an evolving system that is driven by successive\ninclusions of new elements or constituents with $m$ random interactions to\nolder ones. Each constitutive element in the model stays either active or is\ntemporarily inactivated depending upon the influence of the other active\nelements. If the time spent by an element in the inactivated state reaches\n$T_W$, it gets extinct. The phase diagram of this dynamic model as a function\nof $m$ and $T_W$ is investigated by numerical and analytical methods and as a\nresult both growing (robust) as well as non-growing (volatile) phases are\nidentified. It is also found that larger time limit $T_W$ enhances the system's\nrobustness against the inclusion of new elements, mainly due to the system's\nincreased ability to reject \"falling-together\" type attacks. Our results\nsuggest that the ability of an element to survive in an unfavorable situation\nfor a while, either as a minority or in a dormant state, could improve the\nrobustness of the entire system."}, {"title": "Cyclic structure induced by load fluctuations in adaptive transportation networks", "abstract": "Transport networks are crucial to the functioning of natural systems and\ntechnological infrastructures. For flow networks in many scenarios, such as\nrivers or blood vessels, acyclic networks (i.e., trees) are optimal structures\nwhen assuming time-independent in- and outflow. Dropping this assumption,\nfluctuations of net flow at source and/or sink nodes may render the pure tree\nsolutions unstable even under a simple local adaptation rule for conductances.\nHere, we consider tree-like networks under the influence of spatially\nheterogeneous distribution of fluctuations, where the root of the tree is\nsupplied by a constant source and the leaves at the bottom are equipped with\nsinks with fluctuating loads. We find that the network divides into two regions\ncharacterized by tree-like motifs and stable cycles. The cycles emerge through\ntranscritical bifurcations at a critical amplitude of fluctuation. For a simple\nnetwork structure, depending on parameters defining the local adaptation,\ncycles first appear close to the leaves (or root) and then appear closer\ntowards the root (or the leaves). The interaction between topology and dynamics\ngives rise to complex feedback mechanisms with many open questions in the\ntheory of network dynamics. A general understanding of the dynamics in adaptive\ntransport networks is essential in the study of mammalian vasculature, and\nadaptive transport networks may find technological applications in\nself-organizing piping systems."}, {"title": "Conjugate coupling induced symmetry breaking and quenched oscillations", "abstract": "Spontaneous symmetry breaking (SSB) is essential and plays a vital role many\nnatural phenomena, including the formation of Turing pattern in organisms and\ncomplex patterns in brain dynamics. In this work, we investigate whether a set\nof coupled Stuart-Landau oscillators can exhibit spontaneous symmetry breaking\nwhen the oscillators are interacting through dissimilar variables or conjugate\ncoupling. We find the emergence of SSB state with coexisting distinct dynamical\nstates in the parametric space and show how the system transits from symmetry\nbreaking state to out-of-phase synchronized (OPS) state while admitting\nmultistabilities among the dynamical states. Further, we also investigate the\neffect of feedback factor on SSB as well as oscillation quenching states and we\npoint out that the decreasing feedback factor completely suppresses SSB and\noscillation death states. Interestingly, we also find the feedback factor\ncompletely diminishes only symmetry breaking oscillation and oscillation death\n(OD) states but it does not affect the nontrivial amplitude death (NAD) state.\nFinally, we have deduced the analytical stability conditions for in-phase and\nout-of-phase oscillations, as well as amplitude and oscillation death states."}, {"title": "Rethinking network reciprocity over social ties: local interactions make direct reciprocity possible and pave the rational way to cooperation", "abstract": "Since Nowak & May's (1992) influential paper, network reciprocity--the fact\nthat individuals' interactions repeated within a local neighborhood support the\nevolution of cooperation--has been confirmed in several theoretical models.\nEssentially, local interactions allow cooperators to stay protected from\nexploiters by assorting into clusters, and the heterogeneity of the network of\ncontacts--the co-presence of low- and high-connected nodes--has been shown to\nfurther favor cooperation. The few available large-scale experiments on humans\nhave however missed these effects. The reason is that, while models assume that\nindividuals update strategy by imitating better performing neighbors,\nexperiments showed that humans are more prone to reciprocate cooperation than\nto compare payoffs. Inspired by the empirical results, we rethink network\nreciprocity as a rational form of direct reciprocity on networks--networked\nrational reciprocity--indeed made possible by the locality of interactions. We\nshow that reciprocal altruism in a networked prisoner's dilemma can invade and\nfixate in any network of rational agents, profit-maximizing over an horizon of\nfuture interactions. We find that networked rational reciprocity works better\nat low average connectivity and we unveil the role of network heterogeneity.\nOnly if cooperating hubs invest in the initial cost of exploitation, the\ninvasion of cooperation is boosted; it is otherwise hindered. Although humans\nmight not be as rational as here assumed, our results could help the design and\ninterpretation of new experiments in social and economic networks"}, {"title": "Bridging between Load-Flow and Kuramoto-like Power Grid Models: A Flexible Approach to Integrating Electrical Storage Units", "abstract": "In future power systems, electrical storage will be the key technology for\nbalancing feed-in fluctuations. With increasing share of renewables and\nreduction of system inertia, the focus of research expands towards short-term\ngrid dynamics and collective phenomena. Against this backdrop, Kuramoto-like\npower grids have been established as a sound mathematical modeling framework\nbridging between the simplified models from nonlinear dynamics and the more\ndetailed models used in electrical engineering. However, they have a blind spot\nconcerning grid components, which cannot be modeled by oscillator equations,\nand hence do not allow to investigate storage-related issues from scratch. We\nremove this shortcoming by bringing together Kuramoto-like and algebraic\nload-flow equations. This is a substantial extension of the current Kuramoto\nframework with arbitrary grid components. Based on this concept, we provide a\nsolid starting point for the integration of flexible storage units enabling to\naddress current problems like smart storage control, optimal siting and rough\ncost estimations. For demonstration purpose, we here consider a wind power\napplication with realistic feed-in conditions. We show how to implement basic\ncontrol strategies from electrical engineering, give insights into their\npotential with respect to frequency quality improvement and point out their\nlimitations by maximum capacity and finite-time response."}, {"title": "First-order synchronization transition in a large population of strongly coupled relaxation oscillators", "abstract": "Onset and loss of synchronization in coupled oscillators are of fundamental\nimportance in understanding emergent behavior in natural and man-made systems,\nwhich range from neural networks to power grids. We report on experiments with\nhundreds of strongly coupled photochemical relaxation oscillators that exhibit\na discontinuous synchronization transition with hysteresis, as opposed to the\nparadigmatic continuous transition expected from the widely used weak coupling\ntheory. The resulting first-order transition is robust with respect to changes\nin network connectivity and natural frequency distribution. This allows us to\nidentify the relaxation character of the oscillators as the essential parameter\nthat determines the nature of the synchronization transition. We further\nsupport this hypothesis by revealing the mechanism of the transition, which\ncannot be accounted for by standard phase reduction techniques."}, {"title": "Self adaptation of chimera states", "abstract": "Chimera states in spatiotemporal dynamical systems have been investigated in\nphysical, chemical, and biological systems, and have been shown to be robust\nagainst random perturbations. How do chimera states achieve their robustness?\nWe uncover a self-adaptation behavior by which, upon a spatially localized\nperturbation, the coherent component of the chimera state spontaneously drifts\nto an optimal location as far away from the perturbation as possible, exposing\nonly its incoherent component to the perturbation to minimize the disturbance.\nA systematic numerical analysis of the evolution of the spatiotemporal pattern\nof the chimera state towards the optimal stable state reveals an exponential\nrelaxation process independent of the spatial location of the perturbation,\nimplying that its effects can be modeled as restoring and damping forces in a\nmechanical system and enabling the articulation of a phenomenological model.\nNot only is the model able to reproduce the numerical results, it can also\npredict the trajectory of drifting. Our finding is striking as it reveals that,\ninherently, chimera states possess a kind of \"intelligence\" in achieving\nrobustness through self adaptation. The behavior can be exploited for\ncontrolled generation of chimera states with their coherent component placed in\nany desired spatial region of the system."}, {"title": "Sinusoidal-signal detection by active, noisy oscillators on the brink of self-oscillation", "abstract": "Determining the conditions under which an active system best detects\nsinusoidal signals is important for numerous fields. It is known that a\nquiescent, deterministic system possessing a supercritical Hopf bifurcation is\nmore sensitive to sinusoidal stimuli the closer it operates to the bifurcation.\nTo understand signal detection in many natural settings, however, noise must be\ntaken into account. We study the Fokker-Planck equation describing the\nsinusoidally forced dynamics of a noisy supercritical or subcritical Hopf\noscillator. To distinguish an oscillator's motion owing to sinusoidal forcing\nfrom that provoked by noise, we employ the phase-locked amplitude and vector\nstrength, which are zero in the absence of an external signal. The phase-locked\namplitude and entrainment to frequency-detuned forcing but not resonant forcing\npeak as functions of the control parameter. These peaks occur near but not at\nthe bifurcations. Moreover, an oscillator can detect stimuli over the broadest\nfrequency range when it spontaneously oscillates near a Hopf bifurcation.\nAlthough noise exerts the greatest effect on the phase-locked amplitude when a\nHopf oscillator is near a Hopf bifurcation, the oscillator nevertheless\nperforms best as a sinusoidal-signal detector when it operates close to the\nbifurcation. The oscillator's ability to differentiate detuned signals from\nnoise is greatest with it autonomously oscillates near to but not at the\nbifurcation."}, {"title": "Suppression of macroscopic oscillations in mixed populations of active and inactive oscillators coupled through lattice Laplacian", "abstract": "We consider suppression of macroscopic synchronized oscillations in mixed\npopulations of active and inactive oscillators with local diffusive coupling,\ndescribed by a lattice complex Ginzburg-Landau model with discrete Laplacian in\ngeneral dimensions. Approximate expression for the stability of the\nnon-oscillatory stationary state is derived on the basis of the generalized\nfree energy of the system. We show that an effective wavenumber of the system\ndetermined by the spatial arrangement of the active and inactive oscillators is\nan decisive factor in the suppression, in addition to the ratio of active\npopulation to inactive population and relative intensity of each population.\nThe effectiveness of the proposed theory is illustrated with a cortico-thalamic\nmodel of epileptic seizures, where active and inactive oscillators correspond\nto epileptic foci and healthy cerebral cortex tissue, respectively."}, {"title": "When is sync globally stable in sparse networks of identical Kuramoto oscillators?", "abstract": "Synchronization in systems of coupled Kuramoto oscillators may depend on\ntheir natural frequencies, coupling, and underlying networks. In this paper, we\nreduce the alternatives to only one by considering identical oscillators where\nthe only parameter that is allowed to change is the underlying network. While\nsuch a model was analyzed over the past few decades by studying the size of the\nbasin of attraction of the synchronized state on restricted families of graphs,\nhere we address a qualitative question on general graphs. In an analogy to\nresistive networks with current sources, we describe an algorithm that produces\ninitial conditions that are often outside of the basin of attraction of the\nsynchronized state. In particular, if a graph allows a cyclic graph clustering\nwith a sufficient number of clusters or contains a sufficiently long induced\nsubpath without cut vertices of the graph then there is a non-synchronous\nstable phase-locked solution. Thus, we provide a partial answer to when the\nsynchronized state is not globally stable."}, {"title": "Modeling cochlear two-tone suppression using a system of nonlinear oscillators with feed-forward coupling", "abstract": "Mechanism of two-tone suppression is studied using a coupled-oscillator model\nof the cochlea with feed-forward coupling. Local amplification of sound signals\nis modeled by using Stuart-Landau oscillators near the Hopf bifurcation, and\ntransmission of sound signals is described as feed-forward coupling between the\noscillators. Effect of suppressor signals on the response to probe signals is\nanalyzed by numerical simulations. It is found that the effect of suppression\nis qualitatively different depending on relative frequency between probe and\nsuppressor signals. By analyzing a simplified two-oscillator model, we explain\nthe mechanism of the suppression, where configuration of the oscillators plays\nan essential role."}, {"title": "Metastability and Multiscale Extinction Time on a Finite System of Interacting Stochastic Chains", "abstract": "We studied metastability and extinction time of a finite system with a large\nnumber of interacting components in discrete time by means of analytical and\nnumerical investigation. The system is markovian with respect to the potential\nprofile of the components, which are subject to leakage and gain effects\nsimultaneously. We show that the only invariant measure is the null\nconfiguration, that the system ceases activity almost surely in a finite time\nand that extinction time presents a cutoff behavior. Moreover, there is a\ncritical parameter determined by leakage and gain below which the extinction\ntime does not depend on the system size. Above such critical ratio, the\nextinction time depends on the number of components and the system tends to\nstabilize around a unique metastable state. Furthermore, the extinction time\npresents infinitely many scales with respect to the system size."}, {"title": "Simple model of complex dynamics of activity patterns in developing networks of neuronal cultures", "abstract": "Living neuronal networks in dissociated neuronal cultures are widely known\nfor their ability to generate highly robust spatiotemporal activity patterns in\nvarious experimental conditions. These include neuronal avalanches satisfying\nthe power scaling law and thereby exemplifying self-organized criticality in\nliving systems. A crucial question is how these patterns can be explained and\nmodeled in a way that is biologically meaningful, mathematically tractable and\nyet broad enough to account for neuronal heterogeneity and complexity. Here we\npropose a simple model which may offer an answer to this question. Our\nderivations are based on just few phenomenological observations concerning\ninput-output behavior of an isolated neuron. A distinctive feature of the model\nis that at the simplest level of description it comprises of only two\nvariables, a network activity variable and an exogenous variable corresponding\nto energy needed to sustain the activity and modulate the efficacy of signal\ntransmission. Strikingly, this simple model is already capable of explaining\nemergence of network spikes and bursts in developing neuronal cultures. The\nmodel behavior and predictions are supported by empirical observations and\npublished experimental evidence on cultured neurons behavior exposed to oxygen\nand energy deprivation. At the larger, network scale, introduction of the\nenergy-dependent regulatory mechanism enables the network to balance on the\nedge of the network percolation transition. Network activity in this state\nshows population bursts satisfying the scaling avalanche conditions. This\nnetwork state is self-sustainable and represents a balance between global\nnetwork-wide processes and spontaneous activity of individual elements."}, {"title": "Collective dynamics of globally delay-coupled complex Ginzburg-Landau oscillators", "abstract": "The effect of time-delayed coupling on the collective behavior of a\npopulation of globally coupled complex Ginzburg-Landau (GCCGL) oscillators is\ninvestigated. A detailed numerical study is carried out to study the impact of\ntime delay on various collective states that include synchronous states,\nmulticluster states, chaos, amplitude mediated chimeras and incoherent states.\nIt is found that time delay can bring about significant changes in the\ndynamical properties of these states including their regions of existence and\nstability. In general, an increase in time delay is seen to lower the threshold\nvalue of the coupling strength for the occurrence of such states and to shift\nthe existence domain towards more negative values of the linear dispersion\nparameter. Further insights into the numerical findings are provided, wherever\npossible, by exact equilibrium and stability analysis of these states in the\npresence of time delay."}, {"title": "Optimization of linear and nonlinear interaction schemes for stable synchronization of weakly coupled limit-cycle oscillators", "abstract": "Optimization of mutual synchronization between a pair of limit-cycle\noscillators with weak symmetric coupling is considered in the framework of the\nphase reduction theory. By generalizing a previous study on the optimization of\ncross-diffusion coupling matrices between the oscillators, we consider\noptimization of mutual coupling signals to maximize the linear stability of the\nsynchronized state, which are functionals of the past time sequences of the\noscillator states. For the case of linear coupling, optimization of the delay\ntime and linear filitering of coupling signals are considered. For the case of\nnonlinear coupling, general drive-response coupling is considered, and the\noptimal response and driving functions are derived. The theoretical results are\nexemplified by numerical simulations."}, {"title": "Microscopic Cross-Correlations in the Finite-Size Kuramoto Model of Coupled Oscillators", "abstract": "Super-critical Kuramoto oscillators with distributed frequencies separate\ninto two disjoint groups: an ordered one locked to the mean field, and a\ndisordered one consisting of effectively decoupled oscillators -- at least so\nin the thermodynamic limit. In finite ensembles, in contrast, such clear\nseparation fails: The mean field fluctuates due to finite-size effects and\nthereby induces order in the disordered group. To our best knowledge, this\npublication is the first to reveal such an effect, similar to noise-induced\nsynchronization, in a purely deterministic system. We start by modeling the\nsituation as a stationary mean field with additional white noise acting on a\npair of unlocked Kuramoto oscillators. An analytical expression shows that the\ncross-correlation between the two increases with decreasing ratio of natural\nfrequency difference and noise intensity. In a deterministic finite Kuramoto\nmodel, the strength of the mean field fluctuations is inextricably linked to\nthe typical natural frequency difference. Therefore, we let a fluctuating mean\nfield, generated by a finite ensemble of active oscillators, act on pairs of\npassive oscillators with a microscopic natural frequency difference between\nwhich we then measure the cross-correlation, at both super- and sub-critical\ncoupling."}, {"title": "Generative framework for dimensionality reduction of large scale network of non-linear dynamical systems driven by external input", "abstract": "Several studies have proposed constraints under which a low dimensional\nrepresentation can be derived from large-scale real-world networks exhibiting\ncomplex nonlinear dynamics. Typically, these representations are formulated\nunder certain assumptions, such as when solutions converge to attractor states\nusing linear stability analysis or using projections of large-scale dynamical\ndata into a set of lower dimensional modes that are selected heuristically.\nHere, we propose a generative framework for selection of lower dimensional\nmodes onto which the entire network dynamics can be projected based on the\nsymmetry of the input distribution for a large-scale network driven by external\ninputs, thus relaxing the heuristic selection of modes made in the earlier\nreduction approaches. The proposed mode reduction technique is tractable\nanalytically and applied to different kinds of real-world large-scale network\nscenarios with nodes comprising of a) Van der Pol oscillators b) Hindmarsh-Rose\nneurons. These two demonstrations elucidate how order parameter is conserved at\noriginal and reduced descriptions thus validating our proposition."}, {"title": "Hysteretic behavior of spatially coupled phase-oscillators", "abstract": "Motivated by phenomena related to biological systems such as the\nsynchronously flashing swarms of fireflies, we investigate a network of phase\noscillators evolving under the generalized Kuramoto model with inertia. A\ndistance-dependent, spatial coupling between the oscillators is considered.\nZeroth and first order kernel functions with finite kernel radii were chosen to\ninvestigate the effect of local interactions. The hysteretic dynamics of the\nsynchronization depending on the coupling parameter was analyzed for different\nkernel radii. Numerical investigations demonstrate that (1) locally locked\nclusters develop for small coupling strength values, (2) the hysteretic\nbehavior vanishes for small kernel radii, (3) the ratio of the kernel radius\nand the maximal distance between the oscillators characterizes the behavior of\nthe network."}, {"title": "Reduction of oscillator dynamics on complex networks to dynamics on complete graphs through virtual frequencies", "abstract": "We consider the synchronization of oscillators in complex networks where\nthere is an interplay between the oscillator dynamics and the network topology.\nThrough a remarkable transformation in parameter space and the introduction of\nvirtual frequencies we show that Kuramoto oscillators on annealed networks,\nwith or without frequency-degree correlation, and Kuramoto oscillators on\ncomplete graphs with frequency-weighted coupling can be transformed to Kuramoto\noscillators on complete graphs with a re-arranged, virtual frequency\ndistribution, and uniform coupling. The virtual frequency distribution encodes\nboth the natural frequency distribution (dynamics) and the degree distribution\n(topology). We apply this transformation to give direct explanations to a\nvariety of phenomena that have been observed in complex networks, such as\nexplosive synchronization and vanishing synchronization onset."}, {"title": "Cooperation dynamics in the networked geometric Brownian motion", "abstract": "Recent works suggest that pooling and sharing may constitute a fundamental\nmechanism for the evolution of cooperation in well-mixed fluctuating\nenvironments. The rationale is that, by reducing the amplitude of fluctuations,\npooling and sharing increases the steady-state growth rate at which the\nindividuals self-reproduce. However, in reality interactions are seldom\nrealized in a well-mixed structure, and the underlying topology is in general\ndescribed by a complex network. Motivated by this observation, we investigate\nthe role of the network structure on the cooperative dynamics in fluctuating\nenvironments, by developing a model for networked pooling and sharing of\nresources undergoing environmental fluctuations, represented through geometric\nBrownian motion. The study reveals that, while in general cooperation increases\nthe individual steady state growth rates (i.e. is evolutionary advantageous),\nthe interplay with the network structure may yield large discrepancies in the\nobserved individual resource endowments. We comment possible biological and\nsocial implications and discuss relations to econophysics."}, {"title": "Resolving Collisions for the Gipps Car-Following Model", "abstract": "The Gipps car-following model is a widely used tool for studying and\nsimulation traffic dynamics. Despite its popularity an often disregarded\nproperty is that under heterogeneous parametrization on the individual vehicles\nin the traffic flow, the model may produce collisions. This stands in crude\ncontrast to the principle, from which the model was derived: drive as fast as\npossible while guaranteeing a safe headway in case that the leading vehicle\nstarts braking hard. Indeed, Gipps proof for the model being collision-free\nonly holds for ensembles of identical vehicles.\n  In this work we examine the circumstances leading to collisions in\nheterogeneous ensembles and propose a natural model extension, which conveys\nthe original models principles to situations, where collisions occur. For these\ncases we present analytical and numerical results on the stability of the\nequilibrium flow."}, {"title": "Resonant features in a forced population of excitatory neurons", "abstract": "In recent years, the study of coupled excitable oscillators has largely\nbenefited from a new analytical technique developed by Ott and Antonsen. This\ntechnique allows to express the dynamics of certain macroscopic observable in\nthe ensemble in terms of a reduced set of ordinary differential equations. This\nmakes it possible to build low-dimensional models for the global activity of\nneural systems from first principles. We investigated the macroscopic response\nof a large set of excitatory neurons to different forcing strategies. We report\nresonant behavior, that depends on the heterogeneity between the units and\ntheir coupling strength. This contrasts with the type of response that an\nexternal forcing can elicit in simple and widely used phenomenological models."}, {"title": "Effects of Synaptic and Myelin Plasticity on Learning in a Network of Kuramoto Phase Oscillators", "abstract": "Models of learning typically focus on synaptic plasticity. However, learning\nis the result of both synaptic and myelin plasticity. Specifically, synaptic\nchanges often co-occur and interact with myelin changes, leading to complex\ndynamic interactions between these processes. Here, we investigate the\nimplications of these interactions for the coupling behavior of a system of\nKuramoto oscillators. To that end, we construct a fully connected,\none-dimensional ring network of phase oscillators whose coupling strength\n(reflecting synaptic strength) as well as conduction velocity (reflecting\nmyelination) are each regulated by a Hebbian learning rule. We evaluate the\nbehavior of the system in terms of structural (pairwise connection strength and\nconduction velocity) and functional connectivity (local and global\nsynchronization behavior). We find that for conditions in which a system\nlimited to synaptic plasticity develops two distinct clusters both structurally\nand functionally, additional adaptive myelination allows for functional\ncommunication across these structural clusters. Hence, dynamic conduction\nvelocity permits the functional integration of structurally segregated\nclusters. Our results confirm that network states following learning may be\ndifferent when myelin plasticity is considered in addition to synaptic\nplasticity, pointing towards the relevance of integrating both factors in\ncomputational models of learning."}, {"title": "Relay synchronization in multiplex networks of discrete maps", "abstract": "Complex multiplex networks consist of several subnetwork layers, which\ninteract via pairwise inter-layer connections. Relay synchronization between\ndistant layers which are not directly connected, but only via a relay layer,\ncan be observed in multiplex networks. We study three-layer networks of\ndiscrete logistic maps, where each individual layer is a nonlocally coupled\nring, and demonstrate scenarios of relay synchronization of complex patterns in\nthe outer layers which interact via an intermediate layer. We find regimes of\nrelay synchronization for chimera states, i.e., patterns of coexisting coherent\nand incoherent domains, and a transition from phase chimeras to amplitude\nchimeras for increasing inter-layer coupling. We determine analytically the\napproximate critical coupling strengths for the existence of phase chimeras."}, {"title": "Effects of adaptive acceleration response of birds on collective behaviors", "abstract": "Collective dynamics of many interacting particles have been widely studied\nbecause of a wealth of their behavioral patterns quite different from the\nindividual traits. A selective way of birds that reacts to their neighbors is\none of the main factors characterizing the collective behaviors. Individual\nbirds can react differently depending on their local environment during the\ncollective decision-making process, and these variable reactions can be a\nsource of complex spatiotemporal flocking dynamics. Here, we extend the\ndeterministic Cucker-Smale model by including the individual's reaction to\nneighbors' acceleration where the reaction time depends on the local state of\npolarity. Simulation results show that the adaptive reaction of individuals\ninduces the collective response of the flock. Birds are not frozen in a\ncomplete synchronization but remain sensitive to perturbations coming from\nenvironments. We confirm that the adaptivity of the reaction also generates\nnatural fluctuations of orientation and speed, both of which are indeed\nscale-free as experimentally reported. This work may provide essential insight\nin designing resilient systems of many active agents working in complex,\nunpredictable environments."}, {"title": "Growth of a tree with allocations rules: Part 1 Kinematics", "abstract": "A non-local model describing the growth of a tree-like transportation network\nwith given allocation rules is proposed. In this model we focus on tree like\nnetworks, and the network transports the very resource it needs to build\nitself. Some general results are given on the viability tree-like networks that\nproduce an amount of resource based on its amount of leaves while having a\nmaintenance cost for each node. Some analytical studies and numerical surveys\nof the model in \"simple\" situations are made. The different outcomes are\ndiscussed and possible extensions of the model are then discussed."}, {"title": "Competitive percolation strategies for network recovery", "abstract": "Restoring operation of critical infrastructure systems after catastrophic\nevents is an important issue, inspiring work in multiple fields, including\nnetwork science, civil engineering, and operations research. We consider the\nproblem of finding the optimal order of repairing elements in power grids and\nsimilar infrastructure. Most existing methods either only consider system\nnetwork structure, potentially ignoring important features, or incorporate\ncomponent level details leading to complex optimization problems with limited\nscalability. We aim to narrow the gap between the two approaches. Analyzing\nrealistic recovery strategies, we identify over- and undersupply penalties of\ncommodities as primary contributions to reconstruction cost, and we demonstrate\ntraditional network science methods, which maximize the largest connected\ncomponent, are cost inefficient. We propose a novel competitive percolation\nrecovery model accounting for node demand and supply, and network structure.\nOur model well approximates realistic recovery strategies, suppressing growth\nof the largest connected component through a process analogous to explosive\npercolation. Using synthetic power grids, we investigate the effect of network\ncharacteristics on recovery process efficiency. We learn that high structural\nredundancy enables reduced total cost and faster recovery, however, requires\nmore information at each recovery step. We also confirm that decentralized\nsupply in networks generally benefits recovery efforts."}, {"title": "Synchronization Behavior in a Ternary Phase Model", "abstract": "Localized traveling-wave solutions to a nonlinear Schrodinger equation were\nrecently shown to be a consequence of Fourier mode synchronization. The reduced\ndynamics describing mode interaction take the form of a phase model with novel\nternary coupling. We analyze this model in the presence of quenched disorder\nand explore transitions to partial and complete synchronization. For both\nGaussian and uniform disorder, first-order transitions with hysteresis are\nobserved. These results are compared with the phenomenology of the Kuramoto\nmodel which exhibits starkly different behavior. An infinite-oscillator limit\nof the model is derived and solved to provide theoretical predictions for the\nobserved transitions. Treatment of the nonlocal ternary coupling in this limit\nsheds some light on the model's novel structure."}, {"title": "Long-range interaction induced collective dynamical behaviors", "abstract": "Long-range interacting systems are omnipresent in nature. We investigate here\nthe collective dynamical behavior in a long-range interacting system consisting\nof coupled Stuart-Landau limit cycle oscillators. In particular, we analyze the\nimpact of a repulsive coupling along with symmetry breaking coupling. We report\nthat the addition of repulsive coupling of sufficient strength can induce a\nswing of the synchronized state which will start disappearing with an\nincreasing disorder as a function of the repulsive coupling. We also deduce\nanalytical stability conditions for the oscillatory states including\nsynchronized state, solitary state, two-cluster state as well as oscillation\ndeath state. Finally, we have also analyzed the effect of power-law exponent on\nthe observed dynamical states."}, {"title": "A review of swarmalators and their potential in bio-inspired computing", "abstract": "From fireflies to heart cells, many systems in Nature show the remarkable\nability to spontaneously fall into synchrony. By imitating Nature's success at\nself-synchronizing, scientists have designed cost-effective methods to achieve\nsynchrony in the lab, with applications ranging from wireless sensor networks\nto radio transmission. A similar story has occurred in the study of swarms,\nwhere inspiration from the behavior flocks of birds and schools of fish has led\nto 'low-footprint' algorithms for multi-robot systems. Here, we continue this\n'bio-inspired' tradition, by speculating on the technological benefit of fusing\nswarming with synchronization. The subject of recent theoretical work, minimal\nmodels of so-called 'swarmalator' systems exhibit rich spatiotemporal patterns,\nhinting at utility in 'bottom-up' robotic swarms. We review the theoretical\nwork on swarmalators, identify possible realizations in Nature, and discuss\ntheir potential applications in technology."}, {"title": "Abrupt Desynchronization and Extensive Multistability in Globally Coupled Oscillator Simplices", "abstract": "Collective behavior in large ensembles of dynamical units with non-pairwise\ninteractions may play an important role in several systems ranging from brain\nfunction to social networks. Despite recent work pointing to simplicial\nstructure, i.e., higher-order interactions between three or more units at a\ntime, their dynamical characteristics remain poorly understood. Here we present\nan analysis of the collective dynamics of such a simplicial system, namely\ncoupled phase oscillators with three-way interactions. The simplicial structure\ngives rise to a number of novel phenomena, most notably a continuum of abrupt\ndesynchronization transitions with no abrupt synchronization transition\ncounterpart, as well as, extensive multistability whereby infinitely many\nstable partially synchronized states exist. Our analysis sheds light on the\ncomplexity that can arise in physical systems with simplicial interactions like\nthe human brain and the role that simplicial interactions play in storing\ninformation."}, {"title": "Synchronization of Network-Coupled Oscillators with Uncertain Dynamics", "abstract": "Synchronization of network-coupled dynamical units is important to a variety\nof natural and engineered processes including circadian rhythms, cardiac\nfunction, neural processing, and power grids. Despite this ubiquity, it remains\npoorly understood how complex network structures and heterogeneous local\ndynamics combine to either promote or inhibit synchronization. Moreover, for\nmost real-world applications it is impossible to obtain the exact\nspecifications of the system, and there is a lack of theory for how uncertainty\naffects synchronization. We address this open problem by studying the Synchrony\nAlignment Function (SAF), which is an objective measure for the synchronization\nproperties of a network of heterogeneous oscillators with given natural\nfrequencies. We extend the SAF framework to analyze network-coupled oscillators\nwith heterogeneous natural frequencies that are drawn as a multivariate random\nvector. Using probability theory for quadratic forms, we obtain expressions for\nthe expectation and variance of the SAF for given network structures. We\nconclude with numerical experiments that illustrate how the incorporation of\nuncertainty yields a more robust theoretical framework for enhancing\nsynchronization, and we provide new perspectives for why synchronization is\ngenerically promoted by network properties including degree-frequency\ncorrelations, link directedness, and link weight delocalization."}, {"title": "Modular structure in C. elegans neural network and its response to external localized stimuli", "abstract": "Synchronization plays a key role in information processing in neuronal\nnetworks. Response of specific groups of neurons are triggered by external\nstimuli, such as visual, tactile or olfactory inputs. Neurons, however, can be\ndivided into several categories, such as by physical location, functional role\nor topological clustering properties. Here we study the response of the\nelectric junction C. elegans network to external stimuli using the partially\nforced Kuramoto model and applying the force to specific groups of neurons.\nStimuli were applied to topological modules, obtained by the ModuLand\nprocedure, to a ganglion, specified by its anatomical localization, and to the\nfunctional group composed of all sensory neurons. We found that topological\nmodules do not contain purely anatomical groups or functional classes,\ncorroborating previous results, and that stimulating different classes of\nneurons lead to very different responses, measured in terms of synchronization\nand phase velocity correlations. In all cases, however, the modular structure\nhindered full synchronization, protecting the system from seizures. More\nimportantly, the responses to stimuli applied to topological and functional\nmodules showed pronounced patterns of correlation or anti-correlation with\nother modules that were not observed when the stimulus was applied to ganglia."}, {"title": "Inhibition induced explosive synchronization in multiplex networks", "abstract": "To date, explosive synchronization (ES) is shown to be originated from either\ndegree-frequency correlation or inertia of phase oscillators. Of late, it has\nbeen shown that ES can be induced in a network by adaptively controlled phase\noscillators. Here we show that ES is a generic phenomenon and can occur in any\nnetwork by appropriately multiplexing it with another layer. We devise an\napproach which leads to the occurrence of ES with hysteresis loop in a network\nupon its multiplexing with a negatively coupled (or inhibitory) layer. We\ndiscuss the impact of various structural properties of positively coupled (or\nexcitatory) and inhibitory layer along with the strength of multiplexing in\ngaining control over the induced ES transition. This investigation is a step\nforward in highlighting the importance of multiplex framework not only in\nbringing novel phenomena which are not possible in an isolated network but also\nin providing more structural control over the induced phenomena."}, {"title": "Optimal global synchronization of partially forced Kuramoto oscillators", "abstract": "We consider the problem of global synchronization in a large random network\nof Kuramoto oscillators where some of them are subject to an external\nperiodically driven force. We explore a recently proposed dimensional reduction\napproach and introduce an effective two-dimensional description for the\nproblem. From the dimensionally reduced model, we obtain analytical predictions\nfor some critical parameters necessary for the onset of a globally synchronized\nstate in the system. Moreover, the low dimensional model also allows us to\nintroduce an optimization scheme for the problem. Our main conclusion, which\nhas been corroborated by exhaustive numerical simulations, is that for a given\nlarge random network of Kuramoto oscillators, with random natural frequencies\n$\\omega_i$, such that a fraction of them is subject to an external periodic\nforce with frequency $\\Omega$, the best global synchronization properties\ncorrespond to the case where the fraction of the forced oscillators is chosen\nto be those ones such that $|\\omega_i-\\Omega|$ is maximal. Our results might\nshed some light on the structure and evolution of natural systems for which the\npresence or the absence of global synchronization are desired properties. Some\nproperties of the optimal forced networks and its relation to recent results in\nthe literature are also discussed."}, {"title": "Partial synchronization in empirical brain networks as a model for unihemispheric sleep", "abstract": "We analyze partial synchronization patterns in a network of FitzHugh-Nagumo\noscillators with empirical structural connectivity measured in healthy human\nsubjects. We report a dynamical asymmetry between the hemispheres, induced by\nthe natural structural asymmetry. We show that the dynamical asymmetry can be\nenhanced by introducing the inter-hemispheric coupling strength as a control\nparameter for partial synchronization patterns. We specify the possible\nmodalities for existence of unihemispheric sleep in human brain, where one\nhemisphere sleeps while the other remains awake. In fact, this state is common\namong migratory birds and mammals like aquatic species."}, {"title": "Bicycle flow dynamics on wide roads: Experiment and modeling", "abstract": "Cycling is a green transportation mode, and is promoted by many governments\nto mitigate traffic congestion. However, studies concerning the traffic\ndynamics of bicycle flow are very limited. This study experimentally\ninvestigated bicycle flow dynamics on a wide road, modeled using a 3-m-wide\ntrack. The results showed that the bicycle flow rate remained nearly constant\nacross a wide range of densities, in marked contrast to single-file bicycle\nflow, which exhibits a unimodal fundamental diagram. By studying the weight\ndensity of the radial locations of cyclists, we argue that this behavior arises\nfrom the formation of more lanes with the increase of global density. The extra\nlanes prevent the longitudinal density from increasing as quickly as in\nsingle-file bicycle flow. When the density is larger than 0.5 bicycles/m2, the\nflow rate begins to decrease, and stop-and-go traffic emerges. A\ncognitive-science-based model to reproduce bicycle dynamics is proposed, in\nwhich cyclists apply simple cognitive procedures to adapt their target\ndirections and desired riding speeds. To incorporate differences in\nacceleration, deceleration, and turning, different relaxation times are used.\nThe model can reproduce the experimental results acceptably well and may also\nprovide guidance on infrastructure design."}, {"title": "Phase oscillator model for noisy oscillators", "abstract": "The Kuramoto model has become a paradigm to describe the dynamics of\nnonlinear oscillator under the influence of external perturbations, both\ndeterministic and stochastic. It is based on the idea to describe the\noscillator dynamics by a scalar differential equation, that defines the time\nevolution for the phase of the oscillator. Starting from a phase and amplitude\ndescription of noisy oscillators, we discuss the reduction to a phase\noscillator model, analogous to the Kuramoto model. The model derived shows that\nthe phase noise problem is a drift-diffusion process. Even in the case where\nthe expected amplitude remains unchanged, the unavoidable amplitude\nfluctuations do change the expected frequency, and the frequency shift depends\non the amplitude variance. We discuss different degrees of approximation,\nyielding increasingly accurate phase reduced descriptions of noisy oscillators."}, {"title": "A mathematical framework for amplitude and phase noise analysis of coupled oscillators", "abstract": "Synchronization of coupled oscillators is a paradigm for complexity in many\nareas of science and engineering. Any realistic network model should include\nnoise effects. We present a description in terms of phase and amplitude\ndeviation for nonlinear oscillators coupled together through noisy\ninteractions. In particular, the coupling is assumed to be modulated by white\nGaussian noise. The equations derived for the amplitude deviation and the phase\nare rigorous, and their validity is not limited to the weak noise limit. We\nshow that using Floquet theory, a partial decoupling between the amplitude and\nthe phase is obtained. The decoupling can be exploited to describe the\noscillator's dynamics solely by the phase variable. We discuss to what extent\nthe reduced model is appropriate and some implications on the role of noise on\nthe frequency of the oscillators."}, {"title": "Competitive Suppression of Synchronization and Non-Monotonic Transitions in Oscillator Communities with Distributed Time Delay", "abstract": "Community structure and interaction delays are common features of ensembles\nof network coupled oscillators, but their combined effect on the emergence of\nsynchronization has not been studied in detail. We study the transitions\nbetween macroscopic states in coupled oscillator systems with community\nstructure and time delays. We show that the combination of these two properties\ngives rise to non-monotonic transitions, whereby increasing the global coupling\nstrength can both inhibit and promote synchronization, yielding both\ndesynchronization and synchronization transitions. For relatively wide\nparameter choices we also observe asymmetric suppression of synchronization,\nwhere communities compete to suppress one another's synchronization properties\nuntil one or more win, totally suppressing the others to effective incoherence.\nUsing the ansatz of Ott and Antonsen we provide analytical descriptions for\nthese transitions that confirm numerical simulations."}, {"title": "Ott-Antonsen reduction for the non-Abelian Kuramoto model on the 3-sphere", "abstract": "We are interested in low-dimensional dynamics in an ensemble of coupled\nnonidentical generalized oscillators on the 3-sphere. The system of governing\nequations for such an ensemble is referred to as non-Abelian Kuramoto model in\nthe literature. We establish an analogue (or an extension) of the Ott-Antonsen\n(OA) result for this model."}, {"title": "Chimera dynamics in nonlocally coupled moving phase oscillators", "abstract": "Chimera states, a symmetry-breaking spatiotemporal pattern in nonlocally\ncoupled dynamical units, prevail in a variety of systems. However, the\ninteraction structures among oscillators are static in most of studies on\nchimera state. In this work, we consider a population of agents. Each agent\ncarries a phase oscillator. We assume that agents perform Brownian motions on a\nring and interact with each other with a kernel function dependent on the\ndistance between them. When agents are motionless, the model allows for several\ndynamical states including two different chimera states (the type-I and the\ntype-II chimeras). The movement of agents changes the relative positions among\nthem and produces perpetual noise to impact on the model dynamics. We find that\nthe response of the coupled phase oscillators to the movement of agents depends\non both the phase lag $\\alpha$, determining the stabilities of chimera states,\nand the agent mobility $D$. For low mobility, the synchronous state transits to\nthe type-I chimera state for $\\alpha$ close to $\\pi/2$ and attracts other\ninitial states otherwise. For intermediate mobility, the coupled oscillators\nrandomly jump among different dynamical states and the jump dynamics depends on\n$\\alpha$. We investigate the statistical properties in these different\ndynamical regimes and present the scaling laws between the transient time and\nthe mobility for low mobility and relations between the mean lifetimes of\ndifferent dynamical states and the mobility for intermediate mobility."}, {"title": "Control of coherence resonance by self-induced stochastic resonance in a multiplex neural network", "abstract": "We consider a two-layer multiplex network of diffusively coupled\nFitzHugh-Nagumo (FHN) neurons in the excitable regime. It is shown, in contrast\nto SISR in a single isolated FHN neuron, that the maximum noise amplitude at\nwhich SISR occurs in the network of coupled FHN neurons is controllable,\nespecially in the regime of strong coupling forces and long time delays. In\norder to use SISR in the first layer of the multiplex network to control CR in\nthe second layer, we first choose the control parameters of the second layer in\nisolation such that in one case CR is poor and in another case, non-existent.\nIt is then shown that a pronounced SISR cannot only significantly improve a\npoor CR, but can also induce a pronounced CR, which was non-existent in the\nisolated second layer. In contrast to strong intra-layer coupling forces,\nstrong inter-layer coupling forces are found to enhance CR. While long\ninter-layer time delays just as long intra-layer time delays, deteriorates CR.\nMost importantly, we find that in a strong inter-layer coupling regime, SISR in\nthe first layer performs better than CR in enhancing CR in the second layer.\nBut in a weak inter-layer coupling regime, CR in the first layer performs\nbetter than SISR in enhancing CR in the second layer. Our results could find\nnovel applications in noisy neural network dynamics and engineering."}, {"title": "Colored noise in oscillators. Phase-amplitude analysis and a method to avoid the Ito-Stratonovich dilemma", "abstract": "We investigate the effect of time-correlated noise on the phase fluctuations\nof nonlinear oscillators. The analysis is based on a methodology that\ntransforms a system subject to colored noise, modeled as an Ornstein-Uhlenbeck\nprocess, into an equivalent system subject to white Gaussian noise. A\ndescription in terms of phase and amplitude deviation is given for the\ntransformed system. Using stochastic averaging technique, the equations are\nreduced to a phase model that can be analyzed to characterize phase noise. We\nfind that phase noise is a drift-diffusion process, with a noise-induced\nfrequency shift related to the variance and to the correlation time of colored\nnoise. The proposed approach improves the accuracy of previous phase reduced\nmodels."}, {"title": "Aging transition in the absence of inactive oscillators", "abstract": "The role of counter-rotating oscillators in an ensemble of coexisting co- and\ncounter-rotating oscillators is examined by increasing the proportion of the\nlatter. The phenomenon of aging transition was identified at a critical value\nof the ratio of the counter-rotating oscillators, which was otherwise realized\nonly by increasing the number of inactive oscillators to a large extent. The\neffect of the mean-field feedback strength in the symmetry preserving coupling\nis also explored. The parameter space of aging transition was increased\nabruptly even for a feeble decrease in the feedback strength and subsequently,\nthe aging transition was observed at a critical value of the feedback strength\nsurprisingly without any counter-rotating oscillators. Further, the study was\nextended to symmetry breaking coupling using conjugate variables and it was\nobserved that the symmetry breaking coupling can facilitating the onset of\naging transition even in the absence of counter-rotating oscillators and for\nthe unit value of the feedback strength. In general, the parameter space of\naging transition was found to increase by increasing the frequency of\noscillators and by increasing the proportion of the counter-rotating\noscillators in both the symmetry preserving and symmetry breaking couplings.\nFurther, the transition from oscillatory to aging transition occurs via a Hopf\nbifurcation, while the transition from aging transition to oscillation death\nstate emerges via Pitchfork bifurcation. Analytical expressions for the\ncritical ratio of the counter- rotating oscillators are deduced to find the\nstable boundaries of the aging transition."}, {"title": "Dynamic Vulnerability in Oscillatory Networks and Power Grids", "abstract": "Recent work found distributed resonances in driven oscillator networks and AC\npower grids. The emerging dynamic resonance patterns are highly heterogeneous\nand nontrivial, depending jointly on the driving frequency, the interaction\ntopology of the network and the node or nodes driven. Identifying which nodes\nare most susceptible to dynamic driving and may thus make the system as a whole\nvulnerable to external input signals, however, remains a challenge. Here we\npropose an easy-to-compute Dynamic Vulnerability Index (DVI) for identifying\nthose nodes that exhibit largest amplitude responses to dynamic driving signals\nwith given power spectra and thus are most vulnerable. The DVI is based on\nlinear response theory, as such generic, and enables robust predictions. It\nthus shows potential for a wide range of applications across dynamically driven\nnetworks, for instance for identifying the vulnerable nodes in power grids\ndriven by fluctuating inputs from renewable energy sources and fluctuating\npower output to households."}, {"title": "Hierarchical clusters in neuronal populations with plasticity", "abstract": "We report the phenomenon of frequency clustering in a network of\nHodgkin-Huxley neurons with spike timing-dependent plasticity. The clustering\nleads to a splitting of a neural population into a few groups synchronized at\ndifferent frequencies. In this regime, the amplitude of the mean field\nundergoes low-frequency modulations, which may contribute to the mechanism of\nthe emergence of slow oscillations of neural activity observed in spectral\npower of local field potentials or electroencephalographic signals at high\nfrequencies. In addition to numerical simulations of such multi-clusters, we\ninvestigate the mechanisms of the observed phenomena using the simplest case of\ntwo clusters. In particular, we propose a phenomenological model which\ndescribes the dynamics of two clusters taking into account the adaptation of\ncoupling weights. We also determine the set of plasticity functions (update\nrules), which lead to multi-clustering."}, {"title": "Some lattice models with hyperbolic chaotic attractors", "abstract": "Examples of one-dimensional lattice systems are considered, in which patterns\nof different spatial scales arise alternately, so that the spatial phase over a\nfull cycle undergo transformation according to expanding circle map that\nimplies occurrence of Smale-Williams attractors in the multidimensional state\nspace. These models can serve as a basis for design electronic generators of\nrobust chaos within a paradigm of coupled cellular networks. One of the\nexamples is a mechanical pendulum system interesting and demonstrative for\nresearch and educational experimental studies."}, {"title": "Emergence of global synchronization in directed excitatory networks of type I neurons", "abstract": "The collective behaviour of neural networks depends on the cellular and\nsynaptic properties of the neurons. The phase-response curve (PRC) is an\nexperimentally obtainable measure of cellular properties that quantifies the\nshift in the next spike time of a neuron as a function of the phase at which\nstimulus is delivered to that neuron. The neuronal PRCs can be classified as\nhaving either purely positive values (type I) or distinct positive and negative\nregions (type II). Networks of type 1 PRCs tend not to synchronize via mutual\nexcitatory synaptic connections. We study the synchronization properties of\nidentical type I and type II neurons, assuming unidirectional synapses.\nPerforming the linear stability analysis and the numerical simulation of the\nextended Kuramoto model, we show that feedforward loop motifs favour\nsynchronization of type I excitatory and inhibitory neurons, while feedback\nloop motifs destroy their synchronization tendency. Moreover, large directed\nnetworks, either without feedback motifs or with many of them, have been\nconstructed from the same undirected backbones, and a high synchronization\nlevel is observed for directed acyclic graphs with type I neurons. It has been\nshown that, the synchronizability of type I neurons depends on both the\ndirectionality of the network connectivity and the topology of its undirected\nbackbone. The abundance of feedforward motifs enhances the synchronizability of\nthe directed acyclic graphs."}, {"title": "Higher-order interactions in complex networks of phase oscillators promote abrupt synchronization switching", "abstract": "Synchronization processes play critical roles in the functionality of a wide\nrange of both natural and man-made systems. Recent work in physics and\nneuroscience highlights the importance of higher-order interactions between\ndynamical units, i.e., three- and four-way interactions in addition to pairwise\ninteractions, and their role in shaping collective behavior. Here we show that\nhigher-order interactions between coupled phase oscillators, encoded\nmicroscopically in a simplicial complex, give rise to added nonlinearity in the\nmacroscopic system dynamics that induces abrupt synchronization transitions via\nhysteresis and bistability of synchronized and incoherent states. Moreover,\nthese higher-order interactions can stabilize strongly synchronized states even\nwhen the pairwise coupling is repulsive. These findings reveal a self-organized\nphenomenon that may be responsible for the rapid switching to synchronization\nin many biological and other systems that exhibit synchronization without the\nneed of particular correlation mechanisms between the oscillators and the\ntopological structure."}, {"title": "Synchronization of globally coupled oscillators without symmetry in the distribution of natural frequencies", "abstract": "The collective behavior in a population of globally coupled oscillators with\nrandomly distributed frequencies is studied when the natural frequency\ndistribution does not possess an even symmetry with respect to the average\nnatural frequency of oscillators. We study the special case of absence of\nsymmetry induced by a group of scaling transformations of the continuous\ndistribution of frequencies. When coupling between oscillators is increased\nbeyond a critical threshold favoring spontaneous synchronization, we found that\nthe variation in the velocity of the traveling wave depends on the extent of\nasymmetry in the natural frequency distribution. In particular for large\ncoupling this velocity is the average natural frequency whereas at the onset of\nsynchronization it corresponds to the frequency where the Hilbert Transform of\nthe frequency distribution vanishes."}, {"title": "Stochastic properties of the frequency dynamics in real and synthetic power grids", "abstract": "The frequency constitutes a key state variable of electrical power grids.\nHowever, as the frequency is subject to several sources of fluctuations,\nranging from renewable volatility to demand fluctuations and dispatch, it is\nstrongly dynamic. Yet, the statistical and stochastic properties of the\nfrequency fluctuation dynamics are far from fully understood. Here, we analyse\nproperties of power grid frequency trajectories recorded from different\nsynchronous regions. We highlight the non-Gaussian and still approximately\nMarkovian nature of the frequency statistics. Further, we find that the\nfrequency displays significant fluctuations exactly at the time intervals of\nregulation and trading, confirming the need of having a regulatory and market\ndesign that respects the technical and dynamical constraints in future highly\nrenewable power grids. Finally, employing a recently proposed synthetic model\nfor the frequency dynamics, we combine our statistical and stochastic analysis\nand analyse in how far dynamically modelled frequency properties match the ones\nof real trajectories."}, {"title": "Intelligence of small groups", "abstract": "Dunbar hypothesized that $150$ is the maximal number of people with whom one\ncan maintain stable social relationships. We explain this effect as being a\nconsequence of a process of self-organization between $N$ units leading their\nsocial system to the edge of phase transition, usually termed criticality.\nCriticality generates events, with an inter-event time interval distribution\ncharacterized by an inverse power law (IPL) index $\\mu_{S}<2$. These events\nbreak ergodicity and we refer to them as crucial events. The group makes\ndecisions and the time persistence of each decision is given by another IPL\ndistribution with IPL index $\\mu_{R}$, which is different from $\\mu_{S}$ if\n$N\\neq 150$. We prove that when the number of interacting individuals is equal\nto $150$, these two different IPL indexes become identical, with the effect of\ngenerating the Kardar Parisi Zhang (KPZ) scaling $\\delta =1/3$. We argue this\nto be an enhanced form of intelligence, which generates efficient information\ntransmission within the group. We prove the inflrmation transmission efficiency\nis maximal when $N=150$, the Dunbar number."}, {"title": "Quasi-periodic dynamics and a Neimark-Sacker bifurcation in nonlinear random walks on complex networks", "abstract": "We study the dynamics of nonlinear random walks on complex networks. We\ninvestigate the role and effect of directed network topologies on long-term\ndynamics. While a period-doubling bifurcation to alternating patterns occurs at\na critical bias parameter value, we find that some directed structures give\nrise to a different kind of bifurcation that gives rise to quasi-periodic\ndynamics. This does not occur for all directed network structure, but only when\nthe network structure is sufficiently directed. We find that the onset of\nquasi-periodic dynamics is the result of a Neimark-Sacker bifurcation, where a\npair of complex-conjugate eigenvalues of the system Jacobian passes through the\nunit circle, destabilizing the stationary distribution with high-dimensional\nrotations. We investigate the nature of these bifurcations, study the onset of\nquasi-periodic dynamics as network structure is tuned to be more directed, and\npresent an analytically tractable case of a four-neighbor ring."}, {"title": "Measuring complexity", "abstract": "Complexity is a multi-faceted phenomenon, involving a variety of features\nincluding disorder, nonlinearity, and self-organisation. We use a recently\ndeveloped rigorous framework for complexity to understand measures of\ncomplexity. We illustrate, by example, how features of complexity can be\nquantified, and we analyse a selection of purported measures of complexity that\nhave found wide application and explain whether and how they measure\ncomplexity. We also discuss some of the classic information-theoretic measures\nfrom the 1980s and 1990s. This work gives the reader a tool kit for quantifying\nfeatures of complexity across the sciences."}, {"title": "Synchronization in networks of coupled hyperchaotic CO$_2$ lasers", "abstract": "A non-autonomous dynamical system is proposed to study the synchronization in\nnetworks of mutually coupled optically modulated hyperchaotic CO$_2$ lasers. By\nthe method of master stability function (MSF) it is shown that the stable\nsynchronous state can be reached for both the ring of diffusively coupled (RDC)\nand star-coupled (SC) networks of at most $24$ nodes or oscillators. A\nnumerical simulation of the coupled $24$ hyperchaotic CO$_2$ lasers is also\nperformed to show that the corresponding synchronization error\n$\\lesssim10^{-6}$. Furthermore, the chimera states of the networks are found to\ncoexist in some intervals of time and the coupling strengths where the networks\nare not synchronized, implying that the synchronization occurs only in some\nspecific ranges of values of the coupling strengths."}, {"title": "Strange nonchaos in self-excited singing flames", "abstract": "We report the first experimental evidence of strange nonchaotic attractor\n(SNA) in the natural dynamics of a self-excited laboratory-scale system. In the\nprevious experimental studies, the birth of SNA was observed in\nquasiperiodically forced systems; however, such an evidence of SNA in an\nautonomous laboratory system is yet to be reported. We discover the presence of\nSNA in between the attractors of quasiperiodicity and chaos through a\nfractalization route in a laboratory thermoacoustic system. The observed\ndynamical transitions from order to chaos via SNA is confirmed through various\nnonlinear characterization methods prescribed for the detection of SNA."}, {"title": "On the Concept of Dynamical Reduction : The Case of Coupled Oscillators", "abstract": "An overview is given on two representative methods of dynamical reduction\nknown as center-manifold reduction and phase reduction. These theories are\npresented in a somewhat more unified fashion than the theories in the past. The\ntarget systems of reduction are coupled limit-cycle oscillators. Particular\nemphasis is placed on the remarkable structural similarity existing between\nthese theories. While the two basic principles, i.e. (i) reduction of dynamical\ndegrees of freedom and (ii) transformation of reduced evolution equation to a\ncanonical form, are shared commonly by reduction methods in general, it is\nshown how these principles are incorporated into the above two reduction\ntheories in a coherent manner. Regarding the phase reduction, a new formulation\nof perturbative expansion is presented for discrete populations of oscillators.\nThe style of description is intended to be so informal that one may digest,\nwithout being bothered with technicalities, what has been done after all under\nthe word reduction."}, {"title": "Traffic flow on star graph: Nonlinear diffusion", "abstract": "We study the urban-scale macroscopic traffic flow in city networks. Star\ngraph is considered as traffic network. Star graphs with controlled traffic\nflow are transformed to various cell-transmission graphs by using the cell\ntransmission method. The dynamic equations of vehicular densities on all nodes\n(roads) are presented on cell-transmission graphs by using the speed-matching\nmodel. The density equations are given by nonlinear-diffusion equations. The\ntraffic flow on star graph is mapped to the nonlinear diffusion process on the\ncell-transmission graphs. At low mean density, the dynamic equations of\ndensities can be approximated by the conventional diffusion equations. At low\nand high mean densities, the analytical solutions of densities on all nodes\n(roads) are obtained on cell-transmission complete, cycle and star graphs. By\nsolving the dynamic equations numerically, the densities on all roads are\nderived at a steady state. The urban-scale macroscopic fundamental diagrams are\nobtained numerically on the cell-transmission graphs. The analytical solutions\nagree with the numerical solutions."}, {"title": "Towards synthetic neural networks: Can artificial electrochemical neurons be coupled with artificial memristive synapses?", "abstract": "The enormous amount of data generated nowadays worldwide is increasingly\ntriggering the search for unconventional and more efficient ways of processing\nand classifying information, eventually able to transcend the conventional\nvon-Neumann-Turing computational central dogma. It is, therefore, greatly\nappealing to draw inspiration from less conventional but computationally more\npowerful systems such as the neural architecture of the human brain. This\nneuromorphic route has the potential to become one of the most influential and\nlong-lasting paradigms in the field of unconventional computing. The\nmaterial-based workhorse for current hardware platforms is largely based on\nstandard CMOS technologies, intrinsically following the above mentioned\nvon-Neumann-Turing prescription; we do know, however, that the brain hardware\noperates in a massively parallel way through a densely interconnected physical\nnetwork of neurons. This requires challenging the intrinsic definition of the\nsingle units and the architecture of computing machines. (...)"}]